<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-04-25T13:51:25+00:00</updated><id>/feed.xml</id><title type="html">まいどぅー</title><subtitle>&lt;b style=&quot;color: #f45;&quot;&gt;All Generation Tasks are Denoising Tasks.&lt;/b&gt;</subtitle><author><name>Norm Inui</name></author><entry><title type="html">What inference-time scaling should be like for Diffusion Model</title><link href="/rlhf-in-diffusion/" rel="alternate" type="text/html" title="What inference-time scaling should be like for Diffusion Model" /><published>2025-01-18T00:00:00+00:00</published><updated>2025-01-18T00:00:00+00:00</updated><id>/rlhf-in-diffusion</id><content type="html" xml:base="/rlhf-in-diffusion/"><![CDATA[<!--more-->

<h2 id="background">Background</h2>

<p>Inference-time scaling in Large Language Models (LLMs) has been a hot topic recently. The concept of “slow thinking” in LLMs, where more computation during inference leads to significant performance improvements, has received widespread attention and has been proved effective. The open-source community has made significant achievement in reproducing the “slow thinking” approach, milestone works such as <a href="https://huggingface.co/papers/2501.07301">Lessons of PRM in Maths</a>, <a href="https://arxiv.org/abs/2501.04519">rStar-Math</a> showing us the use of Monte Carlo Tree Rollout methods to iteratively train Process Reward Models (PRM) and synthesize Chain-of-Thought (CoT) data.</p>

<p>But what about the diffusion model? Is it necessary to implement inference-time scaling there as well? And could this be just another story told by NVIDIA for their stock prices?</p>

<p>The answer, in my view, is a definitive “necessary for diffusion model”.</p>

<h3 id="sde-vs-ode">SDE V.S. ODE</h3>

<p>In fact, we’ve already seen how inference-time scaling in diffusion models can be beneficial, particularly for the ODE v.s. SDE sampling.</p>

\[dx=-\underbrace{\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t))dt}_{\text{PFODE}}\;  
\underbrace{-\;\underbrace{\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t))dt}_{\text{deterministic noise decay}}+\underbrace{\sqrt{2\beta(t)}\sigma(t)dw_{t}}_{\text{noise injection}}}_{\text{Langevin diffusion SDE}}\]

<p>Above equation represent the general form of diffusion sampling. As you can see, sampling consists of two main terms: PFODE and Langevin SDE. The most interesting aspect here is that during noise decay, Langevin Diffusion SDE also injects noise (the third term). This dual role of denoising and noise injection helps mitigate the error caused by pure denoising, effectively creating an inference-time scaling effect by increasing computation.</p>

<p><a href="https://arxiv.org/pdf/2401.08740">SiT paper</a> propose a similar conclusion:</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/9/sit-fig5.png" alt="" /></p>

<p><strong>Figure 1.</strong> The ODE converges faster with fewer NFE, while the SDE is capable of reaching a much lower final FID score when given a larger computational budget</p>

<p>This observation emphasizes an essential trade-off: ODE is computationally cheaper and converges faster, while SDE excels with a larger computational budget, producing more refined results (in terms of FID score). Therefore, while ODE provides faster convergence, SDE’s ability to inject noise in the diffusion process to mitigate accumulated errors during sampling can be computationally scaled up appropriately.</p>

<h3 id="implications-for-world-model-and-video-generation">Implications for World Model and Video Generation</h3>

<p>I believe that the inference-time scaling in diffusion models is a critical factor in solving issues in world models and video generation models, especially in cases where physical simulation is important. Intuitively, scaling up computations during inference should be beneficial to achieve higher-quality results, since diffusion models can allocate more computations to render complex scenes and simulate complex physics phenomenon.</p>

<h2 id="how-can-we-implement-inference-time-scaling-in-diffusion-models">How Can We Implement Inference-Time Scaling in Diffusion Models?</h2>

<p>If inference-time scaling offers benefits in diffusion-based generation, the next question is: <strong>how can we implement this in diffusion models?</strong></p>

<p>In fact, this is actually not a simple task. The challenge lies in the principles of Diffusion Models.</p>

<h3 id="increasing-nfe">Increasing NFE?</h3>

<p>Diffusion Models work by denoising, gradually transforming pure noise into an image or video latent feature. Therefore, it seems that inference-time scaling for Diffusion Models should be easy, why not just simply increase the denoising steps? This is often known as increasing the number of function evaluations (NFE). However, many studies have already found that increasing NFEs will soon reach a plateau in the generated image/video quality after a certain number of steps. Therefore, merely increasing the NFE is not a viable approach.</p>

<h3 id="dpo-for-diffusion">DPO for Diffusion?</h3>

<p>Although there have been methods like <a href="https://arxiv.org/pdf/2405.13637">SD + DPO</a>, DPO’s reward is calculated at the instance level through preference pair data. Whether for images or videos, this level of granularity is too coarse. Coarse-grained rewards struggle to improve training data efficiency, and the model can easily “hack” preference data. This makes preference data challenging to work with. At least, making a preference data for images is one thing, but what about videos? Can we really have a clear preference between frames of two videos?</p>

<h3 id="prm--mct-rollout-like-what-llm-does">PRM + MCT rollout like what LLM does?</h3>

<p>Then, Could we just follow LLMs using PRM + MCT rollout? As of the time I’m writing this blog (Jan 18th, 2025), the answer seems to be no.</p>

<p>Unlike LLMs, where generation happens with discrete tokens, most diffusion models generate in a continuous space. This makes applying PRM + MCT rollout methods to diffusion models infeasible. We cannot create CoT-like data for training diffusion models in the same way we do with text modalities, because images and videos are not discrete tokens. Actually, we even have no idea what CoT-like data shoule be like for diffusion model. We can’t extend CoT chains, manually set terminal points, calculate terminal rewards, and then use various rules to roll the data.</p>

<p>Of course, one path could be to drop the continuous latent space of diffusion models and switch to a discrete latent space and use AR (autoregressive) generation for images and videos. Once generation becomes a token-by-token process, all of the LLM’s RLHF methods become applicable.</p>

<p>In this area, I think NVIDIA’s <a href="[[2501.03575] Cosmos World Foundation Model Platform for Physical AI](https://arxiv.org/abs/2501.03575)">Cosmos-1.0-Autoregressive</a> has made the most solid advancements.</p>

<p>Cosmos-1.0-Autoregressive has explored extensively and gained many insights into design.</p>

<p>First,  the key challenge of training an image/video generation in AR fashion is how to develop a tokenizer with sufficient spatial-temporal compression.</p>

<p>Cosmos uses a discrete tokenizer of 8x16x16, but this compression rate still seems inefficient. The sequence length of high-resolution images and long videos with such a tokenizer will become extremely long, even longer than the training context length that has been validated for text modalities. A natural follow-up is to pursue a tokenizer with even higher compression, such as 16x32x32. However, the Cosmos team considers 8x16x16 to be already quite aggressive.</p>

<blockquote>
  <p>“As a result of aggressive compression, it could sometimes lead to blurriness and visible artifacts in video generation, especially in the autoregressive WFM setting, where only a few integers are used to represent a rich video through discrete tokenization.”</p>
</blockquote>

<p>Thus, they had to design a diffusion decoder, where discrete token video is treated as the conditional input to the denoiser. The final image/video output is then generated by the diffusion decoder.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/9/cosmos-ar.png" alt="" /></p>

<p>The upper bound of this approach is clear. It depends on the quality and compression rate of the tokenizer, but it still doesn’t drop the continuous space for generation. After all, images and videos are inherently continuous modalities. Furthermore, the long sequence lengths far exceed those of text, which poses a significant challenge for training infrastructure perspective, not even mention serving such a model.</p>

<h3 id="search-noise-and-path">Search Noise and Path</h3>

<p>On January 17th, 2025, DeepMind published a paper on <a href="https://arxiv.org/pdf/2501.09732">scaling Diffusion Models</a>. The paper proposes what seems to be the most promising approach up till now.</p>

<ul>
  <li>
    <p><strong>Rollout Initial Noise</strong>, referred to as Zero-Order Search, uses verifiers’ feedback to iteratively refine noise candidates.</p>
  </li>
  <li>
    <p><strong>Search over Paths</strong>, which leverages verifiers’ feedback to iteratively refine diffusion sampling trajectories.</p>
  </li>
</ul>

<p>The paper also introduces, for the first time, a mechanism similar to PRM within the entire pipeline, referred to as the <strong>verifier</strong>. These verifiers are classifiers trained based on CLIP and DINOv2, which use class label logits to help roll out the Initial Noise and Search over Paths. However, this rollout pipeline is still in its very early stages. The search algorithm is quite easy to hack via the reward verifier.</p>

<p>From my understanding, although Rollout Initial Noise has some solid theory behind it, as many studies have shown that certain good noise do exist for better generation results. However, no single configuration is universally optimal.</p>

<h2 id="what-a-rlhf-for-diffusion-should-be-like">What a RLHF For Diffusion should be like?</h2>

<p>Upon reviewing it again, the problems that need to be solved to run RLHF for the diffusion model are:</p>

<ul>
  <li>
    <p>How to define process rewards in continuous space?</p>
  </li>
  <li>
    <p>How to enable the diffusion model to self-improve, similar to how LLMs perform data rollouts based on MCT?</p>
  </li>
  <li>
    <p>What exactly does the diffusion process reward model look like?</p>
  </li>
</ul>

<p>Below, I propose a method I believe to be feasible.</p>

<p>The current flow-matching method actually starts by randomly initializing noise and letting the model fit the distribution of training data to construct a vector field. This vector field pushes the initial noise towards the expecting results. Interestingly, in robotics path planning, similar algorithms have existed for a long time, and they are called potential fields.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/9/potential_field.jpeg" alt="" /></p>

<p><strong>Figure 2.</strong> potential field for robot path planning.</p>

<p>When you examine potential fields, you’ll find many similarities with flow matching. In potential fields, the robot is regarded as a positive charge, and the goal as a negative charge, with opposite charges attracting each other, creating a potential field that guides the robot’s path planning.</p>

<p>However, in the real world, physical obstacles exist. To avoid obstacles, robot often regard obstacles as positive charges, like the robot itself, so that positive charges repel each other, and negative charges attract.</p>

<p>Now you see the inspiration: <strong>The current diffusion training only considers the start and end points, without considering obstacles along the probabilistic path.</strong> These obstacles should be human preferences. For example, when generating a video, if a person walks through a wall, this breaks physical laws, and thus the obstacle represents physical law. Similarly, if an image generates a person with six fingers, the obstacle represents human preferences.</p>

<p>Therefore, in continuous space, the process reward should be:</p>

\[x_{t-1} = U_{\text{att}}(x_t, t) + U_{\text{rep}}(x_t, t)\]

<p>where \(x \in [0, 1]\)</p>

<p>\(U_{\text{att}}\) is the “attractive” vector field, move the initial noise \(x_1\) to the goal \(x_0\)</p>

<p>\(U_{\text{att}}\) is the “repulsive” vector field, make the initial noise \(x_1\) avoid obstacles.</p>

<p>Thus, the process reward in continuous space should be such that when the probabilistic path approaches obstacles, it is repelled by them. This repulsion force should have a threshold, Q∗, which represents the process reward score that needs to be searched via RLHF.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/9/repulsive.png" alt="" /></p>

<p><strong>Figure 3</strong>; Repulsive by Obstacles then point locates inside \(Q^*\)</p>

<p>More specifically, this process reward score should be: when a denoising step hits an obstacle/locate inside the $Q^*$, the model should “feel the pain” and revert to the previous step, continuing to search for a collision-free path.</p>

<p>This is similar to search over paths and also shares similarities with the Langevin dynamics term in SDEs. The algorithm for search over paths is as follows:</p>

<blockquote>
  <p>Step 1: Sample N initial i.i.d. noises and run the ODE solver until a certain noise level \(σ\). The noisy samples \(x_σ\) serve as the starting point for the search.</p>

  <p>Step 2: Sample M i.i.d. noises for each noisy sample and simulate the forward noising process from \(σ\) to \(σ+Δf\) to produce \(x_{σ+Δf}\) with size M.</p>

  <p>Step 3: Run the ODE solver on each \(x_{σ+Δf}\) to the noise level \(σ+Δf−Δb\), obtaining \(x_{σ+Δf−Δb}\). Run verifiers on these samples and keep the top N candidates. Repeat steps 2-3 until the ODE solver reaches \(σ=0\).</p>

  <p>Step 4: Run the remaining N samples through random search and keep the best one.</p>
</blockquote>

<p>However, Langevin dynamics does not well incorporate a repulsive potential; it’s more focused on random search. Search over paths relies on verifiers to provide process rewards, but verifiers are vulnerable and can be easily “hacked” by the model.</p>

<p>Thus, I propose the following process reward:</p>

\[U_{\text{rep}}(q) = \left\{
\begin{array}{ll}
\frac{1}{2} \eta \left( \frac{1}{D(x_t, t)} - \frac{1}{Q^*} \right)^2, &amp; D(x_t, t) \leq Q^* \\
0, &amp; D(x_t, t) &gt; Q^*
\end{array}
\right.\]

<p>where \(D(x_t, t)\) is the distance from the noisy latent feature \(x_t\) to the obstacles under the vector field.</p>

<p>We assume that such a vector field, combining both attractive and repulsive fields, has already been learned through the training data. For the diffusion model, this allows it to self-improve, similar to how LLMs perform data rollouts based on MCT—constantly starting from random points in latent space to search for the best collision-free probabilistic path.</p>

<p>Now, the key question is: <strong>how do we represent these obstacles in latent space?</strong> In other words, what exactly does the diffusion process reward model look like?</p>

<p>When we lack ideas, we can look at how robotics addresses this problem.</p>

<h3 id="configuration-space">Configuration Space</h3>

<p>Firstly, robotics path planning is not always done in Cartesian space. Some method does path planning in a configuration space.</p>

<p>What is a configuration space?</p>

<blockquote>
  <p>The space of all configurations is the configuration space or C-space.</p>

  <p>— C-space formalism: Lozano-Perez ‘79</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/9/c-space.png" alt="" /></p>

<p>For a robot with two degrees of freedom, \(\alpha\) and \(\beta\), the space defined by these two variables is the robot’s C-space \(C_{\text{robot}} \in\mathbb{R}^2\). Each degree of freedom theoretically spans \([0,2π]\), but due to physical constraints, \(\alpha \in [0,\pi]\) and \(\beta\in [0,2\pi]\). In \(C_{\text{robot}}\), each point represents a robot’s pose configuration. Points that do not satisfy physical laws, or where the robot hit itself, are termed singularity points. All singularity points form the singularity space.</p>

<p>Therefore, moving collision-freely from pose A to pose B in the configuration space becomes a simple point-to-point path planning problem. The space of poses that cause collisions constitutes the obstacles in C-space.</p>

<p>You may notice that there’s an amazing similarity between C-space and VAE. Both serve as compressed spaces, translating a higher-dimensional path planning problem into a lower-dimensional one. Also, it’s common knowledge that VAE’s dimensionality is generally not large. Even in temporal and spatial compression, such as 8x16x16, the latent dimension typically remains small (e.g., 8, 12, 16). One reason is that increasing the latent dimension adds redundancy in the latent space, making DiT learning harder. Although scaling up DiT parameters can somewhat mitigate the redundancy of larger latent dimensions, most developers don’t follow this practice.</p>

<p>This can be verified in C-space: <strong>as a robot’s degrees of freedom increase, the C-space’s dimensionality increases, and so does the singularity space.</strong> This is why VAE redundancy increases when latent dimension increases.</p>

<h3 id="semantic-vae">Semantic VAE</h3>
<p>Returning to obstacle representation, we can follow how C-space represent obstacles. By projecting data or concepts that do not align with human preferences into the VAE space, we can guide the initial noise to avoid falling into singularity or collision spaces. This is what we call rollout initial noise, also known as zero-order search.</p>

<p>Thus, a promising future research for optimizing VAEs seems clear: make the VAE latent features not just a compressed space, but also introduce semantics to align with them. This way, we can project human preferences and physical laws into the VAE latent space, similar to how obstacles are represented in C-space.</p>

<p>Fortunately, some papers have already taken this approach. For example, <a href="https://arxiv.org/abs/2501.01423">Reconstruction vs. Generation</a> demonstrates that by introducing DINOv2, the latent space not only learns more efficiently but also lays the foundation for expressing human preferences and physical laws in the future.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/9/taming_vae.png" alt="" />
<strong>Figure 4.</strong> The latent space learns more efficiently by aligning DINOv2</p>

<p>Thus, if self-supervision vision foundation models can serve as a bridge for reconstruction and representation, we can project more obstacles into the VAE space and even directly use this feature as a classifier to learn human preferences.</p>

<h2 id="conclusion">Conclusion</h2>

<p>For the diffusion model, when doing Process Reward in continuous space, we can reference the robotics potential field method. We treat the initial noise as a positive charge, the generated target as a negative charge, and introduce obstacles as positive charges. The repulsive vector fields between the positive charges and the attractive vector field between the positive and negative charges should be superimposed to search for the strength and effective range of the repulsive vector field of each obstacle.</p>

<p>To achieve this, we need to align the VAE with world knowledge, allowing us to construct obstacles in the VAE space by inputting text, images, or videos. At the same time, the VAE features should ideally integrate reconstruction and representation. By training a large number of human preference classifiers, we can build an effective search space within the VAE space.</p>]]></content><author><name>Norm Inui</name></author><category term="Sparks" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">A Unified Perspective of Mathematical Diffusion</title><link href="/diffusion-cheat-sheet/" rel="alternate" type="text/html" title="A Unified Perspective of Mathematical Diffusion" /><published>2024-09-16T00:00:00+00:00</published><updated>2024-09-16T00:00:00+00:00</updated><id>/diffusion-cheat-sheet</id><content type="html" xml:base="/diffusion-cheat-sheet/"><![CDATA[<p>This is a cheat sheet of all denoising-based diffusion method. No mathematics derivations are included for concise. Check the reference links if you’re interested in the derivations.</p>

<p><a href="https://arxiv.org/pdf/2206.00364">EDM</a> introduces a <strong>general denoising equation</strong> that nearly unifies all widely recognized denoising and noise-adding processes within its framework.</p>

<h2 id="a-unified-perspective">A Unified Perspective</h2>

<h3 id="general-forward-process">General Forward Process</h3>

<p>Noise-adding Process is also known as forward process. The general form of this Forward Process is:</p>

\[p(x_t | x_0) = \mathcal{N}(x_t; s(t)x_0, \sigma^2(t) s^2(t) \mathbf{I})\]

<p>This equation describes how noise is gradually added to an image, represented as  \(x_0\), over time steps \(t\). The two functions, \(s(t)\) and \(\sigma(t)\), control the scale and variance of the noise, determining the trajectory of noise addition as \(x_t\) moves further from the original image.</p>

<h3 id="general-reverse-process">General Reverse Process</h3>

<p>The Forward Process equation above can be written into a <strong>Stochastic Differential Equation (SDE)</strong> format as:</p>

\[dx_t = f(x_t, t) \, dt + g(t) \, d\mathbf{w}\]

<p>Here, \(s(t) = e^{\int_{0}^{t} f(r) \, dr}\) and \(\sigma^2(t) = \int_{0}^{t} \frac{g^2(r)}{s^2(r)} \, dr\)</p>

<p>\(d\mathbf{w}\) represents a <strong>Wiener process</strong>, where \(\mathbf{w_t} \sim \mathcal{N}(0, t)\); \(d\mathbf{w} = \sqrt{dt} \; \epsilon\) , where \(\epsilon \sim \mathcal{N}(\mu, \sigma^2)\)</p>

<p>Different methods define their own specific functions for \(f(x_t, t)\) and \(g(t)\). For example, in diffusion models like <strong>DDPM</strong> and <strong>SMLD</strong> (Score Matching with Langevin Dynamics), \(f(x_t, t)\) is typically a linear function, \(f(x_t, t) = f(t)x_t\), where \(f(t)\) is a time-dependent term that modulates the trajectory of the image’s transformation over time.  EDM simply set \(s(t) = 1, \, \sigma(t) = t\).</p>

<p>Now we have a forward SDE to corrupt a data into noise. To reverse such as process, we have two choices:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">x_data</span> <span class="o">--&gt;|</span><span class="n">Forward</span> <span class="n">SDE</span><span class="o">|</span> <span class="o">--&gt;</span> <span class="n">x_noise</span>
    <span class="n">x_noise</span> <span class="o">--&gt;|</span><span class="n">Reverse</span> <span class="n">SDE</span><span class="o">|</span> <span class="o">--&gt;</span> <span class="n">x_data</span>
    <span class="n">x_noise</span> <span class="o">--&gt;|</span><span class="n">PFODE</span><span class="o">|</span> <span class="o">--&gt;</span> <span class="n">x_data</span>
</code></pre></div></div>

<p>Using the <strong>Fokker-Planck equation</strong>, we can transform the forward SDE as:</p>

\[dx = \left(\frac{1}{2}g^{2}(t) - \dot{\sigma}(t)\sigma(t)\right) \nabla_x \log p(x;\sigma(t)) dt + g(t) dw_t\]

<p>1.when \(g(t) = \sqrt{2\beta(t)} \sigma(t)\)</p>

\[dx_{\pm}=-\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t))dt\pm\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t))dt+\sqrt{2\beta(t)}\sigma(t)dw_{t}\]

<p>the \(dx_+\) here means forward process; the \(dx_-\) here means reverse process.</p>

<p>The equation consists of three key parts:</p>

<ul>
  <li>
    <p><strong>Deterministic Process</strong>: \(-\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t)) dt\) represents a deterministic process. This term is also present in the formulation of the PFODE.</p>
  </li>
  <li>
    <p><strong>Deterministic Noise Decay</strong>: \(\pm\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t)) dt\)
corresponds to the deterministic decay of noise. Here, \(\beta(t)\) controls the rate at which noise is reduced over time. This term needs to point in the direction of higher probability density (i.e., the score direction). Therefore, when \(dt\) is positive, it’s a forward process, the sign in front is \(+\), and when \(dt\) is negative, it’s a reverse process and the sign in front is \(-\)</p>
  </li>
  <li>
    <p><strong>Stochastic Noise Injection</strong>: \(\sqrt{2\beta(t)}\sigma(t) dw_{t}\)
represents the noise injection. This term introduces randomness into the system, modeled by the Wiener process \(dw_{t}\).</p>
  </li>
</ul>

<p>The second term can be interpreted as denoising, where noise is systematically removed from the process. The third term, on the other hand, adds noise back into the system. Thus, the equation can be viewed as a process where, for each deterministic ODE, we first perform denoising and then reintroduce random noise. The relative speed at which denoising and noise addition occur is governed by \(\beta(t)\).</p>

<p>In conclusion,</p>

\[dx_{\pm}=-\underbrace{\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t))dt}_{\text{PFODE}}\;
\underbrace{\pm\;\underbrace{\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t))dt}_{\text{deterministic noise decay}}+\underbrace{\sqrt{2\beta(t)}\sigma(t)dw_{t}}_{\text{noise injection}}}_{\text{Langevin diffusion SDE}}\]

<p>2.when \(g(t)=0\), the SDE becomes an <strong>Ordinary Differential Equation (ODE)</strong>, called the <strong>Probability Flow ODE</strong> <strong>(PFODE)</strong>:</p>

<p>Remember, PFODE is a deterministic process:</p>

\[d\mathbf{x}_t = -\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t)) dt\]

<p>Interestingly, this equation lets us denoise without needing to directly solve for \(f(t)\) and \(g(t)\). By knowing only \(s(t)\) and \(\sigma(t)\), we can effectively denoise and sample high-quality images.</p>

<p>A common question is: <strong>Why isn’t there a forward PFODE process?</strong></p>

<p>The answer is simple. PFODE is designed for sampling, not for the forward process. Since PFODE is a fully deterministic process, it cannot model a data distribution without injecting noise. Our objective is to model the distribution of data, and noise injection is essential for this. Because PFODE cannot introduce stochasticity into the forward process, the neural network is unable to learn any distribution from it.</p>

<p>In practice, instead of explicitly computing the score function \(\nabla_{x_t} \log p_t(x_t)\), we approximate it with a neural network \(D_\theta\).</p>

\[\mathrm{d} \mathbf{x}_t = \left[ \left( \frac{\dot{s}(t)}{s(t)} + \frac{\dot{\sigma}(t)}{\sigma(t)} \right) \mathbf{x}_t - \frac{s(t) \, \dot{\sigma}(t)}{\sigma(t)} D_\theta \left( \frac{\mathbf{x}_t}{s(t)} ; \sigma \right) \right] \, dt\]

<p>The score function can also be derived from a neural network prediction:</p>

\[\nabla_{\mathbf{x}} \log p(\mathbf{x}; \sigma) = \frac{D_{\theta}(\mathbf{x}; \sigma) - \mathbf{x}}{\sigma^2}\]

<p>Now, we have a general PFODE equation that models the reverse process. After training a neural network to approximate \(D_\theta (\mathbf{x}_t; \sigma(t))\), we can sample from noise by simply solving the ODE.</p>

<p>During reverse process, the noise schedule could also influence the sampling quality. An emperical noise scheduler equation is as below:</p>

\[\sigma_{i &lt; N} = \left( \sigma_{\text{max}}^{\frac{1}{\rho}} + \frac{i}{N-1} \left( \sigma_{\text{min}}^{\frac{1}{\rho}} - \sigma_{\text{max}}^{\frac{1}{\rho}} \right) \right)^{\rho} \quad \text{and} \quad \sigma_N = 0\]

<p>Finally, to accurately sample from the distribution, we use <strong>2nd-order Heun’s method</strong> rather than the simpler Euler method (1st-order). Heun’s method reduces sampling error by averaging the initial and predicted values of the function, providing a more stable path for denoising:</p>

\[x_{t + \Delta t} = x_t + \frac{1}{2} (f(x_t, t) + f(x_{t + \Delta t}, t + \Delta t)) \Delta t\]

<p>However, since Heun’s method requires knowing both \(f(x_t, t)\) and \(f(x_{t + \Delta t}, t + \Delta t)\), we still use an <strong>Euler step</strong> to make an initial estimate before applying Heun’s correction.</p>

<h3 id="general-model-design">General Model Design</h3>

<p>The score function \(D_\theta \left( \frac{\mathbf{x}_t}{s(t)} ; \sigma \right)\) is to predict how the noisy image should be transformed back towards the clean image.</p>

<p>In the case of <strong>DDPM</strong>:</p>

\[D_\theta (\mathbf{x}_t; \sigma(t)) \approx \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \, \varepsilon}{\sqrt{\bar{\alpha}_t}} \approx \frac{1}{\sqrt{\bar{\alpha}_t}} \mathbf{x}_t - \frac{\sqrt{1 - \bar{\alpha}_t}}{\sqrt{\bar{\alpha}_t}} \, \varepsilon_\theta (\mathbf{x}_t, t)\]

<p>In <strong>Score Matching</strong>:</p>

\[D_\theta (\mathbf{x}; \sigma) \approx \mathbf{x} + \sigma^2 s_\theta (\mathbf{x}; \sigma)\]

<p>In Flow Matching:</p>

\[D_\theta (\mathbf{x}_t; \sigma(t)) \approx \mathbf{x}_t + (1 - t) v_\theta (\mathbf{x}_t, t)\]

<p>Although DDPM, Score Matching, and Flow Matching have different formulations, their forward process can be unified into one general equation:</p>

\[D_\theta (\hat{\mathbf{x}}; \sigma) = C_{\text{skip}}(\sigma) \, \hat{\mathbf{x}} + C_{\text{out}}(\sigma) F_\theta \left( C_{\text{in}}(\sigma) \, \hat{\mathbf{x}}; C_{\text{noise}}(\sigma) \right)\]

<p>This equation introduces several new terms, each playing a key role in aligning the three approaches. Let’s break these down:</p>

<ul>
  <li>
    <p><strong>What is \(\hat{\mathbf{x}}\)</strong>?<br />
To unify the input pixel range across the models, we convert the image from the noisy range \([-s(t), s(t)]\) to a normalized range of \([-1, 1]\). The term \(\hat{\mathbf{x}}\) represents this normalized version of the input image, making it easier to handle across different processes. Given the noise schedule, any image \(\mathbf{x} = s(t) \hat{\mathbf{x}}\) will have its pixel range scaled to \([-s(t), s(t)]\), and thus \(\hat{\mathbf{x}}\) allows us to operate within a consistent range for training.</p>
  </li>
  <li>
    <p><strong>What is \(C_{\text{in}}(\sigma)\)?</strong><br />
The term \(C_{\text{in}}(\sigma)\) scales the input image before it passes through the neural network. In the case of <strong>DDPM</strong> and <strong>Flow Matching</strong>, the input images at different time steps \(t\) have ranges from \([-s(t), s(t)]\). These terms ensure that the input is correctly scaled before passing into the model’s score network, where \(s(t)\) may change depending on the chosen process (e.g., DDPM’s \(s(t) = \sqrt{\bar{\alpha}_t}\) or FM’s \(s(t) = t\)).</p>
  </li>
  <li>
    <p><strong>What is \(F_{\theta}()\)?</strong></p>

    <p>The neural network can be a U-Net or a DiT. The proxy target can be predicting noise, \(x_0\) or velocity</p>
  </li>
</ul>

<p>In <a href="https://github.com/NVIDIA/Cosmos/blob/c47b35b7618a6e263556f3e3fb7cfba3705c08a5/cosmos1/models/diffusion/diffusion/modules/denoiser_scaling.py">Cosmos</a>, the pararmeters of the forward function is implemented as:</p>

<p>\(c_{\text{skip}} = \frac{\sigma_{\text{data}}^2}{\sigma^2 + \sigma_{\text{data}}^2}\), \(c_{\text{out}} = \frac{\sigma \cdot \sigma_{\text{data}}}{\sqrt{\sigma^2 + \sigma_{\text{data}}^2}}\), \(c_{\text{in}} = \frac{1}{\sqrt{\sigma^2 + \sigma_{\text{data}}^2}}\) , \(c_{\text{noise}} = 0.25 \cdot \log(\sigma)\)</p>

<hr />

<h2 id="ddpm">DDPM</h2>

<p>DDPM is one of the most well-studied diffusion processes.</p>

<h3 id="forward-process">Forward Process</h3>

<p>In the original paper, the forward process is defined as:</p>

\[x_t = \sqrt{1-\beta_t} x_{t-1} + \sqrt{\beta_t} \varepsilon\]

<p>This can also be expressed as:</p>

\[x_t = \sqrt{\bar{\alpha_t}} x_{\text{data}} + \sqrt{1 - \bar{\alpha_t}} \varepsilon\]

<p>Where \(\alpha_t := 1 - \beta_t\) and \(\bar{\alpha}_t := \prod_{s=1}^t \alpha_s\)</p>

<p>In this formulation, \(\beta_t\) represents the noise variance at time step \(t\), and it can either be a learnable parameter, a value from a cosine scheduler, or defined using simple schedules like a linear or sigmoid scheduler.</p>

<h3 id="noise-scheduler-function">Noise Scheduler Function</h3>

<p>Several noise schedulers have been proposed for DDPM. High-resolution images/ videos have much higher redundancy, so their SNR is higher after noise is added, and the noise scheduler need to be adjusted / shifted.</p>

<ul>
  <li>
    <p>Original Cosine Noise Scheduler</p>
  </li>
  <li>
    <p>Linear Noise scheduler <a href="https://arxiv.org/pdf/2301.10972">\(^{\text{sec 2.1 page 2}}\)</a></p>
  </li>
</ul>

<p>The forward process can be written in a general format:</p>

\[x_t = \sqrt{ 1 - t }\ x_{\text{data}}+ \sqrt{t}\ \varepsilon\]

<p>This looks pretty like flow matching method, but it is not. Note there is a square rood of the \(t\).</p>

<ul>
  <li>offset-noise <a href="https://www.crosslabs.org/blog/diffusion-with-offset-noise">\(^{\text{link}}\)</a></li>
</ul>

<p>Add noise that looks like an iid sample per pixel added to a single iid sample that is the same over the entire image.</p>

<ul>
  <li>time-shift <a href="https://arxiv.org/pdf/2301.10972">\(^{\text{page 34, Time Shifting}}\)</a></li>
</ul>

<h3 id="loss-type">Loss Type</h3>

<p>The loss function for DDPM has been extensively studied:</p>

<h4 id="predict-x_textdata">Predict \(x_{\text{data}}\)</h4>

<p>\(x_{\text{data}}\) prediction is problematic at low noise levels, because \(x_{\text{data}}\)  as a target is not informative when added noise is small.</p>

<h4 id="predict-noise">Predict Noise</h4>

<p>Noise prediction can be problematic at high noise levels, because any error in will get amplified in</p>

\[x_{\text{pred}} = (x_t - \sqrt{1 − \bar{\gamma_t}}\ \varepsilon) / (\sqrt{\bar{\alpha_t}})\]

<p>as  \(\sqrt{\bar{\alpha_t}}\) is close to 0. It means that small changes create a large loss under some weightings. <a href="https://diffusionflow.github.io/">\(^{\text{sec Training}}\)</a></p>

<h4 id="predict-velocity">Predict Velocity</h4>

<p>V-Pred is first introduced in <a href="https://arxiv.org/pdf/2202.00512">\(^{\text{page 14, Appendix D}}\)</a></p>

<p>V-Pred is the most stable option, as it has the unique property of making DDIM
step-sizes <mark>independent of the SNR.</mark></p>

<p>(note: this is different from the velocity defined in the flow matching, so we denote it as V-Pred to differ from Flow Velocity Predicition)</p>

\[v := \sqrt{\bar{\alpha_t}} \epsilon - \sqrt{1-\bar{\alpha_t}} x_{\text{data}}\]

<p>V-Pred being a specific case of velocity prediction under the VP path/scheduler, differing only by a scaling/weighting coefficient dependent on time .</p>

<p>If we represent the forward equation as:</p>

\[\begin{equation} x_t = \alpha_t \cdot x_{\text{data}} + \sigma_t \cdot x_{\text{noise}} \end{equation}\]

<p>As for the Variance Preservation Condition, we always have:</p>

\[\begin{equation} \alpha_t^2 + \sigma_t^2 = 1\end{equation}\]

<p>The <strong>Flow Velocity Prediction</strong> is defined as :</p>

\[\begin{equation}u_t = \frac{dx_t}{dt} = \dot{\alpha_t} \cdot x_{\text{data}} + \dot{\sigma_t} \cdot x_{\text{noise}} \end{equation}\]

<p>Taking the derivative of Equation (2), we get:</p>

<p>we get:
\(\alpha_t \dot{\alpha_t} + \sigma_t \dot{\sigma_t} = 0\)</p>

<p>which implies that:</p>

\[\dot{\sigma_t} = - \frac{\alpha_t \dot{\alpha_t}}{\sigma_t}\]

<p>Substituting into Equation (3) for \(u_t​\), we get:</p>

\[u_t = \dot{\alpha_t} \cdot x_{\text{data}} - \frac{\alpha_t \dot{\alpha_t}}{\sigma_t} \cdot x_{\text{noise}}\]

\[u_t = - \frac{\dot{\alpha_t}}{\sigma_t} (\alpha_t \cdot x_{\text{noise}} - \sigma_t \cdot x_{\text{data}}) = - \frac{\dot{\alpha_t}}{\sigma_t} v_t\]

<p>All these loss types can be converted into one another.</p>

<p>let’s focus on velocity prediction, where the neural network predicts the velocity \(v_{pred}\).  This has been used as an optimal loss type for effective training. However, rather than using the predicted velocity directly to calculate the loss, we convert it into either noise or 
\(x_{\text{data}}\)  and compute the loss based on the converted types.</p>

<ul>
  <li><strong>Convert to Predicted Noise \(\epsilon_{pred}\)</strong> <a href="https://arxiv.org/pdf/2301.11093">\(^{\text{ref-Appendix A, page 12}}\)</a>; <a href="https://github.com/huggingface/diffusers/blob/6a89a6c93ae38927097f5181030e3ceb7de7f43d/src/diffusers/schedulers/scheduling_ddim.py#L416-L429">\(^{\text{diffusers impl}}\)</a></li>
</ul>

\[\epsilon_{pred} = \sqrt{\bar{\alpha_t}} v_{pred} + \sqrt{1-\bar{\alpha_t}} x_t\]

<p>The MSE loss we use is:</p>

\[\bar{\alpha_t} \text{MSE}(v_{pred}, v) = \text{MSE}(\varepsilon_{pred}, \varepsilon)\]

<ul>
  <li><strong>Convert to Predict \(x_{data}\)</strong></li>
</ul>

\[\hat{x_0} = \sqrt{\bar{\alpha_t}} x_t - \sqrt{1-\bar{\alpha_t}} v_{pred}\]

<p>The MSE loss we use is:</p>

\[(1-\bar{\alpha_t}) \text{MSE}(v_{pred}, v) = \text{MSE}(x_{data}, x_{pred})\]

<hr />

<h2 id="score-matching">Score Matching</h2>

<h3 id="forward-process-1">Forward Process</h3>

<p>The forward process in score matching is:</p>

\[x_t = x_{data} + \sigma_t \varepsilon\]

<p>Here,  \(x_{data} \sim p_{data}(x)\) , where  \(p_{data}(x)\) represents the distribution of the training dataset. The noise variance decreases over time:</p>

\[\sigma_1 &gt; \sigma_2 &gt; \sigma_3 &gt; \dots\]

<p>This means the noise variance added to the data gradually decreases. The corresponding ODE is:</p>

\[dx_t = \sqrt{\frac{d\sigma^2_t}{dt}} d\bar{\mathbf{w}}\]

<p>The forward process can be imagined a straight line going from data to noise where the velocity (variance of noise) gradually decreases from large to small.</p>

<h3 id="reverse-process">Reverse Process</h3>

\[d\mathbf{x_t} = -\left(\frac{d[\sigma(t)^2]}{dt} \nabla_{\mathbf{x}} \log p(\mathbf{x_t}) \right) dt + \sqrt{\frac{d[\sigma(t)^2]}{dt}} d\bar{\mathbf{w}}\]

<p>The reverse sampling follows the Langevin equation:</p>

\[x_{t+1} = x_t + \tau \nabla_x \log p(x_t) + \sqrt{2\tau} z\]

<p>where  \(z \sim N(0, I)\)</p>

<p>We can see from the sampling equation that although the forward process is linear, the reverse process is stochastic, which makes score-matching sampling hard to hack.</p>

<h3 id="loss-type-1">Loss Type</h3>

<ul>
  <li>Noise Conditional Score Matching <a href="https://arxiv.org/pdf/2403.18103">\(^{\text{Theorem 3.3}}\)</a></li>
</ul>

\[J_{\text{NCSM}}(\theta, \sigma_i) = \sum_i^{L} \lambda_i \;\mathbb{E}_{p(\mathbf{x})} \left[ \frac{1}{2} \left\| s_\theta(x_{data} + \sigma_i \varepsilon) + \frac{\varepsilon}{\sigma_i} \right\|^2 \right]\]

<p>Note this is the loss function when we use score matching forward process to add noise progressively.</p>

<p>If we use DDPM noise scheduler, which means \(x_t = \sqrt{\bar{\alpha_t}} x_{data} + \sqrt{1 - \bar{\alpha_t}} \varepsilon\) holds true,</p>

<p>Then, the score can be approximated by \(\varepsilon\)</p>

\[\nabla_x \log p(x_t \|x_{data}) = -\frac{x_t-\sqrt{ \bar{ \alpha_t} } x_{data}}{1-\bar{\alpha_t}} = -\frac{\varepsilon}{\sqrt{1-\bar{\alpha_t}}}\]

<p>We can claim that:</p>

<p>\(x_t = \sqrt{\bar{\alpha_t}} x_{data} - (1 - \bar{\alpha_t}) s_{\theta}\)  holds true. \(s_{\theta}\) is the score predicted by neural network.</p>

<hr />

<h2 id="flow-matching">Flow Matching</h2>

<h3 id="forward-process-2">Forward Process</h3>

<p>In flow matching, the forward process is:</p>

<p>\(x_t =  (1 - t) x_{data} + t \varepsilon = a_t x_{data} + b_t \varepsilon\), where \(t \in [0, 1]\)</p>

<p>Flow matching can be regarded as a uniform linear motion between data and noise.</p>

<h3 id="reverse-process-1">Reverse Process</h3>

<p>The reverse ODE is:</p>

\[\frac{dx_t}{dt} = \varepsilon - x_{data} = v_t(x)\]

<p>You can solve this ODE using Euler’s method. An interesting fact is that the direction of the velocity in Rectified Flow is from noise to data; whereas in DDPM-v-pred, it is from data to noise.</p>

<h3 id="loss-type-2">Loss Type</h3>

<p>The objective function for flow matching <a href="https://arxiv.org/pdf/2403.03206">\(^{\text{ref-Section 2}}\)</a>;  <a href="https://arxiv.org/pdf/2210.02747">\(^{\text{ref-Theorem 3}}\)</a>  is:</p>

\[\text{MSE}(v(x_{data}, t) - u(x_t| \varepsilon))\]

<p>where \(u(x_t \| \varepsilon) = \frac{a'_t}{a_t} x_t - \frac{b_t}{2} \left(\log \frac{a^2_t}{b^2_t}\right)' \varepsilon\)</p>]]></content><author><name>Norm Inui</name></author><category term="Diffusion" /><summary type="html"><![CDATA[This is a cheat sheet of all denoising-based diffusion method. No mathematics derivations are included for concise. Check the reference links if you’re interested in the derivations. EDM introduces a general denoising equation that nearly unifies all widely recognized denoising and noise-adding processes within its framework. A Unified Perspective General Forward Process Noise-adding Process is also known as forward process. The general form of this Forward Process is: \[p(x_t | x_0) = \mathcal{N}(x_t; s(t)x_0, \sigma^2(t) s^2(t) \mathbf{I})\] This equation describes how noise is gradually added to an image, represented as \(x_0\), over time steps \(t\). The two functions, \(s(t)\) and \(\sigma(t)\), control the scale and variance of the noise, determining the trajectory of noise addition as \(x_t\) moves further from the original image. General Reverse Process The Forward Process equation above can be written into a Stochastic Differential Equation (SDE) format as: \[dx_t = f(x_t, t) \, dt + g(t) \, d\mathbf{w}\] Here, \(s(t) = e^{\int_{0}^{t} f(r) \, dr}\) and \(\sigma^2(t) = \int_{0}^{t} \frac{g^2(r)}{s^2(r)} \, dr\) \(d\mathbf{w}\) represents a Wiener process, where \(\mathbf{w_t} \sim \mathcal{N}(0, t)\); \(d\mathbf{w} = \sqrt{dt} \; \epsilon\) , where \(\epsilon \sim \mathcal{N}(\mu, \sigma^2)\) Different methods define their own specific functions for \(f(x_t, t)\) and \(g(t)\). For example, in diffusion models like DDPM and SMLD (Score Matching with Langevin Dynamics), \(f(x_t, t)\) is typically a linear function, \(f(x_t, t) = f(t)x_t\), where \(f(t)\) is a time-dependent term that modulates the trajectory of the image’s transformation over time. EDM simply set \(s(t) = 1, \, \sigma(t) = t\). Now we have a forward SDE to corrupt a data into noise. To reverse such as process, we have two choices: x_data --&gt;|Forward SDE| --&gt; x_noise x_noise --&gt;|Reverse SDE| --&gt; x_data x_noise --&gt;|PFODE| --&gt; x_data Using the Fokker-Planck equation, we can transform the forward SDE as: \[dx = \left(\frac{1}{2}g^{2}(t) - \dot{\sigma}(t)\sigma(t)\right) \nabla_x \log p(x;\sigma(t)) dt + g(t) dw_t\] 1.when \(g(t) = \sqrt{2\beta(t)} \sigma(t)\) \[dx_{\pm}=-\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t))dt\pm\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t))dt+\sqrt{2\beta(t)}\sigma(t)dw_{t}\] the \(dx_+\) here means forward process; the \(dx_-\) here means reverse process. The equation consists of three key parts: Deterministic Process: \(-\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t)) dt\) represents a deterministic process. This term is also present in the formulation of the PFODE. Deterministic Noise Decay: \(\pm\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t)) dt\) corresponds to the deterministic decay of noise. Here, \(\beta(t)\) controls the rate at which noise is reduced over time. This term needs to point in the direction of higher probability density (i.e., the score direction). Therefore, when \(dt\) is positive, it’s a forward process, the sign in front is \(+\), and when \(dt\) is negative, it’s a reverse process and the sign in front is \(-\) Stochastic Noise Injection: \(\sqrt{2\beta(t)}\sigma(t) dw_{t}\) represents the noise injection. This term introduces randomness into the system, modeled by the Wiener process \(dw_{t}\). The second term can be interpreted as denoising, where noise is systematically removed from the process. The third term, on the other hand, adds noise back into the system. Thus, the equation can be viewed as a process where, for each deterministic ODE, we first perform denoising and then reintroduce random noise. The relative speed at which denoising and noise addition occur is governed by \(\beta(t)\). In conclusion, \[dx_{\pm}=-\underbrace{\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t))dt}_{\text{PFODE}}\; \underbrace{\pm\;\underbrace{\beta(t)\sigma^{2}(t)\nabla_{x}\log p(x;\sigma(t))dt}_{\text{deterministic noise decay}}+\underbrace{\sqrt{2\beta(t)}\sigma(t)dw_{t}}_{\text{noise injection}}}_{\text{Langevin diffusion SDE}}\] 2.when \(g(t)=0\), the SDE becomes an Ordinary Differential Equation (ODE), called the Probability Flow ODE (PFODE): Remember, PFODE is a deterministic process: \[d\mathbf{x}_t = -\dot{\sigma}(t)\sigma(t)\nabla_{x}\log p(x;\sigma(t)) dt\] Interestingly, this equation lets us denoise without needing to directly solve for \(f(t)\) and \(g(t)\). By knowing only \(s(t)\) and \(\sigma(t)\), we can effectively denoise and sample high-quality images. A common question is: Why isn’t there a forward PFODE process? The answer is simple. PFODE is designed for sampling, not for the forward process. Since PFODE is a fully deterministic process, it cannot model a data distribution without injecting noise. Our objective is to model the distribution of data, and noise injection is essential for this. Because PFODE cannot introduce stochasticity into the forward process, the neural network is unable to learn any distribution from it. In practice, instead of explicitly computing the score function \(\nabla_{x_t} \log p_t(x_t)\), we approximate it with a neural network \(D_\theta\). \[\mathrm{d} \mathbf{x}_t = \left[ \left( \frac{\dot{s}(t)}{s(t)} + \frac{\dot{\sigma}(t)}{\sigma(t)} \right) \mathbf{x}_t - \frac{s(t) \, \dot{\sigma}(t)}{\sigma(t)} D_\theta \left( \frac{\mathbf{x}_t}{s(t)} ; \sigma \right) \right] \, dt\] The score function can also be derived from a neural network prediction: \[\nabla_{\mathbf{x}} \log p(\mathbf{x}; \sigma) = \frac{D_{\theta}(\mathbf{x}; \sigma) - \mathbf{x}}{\sigma^2}\] Now, we have a general PFODE equation that models the reverse process. After training a neural network to approximate \(D_\theta (\mathbf{x}_t; \sigma(t))\), we can sample from noise by simply solving the ODE. During reverse process, the noise schedule could also influence the sampling quality. An emperical noise scheduler equation is as below: \[\sigma_{i &lt; N} = \left( \sigma_{\text{max}}^{\frac{1}{\rho}} + \frac{i}{N-1} \left( \sigma_{\text{min}}^{\frac{1}{\rho}} - \sigma_{\text{max}}^{\frac{1}{\rho}} \right) \right)^{\rho} \quad \text{and} \quad \sigma_N = 0\] Finally, to accurately sample from the distribution, we use 2nd-order Heun’s method rather than the simpler Euler method (1st-order). Heun’s method reduces sampling error by averaging the initial and predicted values of the function, providing a more stable path for denoising: \[x_{t + \Delta t} = x_t + \frac{1}{2} (f(x_t, t) + f(x_{t + \Delta t}, t + \Delta t)) \Delta t\] However, since Heun’s method requires knowing both \(f(x_t, t)\) and \(f(x_{t + \Delta t}, t + \Delta t)\), we still use an Euler step to make an initial estimate before applying Heun’s correction. General Model Design The score function \(D_\theta \left( \frac{\mathbf{x}_t}{s(t)} ; \sigma \right)\) is to predict how the noisy image should be transformed back towards the clean image. In the case of DDPM: \[D_\theta (\mathbf{x}_t; \sigma(t)) \approx \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \, \varepsilon}{\sqrt{\bar{\alpha}_t}} \approx \frac{1}{\sqrt{\bar{\alpha}_t}} \mathbf{x}_t - \frac{\sqrt{1 - \bar{\alpha}_t}}{\sqrt{\bar{\alpha}_t}} \, \varepsilon_\theta (\mathbf{x}_t, t)\] In Score Matching: \[D_\theta (\mathbf{x}; \sigma) \approx \mathbf{x} + \sigma^2 s_\theta (\mathbf{x}; \sigma)\] In Flow Matching: \[D_\theta (\mathbf{x}_t; \sigma(t)) \approx \mathbf{x}_t + (1 - t) v_\theta (\mathbf{x}_t, t)\] Although DDPM, Score Matching, and Flow Matching have different formulations, their forward process can be unified into one general equation: \[D_\theta (\hat{\mathbf{x}}; \sigma) = C_{\text{skip}}(\sigma) \, \hat{\mathbf{x}} + C_{\text{out}}(\sigma) F_\theta \left( C_{\text{in}}(\sigma) \, \hat{\mathbf{x}}; C_{\text{noise}}(\sigma) \right)\] This equation introduces several new terms, each playing a key role in aligning the three approaches. Let’s break these down: What is \(\hat{\mathbf{x}}\)? To unify the input pixel range across the models, we convert the image from the noisy range \([-s(t), s(t)]\) to a normalized range of \([-1, 1]\). The term \(\hat{\mathbf{x}}\) represents this normalized version of the input image, making it easier to handle across different processes. Given the noise schedule, any image \(\mathbf{x} = s(t) \hat{\mathbf{x}}\) will have its pixel range scaled to \([-s(t), s(t)]\), and thus \(\hat{\mathbf{x}}\) allows us to operate within a consistent range for training. What is \(C_{\text{in}}(\sigma)\)? The term \(C_{\text{in}}(\sigma)\) scales the input image before it passes through the neural network. In the case of DDPM and Flow Matching, the input images at different time steps \(t\) have ranges from \([-s(t), s(t)]\). These terms ensure that the input is correctly scaled before passing into the model’s score network, where \(s(t)\) may change depending on the chosen process (e.g., DDPM’s \(s(t) = \sqrt{\bar{\alpha}_t}\) or FM’s \(s(t) = t\)). What is \(F_{\theta}()\)? The neural network can be a U-Net or a DiT. The proxy target can be predicting noise, \(x_0\) or velocity In Cosmos, the pararmeters of the forward function is implemented as: \(c_{\text{skip}} = \frac{\sigma_{\text{data}}^2}{\sigma^2 + \sigma_{\text{data}}^2}\), \(c_{\text{out}} = \frac{\sigma \cdot \sigma_{\text{data}}}{\sqrt{\sigma^2 + \sigma_{\text{data}}^2}}\), \(c_{\text{in}} = \frac{1}{\sqrt{\sigma^2 + \sigma_{\text{data}}^2}}\) , \(c_{\text{noise}} = 0.25 \cdot \log(\sigma)\) DDPM DDPM is one of the most well-studied diffusion processes. Forward Process In the original paper, the forward process is defined as: \[x_t = \sqrt{1-\beta_t} x_{t-1} + \sqrt{\beta_t} \varepsilon\] This can also be expressed as: \[x_t = \sqrt{\bar{\alpha_t}} x_{\text{data}} + \sqrt{1 - \bar{\alpha_t}} \varepsilon\] Where \(\alpha_t := 1 - \beta_t\) and \(\bar{\alpha}_t := \prod_{s=1}^t \alpha_s\) In this formulation, \(\beta_t\) represents the noise variance at time step \(t\), and it can either be a learnable parameter, a value from a cosine scheduler, or defined using simple schedules like a linear or sigmoid scheduler. Noise Scheduler Function Several noise schedulers have been proposed for DDPM. High-resolution images/ videos have much higher redundancy, so their SNR is higher after noise is added, and the noise scheduler need to be adjusted / shifted. Original Cosine Noise Scheduler Linear Noise scheduler \(^{\text{sec 2.1 page 2}}\) The forward process can be written in a general format: \[x_t = \sqrt{ 1 - t }\ x_{\text{data}}+ \sqrt{t}\ \varepsilon\] This looks pretty like flow matching method, but it is not. Note there is a square rood of the \(t\). offset-noise \(^{\text{link}}\) Add noise that looks like an iid sample per pixel added to a single iid sample that is the same over the entire image. time-shift \(^{\text{page 34, Time Shifting}}\) Loss Type The loss function for DDPM has been extensively studied: Predict \(x_{\text{data}}\) \(x_{\text{data}}\) prediction is problematic at low noise levels, because \(x_{\text{data}}\) as a target is not informative when added noise is small. Predict Noise Noise prediction can be problematic at high noise levels, because any error in will get amplified in \[x_{\text{pred}} = (x_t - \sqrt{1 − \bar{\gamma_t}}\ \varepsilon) / (\sqrt{\bar{\alpha_t}})\] as \(\sqrt{\bar{\alpha_t}}\) is close to 0. It means that small changes create a large loss under some weightings. \(^{\text{sec Training}}\) Predict Velocity V-Pred is first introduced in \(^{\text{page 14, Appendix D}}\) V-Pred is the most stable option, as it has the unique property of making DDIM step-sizes independent of the SNR. (note: this is different from the velocity defined in the flow matching, so we denote it as V-Pred to differ from Flow Velocity Predicition) \[v := \sqrt{\bar{\alpha_t}} \epsilon - \sqrt{1-\bar{\alpha_t}} x_{\text{data}}\] V-Pred being a specific case of velocity prediction under the VP path/scheduler, differing only by a scaling/weighting coefficient dependent on time . If we represent the forward equation as: \[\begin{equation} x_t = \alpha_t \cdot x_{\text{data}} + \sigma_t \cdot x_{\text{noise}} \end{equation}\] As for the Variance Preservation Condition, we always have: \[\begin{equation} \alpha_t^2 + \sigma_t^2 = 1\end{equation}\] The Flow Velocity Prediction is defined as : \[\begin{equation}u_t = \frac{dx_t}{dt} = \dot{\alpha_t} \cdot x_{\text{data}} + \dot{\sigma_t} \cdot x_{\text{noise}} \end{equation}\] Taking the derivative of Equation (2), we get: we get: \(\alpha_t \dot{\alpha_t} + \sigma_t \dot{\sigma_t} = 0\) which implies that: \[\dot{\sigma_t} = - \frac{\alpha_t \dot{\alpha_t}}{\sigma_t}\] Substituting into Equation (3) for \(u_t​\), we get: \[u_t = \dot{\alpha_t} \cdot x_{\text{data}} - \frac{\alpha_t \dot{\alpha_t}}{\sigma_t} \cdot x_{\text{noise}}\] \[u_t = - \frac{\dot{\alpha_t}}{\sigma_t} (\alpha_t \cdot x_{\text{noise}} - \sigma_t \cdot x_{\text{data}}) = - \frac{\dot{\alpha_t}}{\sigma_t} v_t\] All these loss types can be converted into one another. let’s focus on velocity prediction, where the neural network predicts the velocity \(v_{pred}\). This has been used as an optimal loss type for effective training. However, rather than using the predicted velocity directly to calculate the loss, we convert it into either noise or \(x_{\text{data}}\) and compute the loss based on the converted types. Convert to Predicted Noise \(\epsilon_{pred}\) \(^{\text{ref-Appendix A, page 12}}\); \(^{\text{diffusers impl}}\) \[\epsilon_{pred} = \sqrt{\bar{\alpha_t}} v_{pred} + \sqrt{1-\bar{\alpha_t}} x_t\] The MSE loss we use is: \[\bar{\alpha_t} \text{MSE}(v_{pred}, v) = \text{MSE}(\varepsilon_{pred}, \varepsilon)\] Convert to Predict \(x_{data}\) \[\hat{x_0} = \sqrt{\bar{\alpha_t}} x_t - \sqrt{1-\bar{\alpha_t}} v_{pred}\] The MSE loss we use is: \[(1-\bar{\alpha_t}) \text{MSE}(v_{pred}, v) = \text{MSE}(x_{data}, x_{pred})\] Score Matching Forward Process The forward process in score matching is: \[x_t = x_{data} + \sigma_t \varepsilon\] Here, \(x_{data} \sim p_{data}(x)\) , where \(p_{data}(x)\) represents the distribution of the training dataset. The noise variance decreases over time: \[\sigma_1 &gt; \sigma_2 &gt; \sigma_3 &gt; \dots\] This means the noise variance added to the data gradually decreases. The corresponding ODE is: \[dx_t = \sqrt{\frac{d\sigma^2_t}{dt}} d\bar{\mathbf{w}}\] The forward process can be imagined a straight line going from data to noise where the velocity (variance of noise) gradually decreases from large to small. Reverse Process \[d\mathbf{x_t} = -\left(\frac{d[\sigma(t)^2]}{dt} \nabla_{\mathbf{x}} \log p(\mathbf{x_t}) \right) dt + \sqrt{\frac{d[\sigma(t)^2]}{dt}} d\bar{\mathbf{w}}\] The reverse sampling follows the Langevin equation: \[x_{t+1} = x_t + \tau \nabla_x \log p(x_t) + \sqrt{2\tau} z\] where \(z \sim N(0, I)\) We can see from the sampling equation that although the forward process is linear, the reverse process is stochastic, which makes score-matching sampling hard to hack. Loss Type Noise Conditional Score Matching \(^{\text{Theorem 3.3}}\) \[J_{\text{NCSM}}(\theta, \sigma_i) = \sum_i^{L} \lambda_i \;\mathbb{E}_{p(\mathbf{x})} \left[ \frac{1}{2} \left\| s_\theta(x_{data} + \sigma_i \varepsilon) + \frac{\varepsilon}{\sigma_i} \right\|^2 \right]\] Note this is the loss function when we use score matching forward process to add noise progressively. If we use DDPM noise scheduler, which means \(x_t = \sqrt{\bar{\alpha_t}} x_{data} + \sqrt{1 - \bar{\alpha_t}} \varepsilon\) holds true, Then, the score can be approximated by \(\varepsilon\) \[\nabla_x \log p(x_t \|x_{data}) = -\frac{x_t-\sqrt{ \bar{ \alpha_t} } x_{data}}{1-\bar{\alpha_t}} = -\frac{\varepsilon}{\sqrt{1-\bar{\alpha_t}}}\] We can claim that: \(x_t = \sqrt{\bar{\alpha_t}} x_{data} - (1 - \bar{\alpha_t}) s_{\theta}\) holds true. \(s_{\theta}\) is the score predicted by neural network. Flow Matching Forward Process In flow matching, the forward process is: \(x_t = (1 - t) x_{data} + t \varepsilon = a_t x_{data} + b_t \varepsilon\), where \(t \in [0, 1]\) Flow matching can be regarded as a uniform linear motion between data and noise. Reverse Process The reverse ODE is: \[\frac{dx_t}{dt} = \varepsilon - x_{data} = v_t(x)\] You can solve this ODE using Euler’s method. An interesting fact is that the direction of the velocity in Rectified Flow is from noise to data; whereas in DDPM-v-pred, it is from data to noise. Loss Type The objective function for flow matching \(^{\text{ref-Section 2}}\); \(^{\text{ref-Theorem 3}}\) is: \[\text{MSE}(v(x_{data}, t) - u(x_t| \varepsilon))\] where \(u(x_t \| \varepsilon) = \frac{a'_t}{a_t} x_t - \frac{b_t}{2} \left(\log \frac{a^2_t}{b^2_t}\right)' \varepsilon\)]]></summary></entry><entry><title type="html">Build a Codebase from Scratch 🚧 (WIP)</title><link href="/codebase-post9/" rel="alternate" type="text/html" title="Build a Codebase from Scratch 🚧 (WIP)" /><published>2024-04-21T00:00:00+00:00</published><updated>2024-04-21T00:00:00+00:00</updated><id>/codebase-post9</id><content type="html" xml:base="/codebase-post9/"><![CDATA[<h3 class="no_toc"> TL; DR</h3>

<p>When starting a new project, I often ponder which codebase would be the best foundation. While the open-source community offers numerous impressive repositories, they often cater to specific demos and may not prioritize training or inference efficiency.</p>

<p>As a result, I’ve chosen to construct my codebase by drawing inspiration from several awesome open-source projects.</p>

<!--more-->

<p>The codebase should be designed with the following key characteristics:</p>

<ul>
  <li>
    <p><strong>Scalable:</strong> Supporting TP/DP/MP/PP</p>
  </li>
  <li>
    <p><strong>Reproducibility:</strong> Ensuring that results can be replicated precisely using the same configuration file.</p>
  </li>
  <li>
    <p><strong>Extensibility:</strong> Can easily integrate operators or modules from other codebases, such as Megatron-LM.</p>
  </li>
</ul>

<hr />

<h1 id="part1-dataset-and-data-stream">Part1. Dataset and Data Stream</h1>

<h2 id="parallel">Parallel</h2>

<p>MgLM has a very comprehensive <a href="https://github.com/NVIDIA/Megatron-LM/blob/ccfeda47cb5ca10ee3c4efd9b78c6bb15c2cd3d2/megatron/core/parallel_state.py#L310">documentations</a> about TP/CP/DP/MP.</p>

<p>The initialize_model_parallel function mentioned 3 use cases:</p>

<p>Let’s say we have a total of 16 GPUs denoted by g0 … g15</p>

<h4 id="data-parallel"><strong>Data Parallel</strong></h4>

<p>When DP=8, we arrange groups like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[g0, g2]
[g1, g3]
[g4, g6]
[g5, g7]
[g8, g10]
[g9, g11]
[g12, g14]
[g13, g15]
</code></pre></div></div>

<p>The arrangement indicates an alternating pattern where consecutive groups skip one GPU before pairing with the next. This pattern can be explained by two primary factors:</p>

<ol>
  <li>
    <p>In many multi-GPU setups, GPUs are interconnected in a way that adjacent GPUs (like g0 and g1) might share certain system resources (e.g., memory bandwidth, PCIe lanes). By pairing GPUs that are not directly adjacent (e.g., g0 and g2), it might be possible to optimize the usage of these shared resources, potentially reducing bottlenecks that occur when adjacent GPUs are used simultaneously for similar tasks.</p>
  </li>
  <li>
    <p>Alternating GPUs ensures a more uniform distribution of computational load across different parts of the GPU cluster.</p>
  </li>
</ol>

<h4 id="tensor-parallel"><strong>Tensor Parallel</strong></h4>

<p>When TP=8:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[g0, g1]
[g2, g3]
[g4, g5]
[g6, g7]
[g8, g9]
[g10, g11]
[g12, g13]
[g14, g15]
</code></pre></div></div>

<p>While tensor model-parallel groups have a more straightforward and intuitive pattern.</p>

<p>Tensor model parallelism involves splitting the model itself across multiple GPUs. Each GPU handles a part of the model’s computations. This is particularly useful for very large models that might not fit into the memory of a single GPU.</p>

<p>Adjacent GPUs often have faster or more direct communication paths between them. This can be due to their physical proximity on the motherboard or their direct connection via high-speed links like NVLink (in NVIDIA GPUs). Therefore, for tensor parallel groups, we arrange them using adjacent order.</p>

<h4 id="pipeline-parallel"><strong>Pipeline Parallel</strong></h4>

<p>When PP=4:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[g0, g4, g8, g12]
[g1, g5, g9, g13]
[g2, g6, g10, g14]
[g3, g7, g11, g15]
</code></pre></div></div>

<p>We arrange GPUs into 4  groups, ensuring that within each group, GPUs are not placed adjacent to one another. The reasons behind this practice are the same as the 8 data parallel groups.</p>

<h4 id="context-parallel"><strong>Context Parallel</strong></h4>

<p>It is a very interesting concept but lacks documentation. As for transformer-based models, the sequence length could be very long and a large sequence may not fit entirely within the memory of a single GPU, context parallelism is here used to split the input sequence length across multiple GPUs. However, unlike simpler data or model parallelism, context parallelism requires frequent communication among GPUs to share parts of the input sequence they are processing, because that is how the attention mechanism works.</p>

<p>This is critical because each part of the GPU cluster only sees a portion of the input, but computations (like calculating attention scores) require knowledge of the full input array. Therefore, a good practice of Context Group is composed of corresponding GPUs from other tensor parallel groups that handle different segments of the same sequence, which means each context parallel group contains one GPU from each tensor parallel group, ensuring that all segments of the sequence can be combined and communicated across the GPUs as needed.</p>

<p>For instance,</p>

<p><strong>Total GPUs</strong>: 8 (g0 to g7)
<strong>Context Parallel Size</strong>: 4</p>

<p><strong>Tensor Parallel Groups</strong>: Since context parallel size is 4, let’s assume we have 2 tensor parallel groups containing 4 GPUs each. Specifically, the tensor parallel groups are arranged as follows:</p>

<ul>
  <li>Group A: <code class="language-plaintext highlighter-rouge">[g0, g1]</code></li>
  <li>Group B: <code class="language-plaintext highlighter-rouge">[g2, g3]</code></li>
  <li>Group C: <code class="language-plaintext highlighter-rouge">[g4, g5]</code></li>
  <li>Group D: <code class="language-plaintext highlighter-rouge">[g6, g7]</code></li>
</ul>

<p>However, they are divided into 4 groups for context parallelism, each handling different segments of the data. Each context parallel group needs to contain one GPU from each tensor parallel group that corresponds to handling a portion of the sequence:</p>

<p><strong>Context Parallel</strong></p>

<ul>
  <li><strong>Group 1</strong>: Comprised of the first GPU from each tensor parallel group
<code class="language-plaintext highlighter-rouge">[g0, g2, g4, g6]</code></li>
  <li><strong>Group 2</strong>: Comprised of the second GPU from each tensor parallel group:
<code class="language-plaintext highlighter-rouge">[g1, g3, g5, g7]</code></li>
</ul>

<p>This setup ensures that for any given part of the input sequence, there is one GPU from each of the four context parallel groups that can communicate with GPUs from the other context parallel groups to exchange information about different parts of the sequence.</p>

<p>Each context parallel group can communicate within itself (g0 with g2, g4, g6, and so on) to share and gather information from the different segments of the data that each GPU processes.</p>

<h4 id="virtual-pipeline-parallel"><strong>Virtual Pipeline Parallel</strong></h4>

<p>If <strong>tensor_model_parallel_size is 1</strong>, <strong>pipeline_model_parallel_size is 4</strong>, <strong>virtual_pipeline_model_parallel_size is 2</strong>, and there are 16 transformer layers in the model, the model will be split into 8 stages with two layers each and each GPU would get 2 stages as such (layer number starting with 1):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        GPU 0: [1, 2] [9, 10]
        GPU 1: [3, 4] [11, 12]
        GPU 2: [5, 6] [13, 14]
        GPU 3: [7, 8] [15, 16]
</code></pre></div></div>

<p>Why do we need VP?
In standard pipeline parallelism, each GPU executes a fixed set of model layers and then remains idle while waiting for the next batch of data to process. This idle time arises because of dependencies between stages and the sequential nature of execution. Virtual pipeline model parallelism reduces this idle time by interleaving different segments of the workload across GPUs. This way, when one segment is waiting on data dependencies, another segment can be processed, thus keeping the GPUs busier.
Another reason is to reduce Bubble Time: Pipeline parallelism often suffers from “bubbles” or idle times, particularly when data is being passed between stages or during synchronization points. Virtual pipeline model parallelism can minimize these bubbles by ensuring that different stages are ready to execute as soon as they receive their inputs, thereby reducing the wait times that typically occur between stages.</p>

<h2 id="dataloader">Dataloader</h2>

<p>The dataset class should only handle data retrieval and define the  <code class="language-plaintext highlighter-rouge">__getitem__</code> method for various data formats, without being aware of any specific data or transformations required by the downstream tasks.</p>

<p>For instance, when utilizing the ImageNet dataset for downstream tasks such as classification and object detection, the required data formats vary significantly. For classification tasks, the expected format is (image_path, label), whereas for contrastive learning, it’s (image_path, box coordinates).</p>

<p>To prepare the data format that a task wants, I strongly suggest using MapDataset, a PyTorch hook-like style to post-process the data stream.</p>

<p>There are two types of dataset objects, a <a href="https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.Dataset">Dataset</a> and an <a href="https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.IterableDataset">IterableDataset</a>. Whichever type of dataset you choose to use or create depends on the size of the dataset. In general, an <code class="language-plaintext highlighter-rouge">IterableDataset</code> is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages.</p>

<p>As for <code class="language-plaintext highlighter-rouge">IterableDataset</code>, you can access it using a <code class="language-plaintext highlighter-rouge">for</code> loop to load the data progressively as you iterate over the dataset. This way, only a small fraction of examples is loaded in memory, and you don’t write anything on disk.</p>

<p>If your dataset grows very large, since regular Dataset objects are based on Arrow for random access to the rows, its indices mapping will become 10x slower. This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren’t reading contiguous chunks of data anymore. While an <code class="language-plaintext highlighter-rouge">IterableDataset</code> leverages its fast approximate shuffling method. It only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal.</p>

<p>Currently, iterable-style datasets are incompatible with customized samplers in <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code>.  Pytorch Dataloader always expects a map-style dataset. That is why we usually pass a sampler inside an iterable-style dataset for initialization. Specifically, please check the code gists in <a href="https://github.com/facebookresearch/detectron2/blob/a2e43eab54d28ffbd59f5e9b4e3193b82faeb70f/detectron2/data/common.py#L221">detectron2</a>.</p>

<h3 id="serialization">Serialization</h3>

<p><a href="https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/">This</a> blog provides a very clear explanation of why dataset serialization is necessary and how to do dataset serialization effectively.</p>

<p>Please check the code gist from <a href="https://github.com/facebookresearch/detectron2/blob/a2e43eab54d28ffbd59f5e9b4e3193b82faeb70f/detectron2/data/common.py#L114">detectron2</a> for more details.</p>

<p><code class="language-plaintext highlighter-rouge">_TorchSerializedList</code> is defined to serialize each object in the list using Python’s <code class="language-plaintext highlighter-rouge">pickle</code> module. It converts the object into a binary format (<code class="language-plaintext highlighter-rouge">pickle.dumps</code>) and then converts the binary data into a numpy array of unsigned 8-bit integers(<code class="language-plaintext highlighter-rouge">np.frombuffer</code>). All serialized byte arrays are concatenated into a single numpy array and then converted into a PyTorch tensor (<code class="language-plaintext highlighter-rouge">self._lst</code>).</p>

<p>To better access data segment by index, the class also calculates the byte length of each serialized object and stores these lengths in another numpy array.</p>

<h3 id="pytree">pytree</h3>

<p>Pytree was initially introduced within Jax. You can find a comprehensive discussion about pytree on HackerNews <a href="https://news.ycombinator.com/item?id=36029368">here</a>. PyTorch developers may find this feature highly useful, and decide to integrate it in a recent release. Now, we can use it straightforward inside pytorch, without any third-party packages:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">_pytree</span> <span class="k">as</span> <span class="n">pytree</span>

<span class="n">return_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"pixel_tensors"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="s">"labels"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="s">"txt"</span><span class="p">:</span> <span class="s">"a dummy example"</span>
<span class="p">}</span>

<span class="n">return_dict</span> <span class="o">=</span> <span class="n">pytree</span><span class="p">.</span><span class="n">tree_map_only</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                     <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">return_dict</span><span class="p">)</span>

<span class="c1"># all tensors in return_dict are moved to cuda device
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">pytree.tree_map_only</code> is used to selectively apply operations to only those objects within a nested structure that are PyTorch tensors. This is quite helpful where you might have complex data structures containing a mix of tensors, lists, dictionaries, etc., and you want to process only the tensors.  Start using pytree today, your training codes will receive the following benefits for free!</p>

<p><strong>Efficiency and Convenience:</strong> Manually checking the type of each element in a nested structure and applying a function to it can be cumbersome and error-prone, especially for deeply nested or complex structures. <code class="language-plaintext highlighter-rouge">pytree.tree_map_only</code> abstracts this logic, making the code cleaner and more efficient.</p>

<p><strong>Data Preparation for Distributed Computing:</strong> The specific use case involves preparing tensor data for efficient serialization and transfer in a distributed computing environment. Using <code class="language-plaintext highlighter-rouge">tree_map_only</code> allows for a straightforward, generalized way to ensure all tensor data is correctly processed for this environment, without altering the overall structure or non-tensor elements of the data being processed.</p>

<h3 id="sampler">Sampler</h3>

<p>Detectron2 has a good <a href="https://github.com/facebookresearch/detectron2/blob/5c380fdfc62b0124204155d6be3b1016e3dadb2d/detectron2/data/samplers/distributed_sampler.py#L15">implementations</a> of TrainingSampler.</p>

<p>In training, we only care about the “infinite stream” of training data. Therefore, the training sampler is designed to generate an infinite stream of indices and all workers cooperate to correctly shuffle the indices and sample different indices. Ensure that each rank can access different data. This could always lead to a silent bug for training and really hard to be found. Please pay attention when you build your Sampler.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">get_world_size</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_rank</span>
    <span class="k">yield</span> <span class="k">from</span> <span class="n">itertools</span><span class="p">.</span><span class="n">islice</span><span class="p">(</span><span class="n">_infinite_indices</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">_world_size</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Norm Inui</name></author><category term="Engineering" /><summary type="html"><![CDATA[TL; DR When starting a new project, I often ponder which codebase would be the best foundation. While the open-source community offers numerous impressive repositories, they often cater to specific demos and may not prioritize training or inference efficiency. As a result, I’ve chosen to construct my codebase by drawing inspiration from several awesome open-source projects.]]></summary></entry><entry><title type="html">Your SDXL is a Slow Learner</title><link href="/post8/" rel="alternate" type="text/html" title="Your SDXL is a Slow Learner" /><published>2024-01-20T00:00:00+00:00</published><updated>2024-01-20T00:00:00+00:00</updated><id>/post8</id><content type="html" xml:base="/post8/"><![CDATA[<h3 id="tl-dr">TL; DR</h3>

<ul>
  <li>
    <p>Fine-tuning SDXL UNET with high-quality datasets can enhance the AES score of generated images, but there is a limitation to this improvement. Specifically, you can easily upgrade the AES score of SDXL-base, which is initially aesthetically evaluated as 6.0, to reach a score of 6.7. However, a further enhancement in the AES score becomes significantly limited, even tuning on a high-quality dataset whose average AES score exceed 7.5 with many steps.</p>
  </li>
  <li>
    <p>High-quality fine-tuning can negatively affect text fidelity.</p>
  </li>
  <li>
    <p>Fine-tuning UNET without incorporating high-level noise latent features can enhance text fidelity in trade with a slight decrease in aesthetic quality</p>
  </li>
  <li>
    <p>A self-reward SDXL-DPO pipeline can both benefit text fidelity and aesthetic quality</p>
  </li>
</ul>

<h2 id="background">Background</h2>

<h3 id="high-quality-fine-tuning">high-quality fine-tuning</h3>

<p>It is widely known that fine-tuning SDXL using a carefully curated set of aesthetically pleasing data can significantly enhance the quality of SDXL’s output. In the <a href="https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/">EMU</a> paper, the authors assert that quality tuning can effectively enhance the pretrained base model, similar to how we do supervised fine-tuning with a Language Model (LLM).</p>

<p>The EMU paper has provided a good fine-tuning recipe for aesthetic alignment as follows:</p>

<ul>
  <li>
    <p>2k highly visually appealing images</p>
  </li>
  <li>
    <p>a small batch size of 64</p>
  </li>
  <li>
    <p>noise-offset is set to 0.1</p>
  </li>
  <li>
    <p>no more than 15K iterations regardless of loss decreasing</p>
  </li>
</ul>

<p>However, it’s worth noting that the EMU paper hid certain crucial details regarding this recipe. For instance, it doesn’t show us any samples from the 2,000 images to let us know how appealing these images are, the choice of the optimizer, and the criteria for determining when to early stop the fine-tuning process.</p>

<h3 id="sdxl-dpo">SDXL-DPO</h3>

<p>If you download a highly-rated checkpoint from Civitai and use it to create an image, you’ll likely find it  a bit tricky to generate an image that can follow detailed textual descriptions. This issue is likely because these checkpoints are overfitted on a curated dataset. Considering the fact that it is really hard to find when to early stop the training, the checkpoints are easily overfitted and harm the text fidelity.</p>

<p>To address such an issue, a promising solution is to push the diffusion models towards high text fidelity with <a href="https://arxiv.org/abs/2311.12908">Diffusion-DPO</a>. Diffusion-DPO enhances the training target for SDXL besides MES loss of predicting noises by incorporating a rewarding loss without the need for an online rewarding model. However, the efficacy of DPO in fixing text fidelity largely depends on the quality of the preference data pairs. What’s worse, it is hard to find any reliable metrics to determine whether SDXL-DPO can fix the text fidelity without sacrificing aesthetic quality.</p>

<h3 id="self-rewarding">Self-Rewarding</h3>

<p>Recently, in <a href="https://arxiv.org/abs/2401.10020">this</a> paper, Meta researchers showed that using an iteratively updated model, which generates its own rewards during training, can significantly enhance the instruction following ability when compared to a frozen reward model during SFT. What’s particularly surprising is that by fine-tuning Llama 2 70B through only three iterations of this self-rewarding approach, they achieved a model that outperforms several closed-source LLMs, including Claude 2, Gemini Pro, and even GPT-4 0613. This finding has inspired me to incorporate this concept into the SDXL-DPO pipeline to explore the potential benefits for image generation.</p>

<h2 id="method">Method</h2>

<p>Following the idea of Self-Rewarding, I change the SDXL-DPO pipeline by introducing a slowly updated reference UENT based on EMA.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/8/pipeline.png" alt="rewarding_pipeline" />
<strong>Figure 1.</strong>  A Self-Reward SDXL pipeline with EMA updated reference UNET; \(x^w\) and \(x^l\) represents a preferred image and a non-preferred image with the same text prompt. \(x^w_t\), \(x^l_t\) means adding noises to \(x^w\) and \(x^l\) at a random timestep \(t\) respectively;</p>

<h2 id="experiments">Experiments</h2>

<p>Let’s start with a fine-tuned proprietary SDXL-base as our baseline. I gathered 20 carefully selected prompts from <a href="https://prompthero.com/">PromptHero</a>, which are quite challenging for SDXL-base to generate visually pleasing images with only one-shot, as our benchmark. Each of these prompts was used to create 4 images using the same random settings and was refined with SDXL-refiner. Then, I employed <a href="https://github.com/christophschuhmann/improved-aesthetic-predictor">LAION-Aesthetics V2</a> to calculate the average aesthetic scores for a total of 80 generated images. This evaluation helps us determine whether the checkpoints learns to yield more visually pleasing images after fine-tuning.</p>

<p>The high-curated dataset \(D\) was filtered from 4M MidJourney v5 dataset. Since the raw image prompts are not natural language, I regenerated captions for each image using LLaVA-v1.5-13B. Various filters, such as OCR, SigLIP, and an aesthetic evaluator, were applied to curate the data. As a result, I obtained a 2k text-image pairs, with an average aesthetic score of <strong><mark>7.62</mark></strong>.</p>

<p>The experiment are conducted with 8 x A800, batch_size = 64, using deepzeros-2</p>

<table>
  <thead>
    <tr>
      <th>id</th>
      <th>avg aes score</th>
      <th>note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>baseline</td>
      <td>6.7</td>
      <td>a fine-proprietary SDXL checkpoint fine-tuned from SDXL-base</td>
    </tr>
    <tr>
      <td>trail 1</td>
      <td>6.7413</td>
      <td>the best aes score picked from all fine-tuned checkpoints in 10 epochs</td>
    </tr>
    <tr>
      <td>trail 2</td>
      <td>6.7515</td>
      <td>the best aes score picked from all fine-tuned checkpoints in 50 epochs</td>
    </tr>
    <tr>
      <td>trail 3</td>
      <td>6.7420</td>
      <td>continual DPO fine-tuned from the last saved checkpoint (epoch 10 step 300) in trail 1</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Experiments of fine-tuning a proprietary SDXL checkpoint whose generated images can achive average aesthetic score of 6.7 under our benchmark; Please check appendix for more training configuration details.</p>

<p>It’s worth noting that even if the curated dataset was intentionally fine-tuned for 50 epochs and has an average aesthetic score exceeding 7.5, the baseline checkpoint is  still hard to overfit on the dataset and the AES score is only increased from 6.7 to 6.75.</p>

<h2 id="showcase">ShowCase</h2>

<p>The following analysis is based on a cherry-picked example to visualize how different training strategies can directly impact the generated results. To further validate these findings, additional trials are necessary. I leave this task for me in the future or those who are interested in digging it further.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/8/showcase.png" alt="rewarding_pipeline" />
<strong>Figure 2.</strong> <strong>A comparison of generated results under different training settings following Table 1</strong>. All images are generated with the same configurations. Please check appendix for more config details.</p>

<p>According to Figure 2, we can observe that:</p>

<ul>
  <li>
    <p><strong>(a)</strong> Baseline; it tends to generate images in watercolor style. The light and contrast is unrealistic. The right-side image in (a) fails to include “the cape of the world” mentioned in the prompt;</p>
  </li>
  <li>
    <p><strong>(b)</strong> Trail 1, fine-tuned from baseline with 2k high-quality dataset, can generate more aesthetic images, especially the lighting;</p>
  </li>
  <li>
    <p><strong>(c)</strong> Trail 2, fine-tuned with the same configuration as Trail 1 but more steps, ignores the “cape”, yet it retains the lighting and art-style as Trail 1;</p>
  </li>
  <li>
    <p><strong>(d)</strong> The image on the right closely resembles the baseline, particularly the background clouds. It seems to blend the watercolor and realism. The model also successfully draws the “Cape”, which suggests that DPO could restore the text fidelity that may have been broken during high-quality fine-tuning in previous steps.</p>
  </li>
</ul>

<h3 id="appendix">Appendix</h3>

<p>training recipe for trail 1 and trail 2</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">grad_accumulate</span><span class="pi">:</span> <span class="m">4</span>
<span class="na">grad_clip</span><span class="pi">:</span> <span class="m">5.0</span>

<span class="c1"># optimizer config</span>
<span class="na">weight_decay</span><span class="pi">:</span> <span class="m">0.01</span>
<span class="na">optimizer_type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">adamw"</span>
<span class="na">lr</span><span class="pi">:</span> <span class="s">1e-6</span> 
<span class="na">adam_beta1</span><span class="pi">:</span> <span class="m">0.9</span>
<span class="na">adam_beta2</span><span class="pi">:</span> <span class="m">0.999</span>
<span class="na">adam_weight_decay</span><span class="pi">:</span> <span class="s">1e-2</span>
<span class="na">adam_epsilon</span><span class="pi">:</span> <span class="s">1e-8</span>

<span class="na">scheduler</span><span class="pi">:</span>
    <span class="na">scheduler_type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">cosine"</span>
    <span class="na">warmup_steps</span><span class="pi">:</span> <span class="m">200</span>

<span class="na">noise_offset</span><span class="pi">:</span> <span class="m">0.0357</span>
<span class="na">input_perturbation</span><span class="pi">:</span> <span class="m">0.0</span>
</code></pre></div></div>

<p><strong>Generation config for Figure 2</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">prompt</span><span class="pi">:</span> <span class="s">An old man waiting for the end of his life at the cape of the world</span>

<span class="na">inference</span><span class="pi">:</span> <span class="s">fp16</span>
<span class="na">resolution</span><span class="pi">:</span> <span class="m">1024</span>
<span class="na">gs</span><span class="pi">:</span> <span class="m">5.0</span>
<span class="na">sampling_method</span><span class="pi">:</span> <span class="s">DPMkarras</span>
<span class="na">random_seed</span><span class="pi">:</span> <span class="m">100</span>
</code></pre></div></div>]]></content><author><name>Norm Inui</name></author><category term="Diffusion" /><category term="Sparks" /><summary type="html"><![CDATA[TL; DR Fine-tuning SDXL UNET with high-quality datasets can enhance the AES score of generated images, but there is a limitation to this improvement. Specifically, you can easily upgrade the AES score of SDXL-base, which is initially aesthetically evaluated as 6.0, to reach a score of 6.7. However, a further enhancement in the AES score becomes significantly limited, even tuning on a high-quality dataset whose average AES score exceed 7.5 with many steps. High-quality fine-tuning can negatively affect text fidelity. Fine-tuning UNET without incorporating high-level noise latent features can enhance text fidelity in trade with a slight decrease in aesthetic quality A self-reward SDXL-DPO pipeline can both benefit text fidelity and aesthetic quality Background high-quality fine-tuning It is widely known that fine-tuning SDXL using a carefully curated set of aesthetically pleasing data can significantly enhance the quality of SDXL’s output. In the EMU paper, the authors assert that quality tuning can effectively enhance the pretrained base model, similar to how we do supervised fine-tuning with a Language Model (LLM). The EMU paper has provided a good fine-tuning recipe for aesthetic alignment as follows: 2k highly visually appealing images a small batch size of 64 noise-offset is set to 0.1 no more than 15K iterations regardless of loss decreasing However, it’s worth noting that the EMU paper hid certain crucial details regarding this recipe. For instance, it doesn’t show us any samples from the 2,000 images to let us know how appealing these images are, the choice of the optimizer, and the criteria for determining when to early stop the fine-tuning process. SDXL-DPO If you download a highly-rated checkpoint from Civitai and use it to create an image, you’ll likely find it a bit tricky to generate an image that can follow detailed textual descriptions. This issue is likely because these checkpoints are overfitted on a curated dataset. Considering the fact that it is really hard to find when to early stop the training, the checkpoints are easily overfitted and harm the text fidelity. To address such an issue, a promising solution is to push the diffusion models towards high text fidelity with Diffusion-DPO. Diffusion-DPO enhances the training target for SDXL besides MES loss of predicting noises by incorporating a rewarding loss without the need for an online rewarding model. However, the efficacy of DPO in fixing text fidelity largely depends on the quality of the preference data pairs. What’s worse, it is hard to find any reliable metrics to determine whether SDXL-DPO can fix the text fidelity without sacrificing aesthetic quality. Self-Rewarding Recently, in this paper, Meta researchers showed that using an iteratively updated model, which generates its own rewards during training, can significantly enhance the instruction following ability when compared to a frozen reward model during SFT. What’s particularly surprising is that by fine-tuning Llama 2 70B through only three iterations of this self-rewarding approach, they achieved a model that outperforms several closed-source LLMs, including Claude 2, Gemini Pro, and even GPT-4 0613. This finding has inspired me to incorporate this concept into the SDXL-DPO pipeline to explore the potential benefits for image generation. Method Following the idea of Self-Rewarding, I change the SDXL-DPO pipeline by introducing a slowly updated reference UENT based on EMA. Figure 1. A Self-Reward SDXL pipeline with EMA updated reference UNET; \(x^w\) and \(x^l\) represents a preferred image and a non-preferred image with the same text prompt. \(x^w_t\), \(x^l_t\) means adding noises to \(x^w\) and \(x^l\) at a random timestep \(t\) respectively; Experiments Let’s start with a fine-tuned proprietary SDXL-base as our baseline. I gathered 20 carefully selected prompts from PromptHero, which are quite challenging for SDXL-base to generate visually pleasing images with only one-shot, as our benchmark. Each of these prompts was used to create 4 images using the same random settings and was refined with SDXL-refiner. Then, I employed LAION-Aesthetics V2 to calculate the average aesthetic scores for a total of 80 generated images. This evaluation helps us determine whether the checkpoints learns to yield more visually pleasing images after fine-tuning. The high-curated dataset \(D\) was filtered from 4M MidJourney v5 dataset. Since the raw image prompts are not natural language, I regenerated captions for each image using LLaVA-v1.5-13B. Various filters, such as OCR, SigLIP, and an aesthetic evaluator, were applied to curate the data. As a result, I obtained a 2k text-image pairs, with an average aesthetic score of 7.62. The experiment are conducted with 8 x A800, batch_size = 64, using deepzeros-2 id avg aes score note baseline 6.7 a fine-proprietary SDXL checkpoint fine-tuned from SDXL-base trail 1 6.7413 the best aes score picked from all fine-tuned checkpoints in 10 epochs trail 2 6.7515 the best aes score picked from all fine-tuned checkpoints in 50 epochs trail 3 6.7420 continual DPO fine-tuned from the last saved checkpoint (epoch 10 step 300) in trail 1 Table 1: Experiments of fine-tuning a proprietary SDXL checkpoint whose generated images can achive average aesthetic score of 6.7 under our benchmark; Please check appendix for more training configuration details. It’s worth noting that even if the curated dataset was intentionally fine-tuned for 50 epochs and has an average aesthetic score exceeding 7.5, the baseline checkpoint is still hard to overfit on the dataset and the AES score is only increased from 6.7 to 6.75. ShowCase The following analysis is based on a cherry-picked example to visualize how different training strategies can directly impact the generated results. To further validate these findings, additional trials are necessary. I leave this task for me in the future or those who are interested in digging it further. Figure 2. A comparison of generated results under different training settings following Table 1. All images are generated with the same configurations. Please check appendix for more config details. According to Figure 2, we can observe that: (a) Baseline; it tends to generate images in watercolor style. The light and contrast is unrealistic. The right-side image in (a) fails to include “the cape of the world” mentioned in the prompt; (b) Trail 1, fine-tuned from baseline with 2k high-quality dataset, can generate more aesthetic images, especially the lighting; (c) Trail 2, fine-tuned with the same configuration as Trail 1 but more steps, ignores the “cape”, yet it retains the lighting and art-style as Trail 1; (d) The image on the right closely resembles the baseline, particularly the background clouds. It seems to blend the watercolor and realism. The model also successfully draws the “Cape”, which suggests that DPO could restore the text fidelity that may have been broken during high-quality fine-tuning in previous steps. Appendix training recipe for trail 1 and trail 2 grad_accumulate: 4 grad_clip: 5.0 # optimizer config weight_decay: 0.01 optimizer_type: "adamw" lr: 1e-6 adam_beta1: 0.9 adam_beta2: 0.999 adam_weight_decay: 1e-2 adam_epsilon: 1e-8 scheduler: scheduler_type: "cosine" warmup_steps: 200 noise_offset: 0.0357 input_perturbation: 0.0 Generation config for Figure 2 prompt: An old man waiting for the end of his life at the cape of the world inference: fp16 resolution: 1024 gs: 5.0 sampling_method: DPMkarras random_seed: 100]]></summary></entry><entry><title type="html">The Input Context Length Problem Seems to be Solved</title><link href="/Rethinking-Rotary-Position-Embedding-4/" rel="alternate" type="text/html" title="The Input Context Length Problem Seems to be Solved" /><published>2023-11-16T00:00:00+00:00</published><updated>2023-11-16T00:00:00+00:00</updated><id>/Rethinking-Rotary-Position-Embedding-4</id><content type="html" xml:base="/Rethinking-Rotary-Position-Embedding-4/"><![CDATA[<hr />

<h2 id="how-yarn-solves-the-out-of-bound-problem">How YaRN Solves The “Out-of-Bound” Problem</h2>

<p>In <a href="https://arxiv.org/pdf/2309.00071.pdf">YaRN</a> paper, the author mentioned a flaw in current NTK-RoPE:</p>

<blockquote>
  <p>Due to the “out-of-bound” values, the theoretical scale factor \(s\) does not accurately describe the true
context extension scale. In practice, the scale value \(s\) has to be set higher than the expected scale for
a given context length extension.</p>
</blockquote>

<p>To understand how the “out-of-bound” influences the extension scale, we first recall how NTK-aware interpolation works.</p>

<p>For RoPE, the \(\theta_{d} = b^{-\frac{2d}{\|D\|}}\), where we usually set \(b = 10000\), \(\|D\|\) is the dimension of each head.</p>

<p>we define \(\lambda_{d}\) as the wavelength of the RoPE embedding at d-th hidden dimension (Here, \(v\) represents the unit wave speed. In this blog, We assume \(v=1\) and we will refer to the period of a wave as its wavelength for simplicity.):</p>

\[\begin{equation}\lambda_{d}=\frac{2\pi}{\theta_{d}}=2\pi b^{\frac{2d}{\|D\|}} \end{equation}\]

<p>From <strong>eq1</strong> that, we can see that as \(d\) increases, the \(\lambda_{d}\) will also increase: The higher the dimension, the longer the wavelength.</p>

<p>NTK-RoPE expects the longest wavelength to be interpolated so that it can hold more position ids.</p>

\[\begin{equation} \lambda{max}=  2\pi b^{\frac{2d}{\|D\|}} |_{ d=\frac{\|D\|}{2} - 1} \end{equation}\]

<p>we want to expand the context length \(\lambda{max}\) by scaling up \(b\) to \(b^{\prime}\):</p>

\[\begin{equation} \lambda^{\prime}{max} = s \lambda{max} = 2\pi s \cdot b^{\frac{2d}{\|D\|}} |_{ d=\frac{\|D\|}{2} - 1} = 2\pi b^{\prime \frac{2d}{\|D\|}} |_{ d=\frac{\|D\|}{2} - 1} \end{equation}\]

<p>where \(s\) is the expected scale for a given context length extension.</p>

<p>Therefore, we can derive that:</p>

\[b^{\prime}=b\cdot s^{\frac{\|D\|}{\|D\|-2}}\]

<p>Now, we recompute the expanded wavelength \(\lambda^{\prime}_d\) with the \(b^{\prime}\)</p>

\[\lambda^{\prime}_d = 2\pi (b\cdot s^{\frac{\|D\|}{\|D\|-2}})^{\frac{2d}{\|D\|}}\]

<p>the expanded wavelength w.r.t the original wavelength along all dimensions is</p>

\[\mathrm{scale} = \lambda^{\prime}_d / \lambda_d = s^{\frac{2d}{\|D\|-2}}\]

<p>Attention, this is where “out-of-bound” problem happens. Only the last dimension \(d=\frac{\|D\|}{2} - 1\) can expand the wavelength by \(s\).
Dimensions lower than \(d=\frac{\|D\|}{2} - 1\) only scale up its wavelength less than \(s\)</p>

<p>For RoPE-based LLMs pre-trained with context length \(T_{\mathrm{train}}\), there exists a \(d_{\mathrm{extra}}\) dimension that for dimensions smaller than it, their corresponding periodic wavelengths are sufficiently trained.</p>

\[\begin{split}
T_{n}=2\pi\cdot b^{\frac{2n}{\|D\|}}\leq T_{\mathrm{train}},\mathrm{for}\,n=0,\cdot\cdot\cdot,d_{\mathrm{extra}}/2-1 \\
T_{n}=2\pi\cdot b^{\frac{2n}{\|D\|}}&gt;T_{\mathrm{train}},\mathrm{for}\,n=d_{\mathrm{extra}}/2,\cdot\cdot\cdot,\|D\|/2-1
\end{split}\]

<p>According to <a href="https://arxiv.org/abs/2310.05209">Liu, Xiaoran, et al., 2023</a></p>

<blockquote>
  <p>For LLaMA2(Touvron et al., 2023b), the critical dimension \(d_{\mathrm{extra}}\) is 92. This implies that only the
first 92 dimensions of the qt, ks vectors of LLaMA2 have seen the complete positional information
during the pre-training phase and are adequately trained. In other words, the last 36 dimensions lack
sufficient training, contributing to the extrapolation challenges seen in RoPE-based LLMs (Chen
et al., 2023; Han et al., 2023). The critical dimension plays a key role in enhancing extrapolation.</p>
</blockquote>

<p>Therefore, only those dimensions whose wavelength are trained at least one complete period can be extrapolated.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/2/wavelength.jpeg" alt="wavelength" /></p>

<p><strong>Figure 1</strong>. The visualized relationship among the period, training Length, and extrapolation, the periods of dimensions
past the critical dimension (in red) stretch beyond the training context; credited to <a href="https://arxiv.org/abs/2310.05209">Liu, Xiaoran, et al., 2023</a></p>

<p>Now back to NTKRoPE, we’ve concluded that <strong>only</strong> the last dimension \(d=\frac{\|D\|}{2} - 1\) can expand the wavelength by \(s\), even it doesn’t complete a full rotation (\(2\pi\)). In other words, suppose we have a model pretrained with 512 context length, we want to
expand it to 1024, each head dimension is 64, then only the \(\mathrm{dim}=31\) can ensure all interpolated position ids are just located within the wavelength that are sufficiently trained. Dimensions larger than \(d_{extra}\), however, do not complete a full rotation, and some of their position IDs are located outside the sufficiently trained wavelength, which we denote these position IDs as “out-of-bound” for that dimension. 
The farther the dimension deviates from the critical dimension \(d_{extra}\), the more interpolated “out-of-bound” position IDs the dimension have outside the range of its partially trained wavelengths.</p>

<p>What’s even worse, if the period of the signal is longer than the truncation length (max context length for LLM pretraining) in time domain, it means that the signal may not complete a full cycle within the max context. This time domain truncation is equivalent to multiplying the original signal by a rectangular window. In the frequency domain, this is equivalent to convolving the original signal’s spectrum with a sinc function. The sinc function has the property that its main lobe is concentrated at the major frequency, while the side lobes extend outward. Therefore, the energy originally concentrated at a major frequency will spread out to nearby frequencies, a phenomenon known as <strong>spectral leakage.</strong> Because the energy of the major frequency spreads out to surrounding frequencies, the signal’s energy is “dispersed” across a wider frequency range. This means the dimension beyond \(d_{extra}\) has a low SNR compared to those within the \(d_{extra}\), which makes those dimension even less training.</p>

<p>One possible way to mitigate the “out-of-bound” values is slightly increase the scale value so that more dimensions can ensure the interpolated position ids to locate within the critical dimension.</p>

<p>OR</p>

<p>we do what <a href="https://arxiv.org/abs/2308.12950">CodeLLaMA</a> does: scale up the rotation base to <strong>1M</strong>.</p>

<p>In conclusion, why could YaRN be the best choice to expand the context?</p>

<p>It is because it fixes the “Out-of-Bound” Problem in a less elegant but more effective way. In YaRN, we manually define upper and lower frequency bounds. These bounds can vary depending on the specific model in use. When dealing with frequencies below the lower bound, we do interpolation. Conversely, for frequencies beyond the upper bound, we apply extrapolation. For frequencies falling within the range between the lower and upper bounds, we apply a combination of both interpolation and extrapolation.</p>

<p>As long as we can find the sweet point low-bound frequency, the “Out-of-Bound” Problem will be effectively solved.</p>

<h2 id="how-mistral-solves-the-long-context-problem">How Mistral solves the long context problem</h2>

<p>Mistral first introduced the sliding window in their <a href="https://mistral.ai/news/announcing-mistral-7b/">blog</a>. They claim the sliding window attention mechanism can both save compute cost and expand the context length by stacking layers of transformers.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/2/sw_mistral.png" alt="SWA" /><br />
<strong>Figure 2</strong>. Sliding Window Mechanism (SWM); At each attention layer, information can move forward by W tokens at most: after two attention layers, information can move forward by 2W tokens, etc.</p>

<p>At first glance, it seems that Figure 2 is trying to show me that there exists a layer-wise shifting sliding window that can propogate token information to the next layer so that the context input can be extrapolated very long. However, Figure 2 is just to explain how information propagates along the depth of the network.</p>

<p>The main idea of the sliding window mechanism is to restrict each token to only attend to other tokens within a fixed-size window W. Nevertheless, the propagation of information through the network does not solely rely on the size of the attention window, it also relies on the stacking of multiple attention layers, more like an indirectly access.</p>

<p>For example, we have a sequence of tokens <strong>[A, B, C, D, E, F, G, H]</strong>, and let’s say oursliding window (W) is 3 tokens wide</p>

<p>The output of <strong>Layer 1</strong>:</p>

<p>Token \(\hat{A}\) integrates information from [A, B, C].<br />
Token  \(\hat{B}\) integrates information from [A, B, C, D].<br />
Token  \(\hat{C}\) integrates information from [A, B, C, D, E].</p>

<p><strong>Layer 2:</strong>
when token \(\hat{A}\) in the second layer attends to token \(\hat{B}\), it’s indirectly also getting information about token D, and when it attends to token \(\hat{C}\), it’s getting information about tokens D and E.</p>

<p>This means token A in layer 2 has a “reach” that extends itself to token E, even though it can only directly attend to [A, B, C].</p>

<p>As for a decoder-only model, the  SWM is more straightforward, as tokens can only attend to previous tokens in an auto-regression way.</p>

<p>The output of <strong>Layer 1</strong>:<br />
Token \(\hat{A}\) integrates information from <strong>only</strong> A.<br />
Token  \(\hat{B}\) integrates from A, B.<br />
Token  \(\hat{C}\) integrates from A, B, C<br />
and so on.</p>

<p>After Mixtral-8x7B is released recently, people supersingly find that MoE can magically extend the context length without any interpolation / extrapolation tricks we used in DynamicNTKRoPE, YaRN, etc.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/2/mixtral.jpg" alt="Mixtral" />
<strong>Figure 3</strong>. Perplexity evaluation; Mixtral (SMoE) works quite effectively even without the need for any fine-tuning. Moreover, it’s worth noting that disabling sliding window attention can actually enhance model’s the long context ability.</p>

<p>I have to say Figure 3 is hilarious. It shows that extending the context length is only a byproduct of MoE models, yet it still outperforms YaRN-Mistral, which I once bellieve to be the most promising way for manipulating RoPE to expand the context length.</p>

<p>Why it works?</p>

<p>I think it is because every expert is assigned only part of a long token sequence. Imagine there are eight experts simultaneously reading a 1000-token article, with each person assigned a portion of those 1000 tokens. Afterwards, they collaborate to integrate their individual understandings, and that’s how the expansion occurs.</p>

<h3 id="one-more-thing-updated-on-feb-2024">One More Thing (updated on Feb, 2024)</h3>

<p>Before <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle-in-a-Haystack</a> test comes out, researchers often use perplexity (negative log-likelihood of the next token) for evaluation. But is it really a reliable metric? Does low loss always mean high retrival performance on long context? The answer is: <strong>NO</strong></p>

<p><a href="https://arxiv.org/pdf/2402.10171">Chen, Mark, et al.</a> shows us in the paper:</p>

<blockquote>
  <p>similar test loss could result in substantially different behavior when performing precise retrieval</p>
</blockquote>

<p>If you ask me how to expand the LLM context length in Feb, 2024, I will answer you:</p>

<p><mark>Only data matters. </mark></p>

<p>In a word, by continual pretraining with a carefully domain-mixed dataset, and increasing the RoPE base without any modifications such as YaRN, it’s possible to achieve a longer context length than what was initially pre-trained.</p>

<p>Hence, there’s no necessity for further modifications to RoPE. Simply build a carefully curated dataset, inflate your models into MoE, and continue pre-training. These are all steps that’s required to extend the LLM context length. Besides, I think using ‘activate the LLM context length’ to describe the process sounds more reasonable, given that LLMs have already acquired this capability during its pretraining stage.</p>

<p>Just like what <a href="https://arxiv.org/abs/2402.10171">Fu, Yao, et al.</a> claims</p>

<blockquote>
  <p>We hypothesize that the capability to utilize information at  arbitrary locations within long context length is (mostly) already acquired during pretraining, even for models pretrained on substantially shorter 4K contexts.</p>
</blockquote>

<h3 id="reference">Reference</h3>

<ul>
  <li>
    <p><a href="https://github.com/jquesnelle/yarn/tree/master">YaRN: Efficient Context Window Extension of Large Language Models</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2310.05209">Liu, Xiaoran, et al., 2023</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2308.12950">CodeLLaMA</a></p>
  </li>
  <li>
    <p><a href="https://twitter.com/theemozilla/status/1735351012699849164?s=46&amp;t=poxa0AsGDnYfo1XBLblf4Q">@theemozilla</a></p>
  </li>
  <li>
    <p><a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2402.10171">Chen, Mark, et al.</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2402.10171">Fu, Yao, et al.</a></p>
  </li>
</ul>]]></content><author><name>Norm Inui</name></author><category term="LLM" /><summary type="html"><![CDATA[How YaRN Solves The “Out-of-Bound” Problem In YaRN paper, the author mentioned a flaw in current NTK-RoPE: Due to the “out-of-bound” values, the theoretical scale factor \(s\) does not accurately describe the true context extension scale. In practice, the scale value \(s\) has to be set higher than the expected scale for a given context length extension. To understand how the “out-of-bound” influences the extension scale, we first recall how NTK-aware interpolation works. For RoPE, the \(\theta_{d} = b^{-\frac{2d}{\|D\|}}\), where we usually set \(b = 10000\), \(\|D\|\) is the dimension of each head. we define \(\lambda_{d}\) as the wavelength of the RoPE embedding at d-th hidden dimension (Here, \(v\) represents the unit wave speed. In this blog, We assume \(v=1\) and we will refer to the period of a wave as its wavelength for simplicity.): \[\begin{equation}\lambda_{d}=\frac{2\pi}{\theta_{d}}=2\pi b^{\frac{2d}{\|D\|}} \end{equation}\] From eq1 that, we can see that as \(d\) increases, the \(\lambda_{d}\) will also increase: The higher the dimension, the longer the wavelength. NTK-RoPE expects the longest wavelength to be interpolated so that it can hold more position ids. \[\begin{equation} \lambda{max}= 2\pi b^{\frac{2d}{\|D\|}} |_{ d=\frac{\|D\|}{2} - 1} \end{equation}\] we want to expand the context length \(\lambda{max}\) by scaling up \(b\) to \(b^{\prime}\): \[\begin{equation} \lambda^{\prime}{max} = s \lambda{max} = 2\pi s \cdot b^{\frac{2d}{\|D\|}} |_{ d=\frac{\|D\|}{2} - 1} = 2\pi b^{\prime \frac{2d}{\|D\|}} |_{ d=\frac{\|D\|}{2} - 1} \end{equation}\] where \(s\) is the expected scale for a given context length extension. Therefore, we can derive that: \[b^{\prime}=b\cdot s^{\frac{\|D\|}{\|D\|-2}}\] Now, we recompute the expanded wavelength \(\lambda^{\prime}_d\) with the \(b^{\prime}\) \[\lambda^{\prime}_d = 2\pi (b\cdot s^{\frac{\|D\|}{\|D\|-2}})^{\frac{2d}{\|D\|}}\] the expanded wavelength w.r.t the original wavelength along all dimensions is \[\mathrm{scale} = \lambda^{\prime}_d / \lambda_d = s^{\frac{2d}{\|D\|-2}}\] Attention, this is where “out-of-bound” problem happens. Only the last dimension \(d=\frac{\|D\|}{2} - 1\) can expand the wavelength by \(s\). Dimensions lower than \(d=\frac{\|D\|}{2} - 1\) only scale up its wavelength less than \(s\) For RoPE-based LLMs pre-trained with context length \(T_{\mathrm{train}}\), there exists a \(d_{\mathrm{extra}}\) dimension that for dimensions smaller than it, their corresponding periodic wavelengths are sufficiently trained. \[\begin{split} T_{n}=2\pi\cdot b^{\frac{2n}{\|D\|}}\leq T_{\mathrm{train}},\mathrm{for}\,n=0,\cdot\cdot\cdot,d_{\mathrm{extra}}/2-1 \\ T_{n}=2\pi\cdot b^{\frac{2n}{\|D\|}}&gt;T_{\mathrm{train}},\mathrm{for}\,n=d_{\mathrm{extra}}/2,\cdot\cdot\cdot,\|D\|/2-1 \end{split}\] According to Liu, Xiaoran, et al., 2023 For LLaMA2(Touvron et al., 2023b), the critical dimension \(d_{\mathrm{extra}}\) is 92. This implies that only the first 92 dimensions of the qt, ks vectors of LLaMA2 have seen the complete positional information during the pre-training phase and are adequately trained. In other words, the last 36 dimensions lack sufficient training, contributing to the extrapolation challenges seen in RoPE-based LLMs (Chen et al., 2023; Han et al., 2023). The critical dimension plays a key role in enhancing extrapolation. Therefore, only those dimensions whose wavelength are trained at least one complete period can be extrapolated. Figure 1. The visualized relationship among the period, training Length, and extrapolation, the periods of dimensions past the critical dimension (in red) stretch beyond the training context; credited to Liu, Xiaoran, et al., 2023 Now back to NTKRoPE, we’ve concluded that only the last dimension \(d=\frac{\|D\|}{2} - 1\) can expand the wavelength by \(s\), even it doesn’t complete a full rotation (\(2\pi\)). In other words, suppose we have a model pretrained with 512 context length, we want to expand it to 1024, each head dimension is 64, then only the \(\mathrm{dim}=31\) can ensure all interpolated position ids are just located within the wavelength that are sufficiently trained. Dimensions larger than \(d_{extra}\), however, do not complete a full rotation, and some of their position IDs are located outside the sufficiently trained wavelength, which we denote these position IDs as “out-of-bound” for that dimension. The farther the dimension deviates from the critical dimension \(d_{extra}\), the more interpolated “out-of-bound” position IDs the dimension have outside the range of its partially trained wavelengths. What’s even worse, if the period of the signal is longer than the truncation length (max context length for LLM pretraining) in time domain, it means that the signal may not complete a full cycle within the max context. This time domain truncation is equivalent to multiplying the original signal by a rectangular window. In the frequency domain, this is equivalent to convolving the original signal’s spectrum with a sinc function. The sinc function has the property that its main lobe is concentrated at the major frequency, while the side lobes extend outward. Therefore, the energy originally concentrated at a major frequency will spread out to nearby frequencies, a phenomenon known as spectral leakage. Because the energy of the major frequency spreads out to surrounding frequencies, the signal’s energy is “dispersed” across a wider frequency range. This means the dimension beyond \(d_{extra}\) has a low SNR compared to those within the \(d_{extra}\), which makes those dimension even less training. One possible way to mitigate the “out-of-bound” values is slightly increase the scale value so that more dimensions can ensure the interpolated position ids to locate within the critical dimension. OR we do what CodeLLaMA does: scale up the rotation base to 1M. In conclusion, why could YaRN be the best choice to expand the context? It is because it fixes the “Out-of-Bound” Problem in a less elegant but more effective way. In YaRN, we manually define upper and lower frequency bounds. These bounds can vary depending on the specific model in use. When dealing with frequencies below the lower bound, we do interpolation. Conversely, for frequencies beyond the upper bound, we apply extrapolation. For frequencies falling within the range between the lower and upper bounds, we apply a combination of both interpolation and extrapolation. As long as we can find the sweet point low-bound frequency, the “Out-of-Bound” Problem will be effectively solved. How Mistral solves the long context problem Mistral first introduced the sliding window in their blog. They claim the sliding window attention mechanism can both save compute cost and expand the context length by stacking layers of transformers. Figure 2. Sliding Window Mechanism (SWM); At each attention layer, information can move forward by W tokens at most: after two attention layers, information can move forward by 2W tokens, etc. At first glance, it seems that Figure 2 is trying to show me that there exists a layer-wise shifting sliding window that can propogate token information to the next layer so that the context input can be extrapolated very long. However, Figure 2 is just to explain how information propagates along the depth of the network. The main idea of the sliding window mechanism is to restrict each token to only attend to other tokens within a fixed-size window W. Nevertheless, the propagation of information through the network does not solely rely on the size of the attention window, it also relies on the stacking of multiple attention layers, more like an indirectly access. For example, we have a sequence of tokens [A, B, C, D, E, F, G, H], and let’s say oursliding window (W) is 3 tokens wide The output of Layer 1: Token \(\hat{A}\) integrates information from [A, B, C]. Token  \(\hat{B}\) integrates information from [A, B, C, D]. Token  \(\hat{C}\) integrates information from [A, B, C, D, E]. Layer 2: when token \(\hat{A}\) in the second layer attends to token \(\hat{B}\), it’s indirectly also getting information about token D, and when it attends to token \(\hat{C}\), it’s getting information about tokens D and E. This means token A in layer 2 has a “reach” that extends itself to token E, even though it can only directly attend to [A, B, C]. As for a decoder-only model, the SWM is more straightforward, as tokens can only attend to previous tokens in an auto-regression way. The output of Layer 1: Token \(\hat{A}\) integrates information from only A. Token  \(\hat{B}\) integrates from A, B. Token  \(\hat{C}\) integrates from A, B, C and so on. After Mixtral-8x7B is released recently, people supersingly find that MoE can magically extend the context length without any interpolation / extrapolation tricks we used in DynamicNTKRoPE, YaRN, etc. Figure 3. Perplexity evaluation; Mixtral (SMoE) works quite effectively even without the need for any fine-tuning. Moreover, it’s worth noting that disabling sliding window attention can actually enhance model’s the long context ability. I have to say Figure 3 is hilarious. It shows that extending the context length is only a byproduct of MoE models, yet it still outperforms YaRN-Mistral, which I once bellieve to be the most promising way for manipulating RoPE to expand the context length. Why it works? I think it is because every expert is assigned only part of a long token sequence. Imagine there are eight experts simultaneously reading a 1000-token article, with each person assigned a portion of those 1000 tokens. Afterwards, they collaborate to integrate their individual understandings, and that’s how the expansion occurs. One More Thing (updated on Feb, 2024) Before Needle-in-a-Haystack test comes out, researchers often use perplexity (negative log-likelihood of the next token) for evaluation. But is it really a reliable metric? Does low loss always mean high retrival performance on long context? The answer is: NO Chen, Mark, et al. shows us in the paper: similar test loss could result in substantially different behavior when performing precise retrieval If you ask me how to expand the LLM context length in Feb, 2024, I will answer you: Only data matters. In a word, by continual pretraining with a carefully domain-mixed dataset, and increasing the RoPE base without any modifications such as YaRN, it’s possible to achieve a longer context length than what was initially pre-trained. Hence, there’s no necessity for further modifications to RoPE. Simply build a carefully curated dataset, inflate your models into MoE, and continue pre-training. These are all steps that’s required to extend the LLM context length. Besides, I think using ‘activate the LLM context length’ to describe the process sounds more reasonable, given that LLMs have already acquired this capability during its pretraining stage. Just like what Fu, Yao, et al. claims We hypothesize that the capability to utilize information at arbitrary locations within long context length is (mostly) already acquired during pretraining, even for models pretrained on substantially shorter 4K contexts. Reference YaRN: Efficient Context Window Extension of Large Language Models Liu, Xiaoran, et al., 2023 CodeLLaMA @theemozilla Mistral-7B Chen, Mark, et al. Fu, Yao, et al.]]></summary></entry><entry><title type="html">SD/SDXL Tricks beneath the Papers and Codes</title><link href="/sd-tricks/" rel="alternate" type="text/html" title="SD/SDXL Tricks beneath the Papers and Codes" /><published>2023-09-16T00:00:00+00:00</published><updated>2023-09-16T00:00:00+00:00</updated><id>/sd-tricks</id><content type="html" xml:base="/sd-tricks/"><![CDATA[<h3 class="no_toc"> TL; DR</h3>

<ul>
  <li>Collect awesome tricks about SD/SDXL pipeline</li>
</ul>

<!--more-->

<hr />

<p>From the moment the SD/SDXL was unveiled, the pace of advancements in image generation never stops. Almost every day, the open-source community rolls out novel techniques that enhance the pipeline’s aesthetic appeal and versatility. Yet, many of these innovations lack detailed documentation or in-depth explanations. To learn these tricks, one has to spend hours reading through the source codes. To simplify this process for developers and provide a convenient reference, I’ve written this blog to collect the tricks that lie beneath the published and source codes.</p>
<blockquote>
  <p><strong>Artistic Respect and Recognition:</strong> All artworks referenced in this blog are attributed to their original creators. Images inspired by these artists’ unique styles are not permitted for commercial use. If you create an image by referring their art style, please kindly give credit by acknowledging the artist’s name.</p>
</blockquote>

<h2 id="sdxl-architecture">SDXL Architecture</h2>

<p>In the SDXL <a href="https://arxiv.org/abs/2307.01952">paper</a>, the two encoders that SDXL introduces are explained as below:</p>

<blockquote>
  <p>We opt for a more powerful pre-trained text encoder that we use for text conditioning. Specifically, we use OpenCLIP ViT-bigG in combination with CLIP ViT-L, where we concatenate the penultimate text encoder outputs along the channel-axis. Besides using cross-attention layers to condition the model on the text-input, we follow and additionally condition the model on the pooled text embedding from the OpenCLIP model.</p>
</blockquote>

<p>To clarify how the two text encoders work together, here is a diagram I’ve made to illustrate the pipeline.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/sdxl_text_encoder.png" alt="SDXL_Text_Encoders" /></p>

<p>Figure 1. text encoder pipeline in SDXL; text_prompt 1 and text_prompt 2 are two prompts, which can be different; x0, y0, ∆x, ∆y, h, w are 6 spatial conditions newly introduced by SDXL</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/sdxl_refiner.png" alt="SDXL_Refiner" /></p>

<p>Figure 2. SDXL Refiner pipeline; x0, y0, ∆x, ∆y, h, w, aes_score are spatial conditions and aesthetic score introduced to guide the image generation</p>

<p>SDXL also has a refiner stage. Specifically, it is implemented as an img2img pipeline. While the paper may not explicitly mention it, it’s essential to note that the refinement stage takes aesthetic scores as a guiding factor besides the coordinates and image sizes. Specifically, a positive aesthetic score of 6.0 and a negative aesthetic score of 2.0 are used as the thresholds, which means that we expect the SDXL refiner will produce images with an average aesthetic score exceeding 6.0.</p>

<p>It’s interesting to notice that the 768-dim hidden features from the smaller text encoder are directly concatenated to the 1280-dim hidden features of the larger text encoder. Note that <code class="language-plaintext highlighter-rouge">text_prompt_1</code> and <code class="language-plaintext highlighter-rouge">text_prompt_2</code> can be different. Given the intuition that higher dimensions can capture fine-grained features, some AI painters prefer to feed style descriptions into CLIP-ViT-L and prompts about body motion, clothes, or facial expressions into the other text encoder. Additionally, it’s worth noting that the pooled feature from the larger text encoder  acts as a bias to adjust the time embedding</p>

<p>Below are the visualization about the SDXL/SDM UNet structure. These are two graphs I always come to check out</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/sdxl_unet.png" width="600" /><br />
Figure 3. SDXL UNET structure</p>

<p>Compared SDXL UNET with SDM UNET,</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/sdm_unet.png" alt="SDM-Unet" />
Figure 4. Original SDM UNET structure; image credited to <a href="https://openreview.net/pdf?id=bOVydU0XKC">BK-SDM paper</a></p>

<p>we can clearly see from Fig.3 and Fig.4 that the lowest latent dimension is set to 16 rather than 8. According to the paper, they remove the dimension 8 for computation efficiency. Moreover, they also omit the transformer block at the highest feature level, further saving the computation cost.</p>

<h2 id="reference-only-mode">Reference-only Mode</h2>

<p>Reference-only mode is one of my favorite features in the SD pipeline, which enables art style transfer from a reference image to a new image based on a given prompt, without training.</p>

<p>It is first introduced by <a href="https://github.com/lllyasviel">lllyasviel</a> in <a href="https://github.com/Mikubill/sd-webui-controlnet/discussions/1280">this</a> discussion thread.</p>

<p>Here is a showcase from this reference only.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/showcase.png" width="512" /><br />
Figure 5. A reference-only showcase;  <strong>Left:</strong> reference image credit to <a href="https://twitter.com/001_31_/media">@001_31_</a>; <strong>Right:</strong> generated image; We can clearly see the generated image “plagiarizes” the art style of the reference image like colors, lines and character face; please check the appendix for generation setting.</p>

<p>The idea behind the <code class="language-plaintext highlighter-rouge">reference-only</code> is quite straightforward. It requires running the diffusion process twice. In the first round, the VAE encoder encodes the reference image into a latent feature, termed as \(x_{ref}\). This reference latent feature is then interpolated with a randomly initialized latent feature. We denote the noisy latent feature as \(x\).</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/ref_only.png" alt="Reference_only" />
Figure 6. reference-only pipeline; ①: the first diffusion forward pass, ②: the second diffusion forward pass</p>

<p>Suppose we are playing this <code class="language-plaintext highlighter-rouge">reference-only</code> mode with an SD1.5 checkpoint. As we know, SD1.5 has 16 TransformerBlocks across all UNet layers. Therefore, during the first diffusion forward pass, 16 hidden features are generated and stored in the memory band. Subsequently, we pass \(x\) to the second diffusion forward pass.</p>

<p>The self-attention block uses the previously cached features from the corresponding Transformer Block layer in the prior diffusion as a reference clue. In other words, there are now “two” cross-attention blocks in the second diffusion forward pass. One is conditioned on the reference clues generated from \(x_{ref}\), while the other is conditioned on the text features.</p>

<p>As for the self-attention part in the second diffusion forward pass,  There exist implementation variants that can yield different generation outcomes.</p>

<p>In the following discussion, we denote the input features of the self-attention block as \(h_{in}\) within one TransformerBlock, and the resulting feature as \(h_{out}\). Typically, we apply the classifier-free method for text guiding, \(h_{in}, h_{out} \in \mathbb{R^{2 \times N \times dim}}\), where \(N\) is the number of elements in the latent feature, \(dim\) is the hidden feature dimension.</p>

<p>Additionally, we denote the cached features from the same TransformerBlock generated in the first diffusion forward pass as \(h_{cond} \in \mathbb{R^{2 \times N \times dim}}\)</p>

<h4 class="no_toc">  Reference-fine-grained </h4>
<p>One implementation is only the conditioned latent features use the cached features as the cross-attention clue, while the unconditioned latent features compute cross-attention itself. The pseudo-code is provided below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">attn_output_uc</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span> <span class="n">h_in</span><span class="p">,</span>
                       <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h_in</span><span class="p">,</span> <span class="n">h_cond</span><span class="p">]</span> <span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">attn_output_c</span> <span class="o">=</span>  <span class="n">attn</span><span class="p">(</span> <span class="n">h_in</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">h_in</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">h_out</span> <span class="o">=</span> <span class="n">style_fidelity</span> <span class="o">*</span> <span class="n">attn_output_c</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">style_fidelity</span><span class="p">)</span> <span class="o">*</span> <span class="n">attn_output_uc</span>
</code></pre></div></div>

<p>This is how huggingface implements. check the code <a href="https://github.com/huggingface/diffusers/blob/73bb97adfc3d0cb184c5fd66a1d5699c249a7fd8/examples/community/stable_diffusion_reference.py#L405">here</a>.
I’d like to refer it as <code class="language-plaintext highlighter-rouge">reference-fine-grained</code></p>

<h4 class="no_toc">  Reference-coarse</h4>
<p>Another implementation processes both the unconditional and conditional latent features in the same way. I prefer to call it <code class="language-plaintext highlighter-rouge">reference-coarse</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h_out</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span> <span class="n">h_in</span><span class="p">,</span>
              <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">h_in</span><span class="p">,</span> <span class="n">h_cond</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<h4 class="no_toc">  Reference-adain</h4>

<p><code class="language-plaintext highlighter-rouge">reference_adain</code> is another mode proposed by <a href="https://github.com/lllyasviel">lllyasviel</a>, who drew inspiration from the paper, <a href="https://arxiv.org/abs/1703.06868">“Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization”</a>. AdaIN, short for <em>Adaptive Instance Normalization</em>, works on a premise: when given content input \(x\) and style input \(y\), the style transfer can work by aligning the channel-wise mean and variance of \(x\) with those of \(y\).</p>

<p>In the <code class="language-plaintext highlighter-rouge">reference_adain</code> mode, not only are the input hidden features sent to the Transformer Block, but the mean and variance of these features are also cached during the first diffusion forward. Additionally, the mean and variance of the output features of the ResNet Block in the UNet stage without a Transformer, are stored as well. During the second diffusion pass, these stored values are applied to adapt to respective hidden features from the same blocks.</p>

<h4 class="no_toc">  Reference mode comparsion</h4>

<p>The modes mentioned above can work together. Figure 6 compares the generation output in different reference modes.
<img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/reference-comparsion.png" alt="reference-comparsion" />
Figure 7; reference mode comparison. All images are generated with the same setting. For setting details, please check the appendix <strong>(a):</strong> a reference image credit to <a href="https://twitter.com/001_31_/media">@001_31_</a>; <strong>(b)</strong> reference-course; <strong>(c)</strong> reference-fine-grained; <strong>(d)</strong> reference-adain; <strong>(d)</strong> combine reference-course and reference-adain;</p>

<p>From (b) and (c), we can see there are fine-grained added elements in (c) not present (b), such as the decorations on the character’s hat and the detailed hair strands; (d) shows that the reference image in <code class="language-plaintext highlighter-rouge">reference-adain</code> offers limited guiding to the final output. Notably, the colors of the character’s hair, background, and eyes are quite different from those in (a); From (b) and (e), we can find the <code class="language-plaintext highlighter-rouge">reference-adain</code> can help the generation closely align to the style of the reference image, such as the character’s face, hat, face angle, coloring style. Instead, <code class="language-plaintext highlighter-rouge">reference-course</code> enables the pipeline paint more freely, using the reference only as a loose guide.</p>

<p>I recommend playing with these reference mode combinations to discover your favorite settings.</p>

<h2 id="slider-lora">Slider LoRA</h2>

<p>This innovative method offers the flexibility to seamlessly modify the body/concepts of your character, much like using a slider to customize your character in a video game. Moreover, it ensures character consistency without the need for cumbersome prompt engineering and random attempts. While the community hasn’t agreed on how to name it, some suggest “negative LoRA” as a counterpart of “negative embedding”. However, Since I first learned about it from <a href="https://note.com/emanon_14/n/neb46bac832f2">this</a> awesome post by <a href="https://twitter.com/Emanon_14">エマノン</a>, I’ve chosen to follow his terminology, dubbing the technique “Slider LoRA”.
<img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/slider_lora_headsz.png" alt="slider_lora" />
Figure 8; A showcase of slider LoRA, which can adjust the head size of the character with character consistency; credit to <a href="https://twitter.com/Emanon_14/status/1700746112112714236?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700746112112714236%7Ctwgr%5E70c38d02b4fbc72c6baec078647a0744b0f4cb3c%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fnote.com%2Femanon_14%2Fn%2Fneb46bac832f2">@Emanon_14</a></p>

<blockquote>
  <p>エマノン さん’s post introduces how to create a Slider LoRA. For those who don’t read Japanese, I’ve summarized the key points from the original post below.</p>
</blockquote>

<p>エマノン さん proposes two approaches to making a slider LoRA based on コピー機学習法 (copy machine learning). 
<img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/slider_pipeline_2.png" width="700" /> <br />
Figure 9; First, merge a LoRA, which is overfitted to one reference image, into the base mode. Then, merge another LoRA, overfitted an image transformed from the previous one, into the merged model; credit to <a href="https://twitter.com/Emanon_14/status/1700746112112714236?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700746112112714236%7Ctwgr%5E70c38d02b4fbc72c6baec078647a0744b0f4cb3c%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fnote.com%2Femanon_14%2Fn%2Fneb46bac832f2">@Emanon_14</a></p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/slider_pipeline_1.png" width="700" /> <br />
Figure 10; Merge LoRA A, which is trained to overfit one reference image, and LoRA B, which is trained to overfit another image, into the based model using different weights; credit to <a href="https://twitter.com/Emanon_14/status/1700746112112714236?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700746112112714236%7Ctwgr%5E70c38d02b4fbc72c6baec078647a0744b0f4cb3c%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fnote.com%2Femanon_14%2Fn%2Fneb46bac832f2">@Emanon_14</a></p>

<p>According to the post, copy machine learning is a parameter-efficiency training method. To be specific, it leverages LoRA on the transformers block in the UNet, then overfits it until the model can reconstruct the  training image with an empty prompt.</p>

<p>In the following training pipeline, we take the second approach as an example</p>

<h4 class="no_toc">Prepare the Training Dataset</h4>
<p>The training data is organized in pairs. These paired datasets differ in specific angles, with each pair varying only in the concept that you want the LoRA slider can recognize. For example, here is a training pair only different in the head size of the character.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/slider_training_pair.png" width="512" /> <br />
Figure 11; credit to  <a href="https://twitter.com/Emanon_14/status/1700746112112714236?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1700746112112714236%7Ctwgr%5E70c38d02b4fbc72c6baec078647a0744b0f4cb3c%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fnote.com%2Femanon_14%2Fn%2Fneb46bac832f2">@Emanon_14</a>; Except for the head size of the two characters, the other components remain identical at the pixel level.</p>

<h4 class="no_toc">Copy Machine Learning</h4>

<p>Next, let’s train two UNet LoRAs, using images from each set, for about 1000 ~ 3000 steps.</p>

<p>When the LoRA can replicate the original training image using an empty prompt, we can say the training is successful. Empirically, the optimal LoRA rank dimension is 4 ~ 16.</p>

<h4 class="no_toc">Merge the LoRAs</h4>
<p>Finally, it’s time to merge two LoRAs that memorize opposite concepts into one LoRA. This merging is realized by <a href="https://github.com/bmaltais/kohya_ss/blob/ed4e3b0239a40506de9a17e550e6cf2d0b867a4f/networks/svd_merge_lora.py">svd_merge_lora</a> method.</p>

<p>The idea of svd_merge_lora is straightforward, It first merges two LoRA weights into the base model. Then, it leverages Singular Value Decomposition (SVD) on the merged weights to derive the final LoRA. Notably, the rank dimension of the final LoRA derived with SVD is usually double that of the initial LoRA weights.</p>

<h2 id="appendix">Appendix</h2>

<h4 class="no_toc"> 1. the generation setting of Figure 5 (reference-only showcase)</h4>

<ul>
  <li><strong>prompt:</strong>  masterpiece,best quality, ultra highres, detailed illustration, portrait, detailed, girls, detailed frilled clothes, detailed beautiful skin, face focus</li>
  <li><strong>negative embedding:</strong> <a href="https://civitai.com/models/7808/easynegative">EasyNegative</a>, <a href="https://civitai.com/models/17083?modelVersionId=20170">bad-picture-chill</a></li>
  <li><strong>sd1.5 checkpoint:</strong> A fine-tuned checkpoint based on SD1.5 with proprietary dataset</li>
</ul>

<h4 class="no_toc"> 2. the generation setting of Figure 7 (reference mode comparsion)</h4>

<ul>
  <li><strong>prompt:</strong>  masterpiece, best quality, 1girl, medium hair, elf, pointy ears, loli, teen age, looking at viewer, :3</li>
  <li><strong>negative embedding:</strong> <a href="https://civitai.com/models/7808/easynegative">EasyNegative</a>, <a href="https://civitai.com/models/17083?modelVersionId=20170">bad-picture-chill</a></li>
  <li><strong>sd1.5 checkpoint:</strong> A fine-tuned checkpoint based on SD1.5 with proprietary dataset</li>
</ul>]]></content><author><name>Norm Inui</name></author><category term="Diffusion" /><summary type="html"><![CDATA[TL; DR Collect awesome tricks about SD/SDXL pipeline]]></summary></entry><entry><title type="html">The Compatibility between CUDA, GPU, Base Image, and PyTorch</title><link href="/compatibility/" rel="alternate" type="text/html" title="The Compatibility between CUDA, GPU, Base Image, and PyTorch" /><published>2023-08-30T00:00:00+00:00</published><updated>2023-08-30T00:00:00+00:00</updated><id>/compatibility</id><content type="html" xml:base="/compatibility/"><![CDATA[<h3 class="no_toc"> TL; DR</h3>

<ul>
  <li><strong>Host CUDA VS Base Image CUDA</strong>: The CUDA verision within a runtime docker image has no relationship with the CUDA version on the host machie. The only thing we need to care about is whether the driver version on the host supports the base image’s CUDA runtime. Check the driver compatibility <a href="https://docs.nvidia.com/deploy/cuda-compatibility/#minor-version-compatibility">here</a></li>
  <li><strong>PyTorch VS CUDA</strong>: PyTorch is compatible with one or a few specific CUDA versions, more precisely, CUDA runtime APIs. Check the compatible matrix <a href="https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix">here</a></li>
  <li>
    <p><strong>CUDA VS GPU</strong>: Each GPU architecture is compatible with certain CUDA versions, more precisely, CUDA driver versions. Quick check <a href="https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/">here</a></p>
  </li>
  <li><strong>PyTorch and GPU</strong>:  PyTorch only supports GPU specified in <code class="language-plaintext highlighter-rouge">TORCH_CUDA_ARCH_LIST</code> when compiled</li>
</ul>

<!--more-->

<hr />

<p>The relationship between the CUDA version, GPU architecture, and PyTorch version can be a bit complex but is crucial for the proper functioning of PyTorch-based deep learning tasks on a GPU.</p>

<p>Suppose you’re planning to deploy your awesome service on an <strong>NVIDIA A100-PCIE-40Gb</strong> server with <strong>CUDA 11.2</strong> and <strong>Driver Version 460.32.03</strong>. You’ve built your service using <strong>PyTorch 1.12.1</strong>, and your Docker image is built based on an NVIDIA base image, specifically <a href="https://hub.docker.com/layers/andrewseidl/nvidia-cuda/10.2-base-ubuntu20.04/images/sha256-3d4e2bbbf5a85247db30cd3cc91ac4695dc0d093a1eead0933e0dbf09845d1b9?context=explore"><strong>nvidia-cuda:10.2-base-ubuntu20.04</strong></a>. How can you judge whether your service can run smoothly on the machine without iterative attempts?</p>

<p>To clarify this complicated compatibility problem,  let’s take a quick recap of the key terminologies we mentioned above.</p>

<h2 id="basic-concepts">Basic Concepts</h2>
<h3 id="gpu-architecture">GPU Architecture</h3>

<p>NVIDIA releases new generations of GPUs every year that are based on different architectures, such as Kepler, Maxwell, Pascal, Volta, Turing, Ampere, and up to Hopper as of 2023. These architectures have different capabilities and features, specified by their Compute Capability version (e.g., sm_35, sm_60, sm_80, etc.). “sm” stands for “streaming multiprocessor,” which is a key GPU component responsible for carrying out computations. The number following “sm” represents the architecture’s version. We denote it as GPU code in the following context.</p>

<p>For example, “sm_70” which corresponds to the Tesla V100 GPU. When you specify a particular architecture with nvcc,  the compiler will optimize your code for that architecture. As a result, your compiled code may not be fully compatible with GPUs based on different architectures.</p>

<p>You can find more detailed explanations in <a href="https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/">this</a> post.</p>

<h3 id="cuda-version">CUDA Version</h3>

<p>The terms “CUDA” and “CUDA Toolkits” often appear together. “CUDA XX.X” is shorten for the version of the CUDA Toolkits.It serves as an interface between the software (like PyTorch) and the hardware (like NVIDIA GPU).</p>

<p>CUDA Toolkits include:</p>

<ol>
  <li><strong>Libraries and Utilities</strong>: The CUDA Toolkit provides a collection of libraries and utilities that allow developers to build and profile CUDA-enabled applications, such as CuDNN.</li>
  <li><strong>CUDA Runtime API</strong>: The Toolkit includes the CUDA runtime, which provides the application programming interface (API) used for tasks like allocating memory on the GPU, transferring data between the CPU and GPU, and launching kernels (compute functions) on the GPU. CUDA runtime APIs are generally designed to be forward-compatible with newer drivers.</li>
  <li><strong>NVCC Compiler</strong>: The Toolkit includes the <code class="language-plaintext highlighter-rouge">nvcc</code> compiler for compiling CUDA code into GPU-executable code.</li>
</ol>

<h3 id="pytorch-version">PyTorch Version</h3>

<p>PyTorch releases are often tightly bound to specific CUDA versions for compatibility and performance reasons.</p>

<h3 id="base-image">Base Image</h3>

<p>Copied from NVIDIA docker <a href="https://hub.docker.com/r/nvidia/cuda">homepage</a>:</p>

<blockquote>
  <p>base: Includes the CUDA runtime (cudart)</p>

  <p>runtime: Builds on the base and includes the <a href="https://developer.nvidia.com/gpu-accelerated-libraries">CUDA math libraries</a>, and <a href="https://developer.nvidia.com/nccl">NCCL</a>. A runtime image that also includes <a href="https://developer.nvidia.com/cudnn">cuDNN</a> is available.</p>

  <p>devel: Builds on the runtime and includes headers, development tools for building CUDA images. These images are particularly useful for multi-stage builds.</p>
</blockquote>

<h2 id="interrelation">Interrelation</h2>

<h3 id="cuda-and-base-image">CUDA and Base Image</h3>

<p>The base image only contains the minimum required dependencies to deploy a pre-built CUDA application.  Importantly, there’s no requirement for the CUDA version in the base image to match the CUDA version on the host machine.</p>

<p>Back to our deployment case</p>

<ul>
  <li>our service is built based on <code class="language-plaintext highlighter-rouge">nvidia-cuda:10.2-base-ubuntu20.04</code> image</li>
  <li>The host machine has a CUDA driver that supports up to CUDA 11.2</li>
</ul>

<p>In this setup, the service built with <code class="language-plaintext highlighter-rouge">nvidia-cuda:10.2-base-ubuntu20.04</code> image doesn’t mean there installs a driver which supports CUDA 10.2 inside the image; instead, it relies on the host’s driver which can support up to CUDA 11.7.</p>

<p>Therefore, the service container will use the CUDA 10.2 runtime API, and because the host driver (supporting up to CUDA 11.2) is forward-compatible with older CUDA runtime versions, the application should run without any issues.</p>

<p>Therefore, the only one critical point you need to consider is that</p>

<p><b><font color="red">Whether the driver version on the host supports the base image's CUDA runtime</font></b></p>

<p>The CUDA runtime version inside the container must be less than or equal to the CUDA driver version on the host system, or else you might encounter compatibility issues and the service will fail to start with an error message as:</p>

<blockquote>
  <p>CUDA driver version is insufficient for CUDA runtime version</p>
</blockquote>

<p>A version-compatible matrix between the CUDA and driver can be found <a href="https://docs.nvidia.com/deploy/cuda-compatibility/#minor-version-compatibility">here</a>.</p>

<p>Besides, there is still one consideration you should never miss. According to the line 16 in the <a href="https://hub.docker.com/layers/andrewseidl/nvidia-cuda/10.2-base-ubuntu20.04/images/sha256-3d4e2bbbf5a85247db30cd3cc91ac4695dc0d093a1eead0933e0dbf09845d1b9?context=explore">dockerfile</a> of <code class="language-plaintext highlighter-rouge">nvidia-cuda:10.2-base-ubuntu20.04</code></p>

<blockquote>
  <p>ENV NVIDIA_REQUIRE_CUDA=cuda&gt;=10.2</p>
</blockquote>

<p>The base image requires a minimum CUDA version of the host.</p>

<p>Up till now,</p>

<ul>
  <li>
    <p><strong>host has CUDA11.2 &gt;= 10.2. the base image is compatible with host</strong> ✅</p>
  </li>
  <li>
    <p><strong>host driver 460.32.03 meets the minimum requirements of CUDA 10.2</strong> ✅</p>
  </li>
</ul>

<h3 id="pytorch-and-cuda">PyTorch and CUDA</h3>

<p>PyTorch versions is compatible  with one or a few specific CUDA versions, or more precisely, with corresponding CUDA runtime API versions. Using an incompatible version might lead to errors or sub-optimal performance.</p>

<p>Following is the Release Compatibility Matrix for PyTorch, copied from <a href="https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix">here</a>:</p>

<table>
  <thead>
    <tr>
      <th>PyTorch version</th>
      <th>Stable CUDA</th>
      <th>Experimental CUDA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2.1</td>
      <td>CUDA 11.8, CUDNN 8.7.0.84</td>
      <td>CUDA 12.1, CUDNN 8.9.2.26</td>
    </tr>
    <tr>
      <td>2.0</td>
      <td>CUDA 11.7, CUDNN 8.5.0.96</td>
      <td>CUDA 11.8, CUDNN 8.7.0.84</td>
    </tr>
    <tr>
      <td>1.13</td>
      <td>CUDA 11.6, CUDNN 8.3.2.44</td>
      <td>CUDA 11.7, CUDNN 8.5.0.96</td>
    </tr>
    <tr>
      <td>1.12</td>
      <td>CUDA 11.3, CUDNN 8.3.2.44</td>
      <td>CUDA 11.6, CUDNN 8.3.2.44</td>
    </tr>
  </tbody>
</table>

<p>The official PyTorch <a href="https://pytorch.org/get-started/previous-versions/#v1121">webpage</a> provides three examples of CUDA version that are compatible with PyTorch 1.12, ranging from CUDA 10.2 to CUDA 11.6. Therefore, PyTorch 1.12.1 in our scenario passes the compatible test.</p>

<p>So far so good, we have:</p>

<ul>
  <li><strong>PyTorch1.12 is compatible with CUDA 11.2</strong> ✅</li>
</ul>

<h3 id="cuda-and-gpu">CUDA and GPU</h3>

<p>Each GPU architectures is compatible with certain CUDA versions, or more precisely, CUDA driver versions. As for Ampere, the compatibility is shown as below, copied from <a href="https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/">this</a> post:</p>

<blockquote>
  <p><strong>Ampere (CUDA 11.1 and later)</strong></p>

  <ul>
    <li><strong>SM80 or <code class="language-plaintext highlighter-rouge">SM_80, compute_80</code></strong> –
 NVIDIA <a href="https://amzn.to/3GqeDrq">A100</a> (the name “Tesla” has been dropped – GA100), NVIDIA DGX-A100</li>
    <li>
      <p><strong>SM86 or <code class="language-plaintext highlighter-rouge">SM_86, compute_86</code></strong> – (from <a href="https://docs.nvidia.com/cuda/ptx-compiler-api/index.html">CUDA 11.1 onwards</a>)
 Tesla GA10x cards, RTX Ampere – RTX 3080, GA102 – RTX 3090, RTX A2000, A3000, <a href="https://www.amazon.com/PNY-NVIDIA-Quadro-A6000-Graphics/dp/B08NWGS4X1?msclkid=45987a9faa0411ec98c321cb30a0780e&amp;linkCode=ll1&amp;tag=arnonshimoni-20&amp;linkId=ccac0fed7c3cac61b4373d7dac6e7136&amp;language=en_US&amp;ref_=as_li_ss_tl">RTX A4000</a>, A5000, <a href="https://www.amazon.com/PNY-VCNRTXA6000-PB-NVIDIA-RTX-A6000/dp/B09BDH8VZV?crid=3QY8KCKXO3FB8&amp;keywords=rtx+a6000&amp;qid=1647969665&amp;sprefix=rtx+a6000%2Caps%2C174&amp;sr=8-1&amp;linkCode=ll1&amp;tag=arnonshimoni-20&amp;linkId=d292ba4d995d2b034a27441321668ffb&amp;language=en_US&amp;ref_=as_li_ss_tl">A6000</a>, NVIDIA A40, GA106 – <a href="https://www.amazon.com/gp/product/B08W8DGK3X/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=arnonshimoni-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=B08W8DGK3X&amp;linkId=5cb5bc6a11eb10aab6a98ad3f6c00cb9">RTX 3060</a>, GA104 – RTX 3070, GA107 – RTX 3050, RTX A10, RTX A16, RTX A40, A2 Tensor Core GPU</p>
    </li>
    <li><strong>SM87 or <code class="language-plaintext highlighter-rouge">SM_87, compute_87</code></strong> – (from <a href="https://docs.nvidia.com/cuda/ptx-compiler-api/index.html">CUDA 11.4 onwards</a>, introduced with PTX ISA 7.4 / Driver r470 and newer) – for Jetson AGX Orin and Drive AGX Orin only</li>
  </ul>
</blockquote>

<p>We therefore draw the conclusion:</p>

<ul>
  <li><strong>NVIDIA A100-PCIE-40Gb is compatible with CUDA 11.2</strong> ✅</li>
</ul>

<h3 id="pytorch-and-gpu">PyTorch and GPU</h3>
<p>A particular version of PyTorch will be compatible only with the set of GPUs whose compatible CUDA versions overlap with the CUDA versions that PyTorch supports.</p>

<p>PyTorch libraries can be compiled from source codes into two forms, binary <em>cubin</em> objects and forward-compatible <em>PTX</em> assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 7.0 is supported to run on a GPU with compute capability 7.5, however a cubin generated for compute capability 7.5 is <em>not</em> supported to run on a GPU with compute capability 7.0, and a cubin generated with compute capability 7.x is <em>not</em> supported to run on a GPU with compute capability 8.x.</p>

<p>When the developers of PyTorch release a new version, they include a flag, <code class="language-plaintext highlighter-rouge">TORCH_CUDA_ARCH_LIST</code>, in the <a href="https://github.com/pytorch/pytorch/blob/78810d78e82f8e18dbc1c049a2b92e559ab567b2/setup.py#L134">setup.py</a>. In this flag, they can specify which CUDA architecture to build for, such as <code class="language-plaintext highlighter-rouge">TORCH_CUDA_ARCH_LIST="3.5 5.2 6.0 6.1 7.0+PTX 8.0"</code>. Remember numbers in <code class="language-plaintext highlighter-rouge">TORCH_CUDA_ARCH_LIST</code> are not CUDA versions, these numbers refers to the NVIDIA GPU architectures, such as 7.5 for the Turing architecture and 8.x for the Ampere architecture.</p>

<p>Here is a helpful table for reference, credit to <a href="https://stackoverflow.com/questions/68496906/pytorch-installation-for-different-cuda-architectures/74962874#74962874">dagelf</a></p>

<table>
  <thead>
    <tr>
      <th>nvcc tag</th>
      <th>TORCH_CUDA_ARCH_LIST</th>
      <th>GPU Arch</th>
      <th>Year</th>
      <th>eg. GPU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sm_50, sm_52 and sm_53</td>
      <td>5.0 5.1 5.3</td>
      <td>[Maxwell](https://en.wikipedia.org/wiki/Maxwell_(microarchitecture)) support</td>
      <td>2014</td>
      <td>GTX 9xx</td>
    </tr>
    <tr>
      <td>sm_60, sm_61, and sm_62</td>
      <td>6.0 6.1 6.2</td>
      <td>[Pascal](https://en.wikipedia.org/wiki/Pascal_(microarchitecture)) support</td>
      <td>2016</td>
      <td>10xx, Pxxx</td>
    </tr>
    <tr>
      <td>sm_70 and sm_72</td>
      <td>7.0 7.2</td>
      <td>[Volta](https://en.wikipedia.org/wiki/Volta_(microarchitecture)) support</td>
      <td>2017</td>
      <td>Titan V</td>
    </tr>
    <tr>
      <td>sm_75</td>
      <td>7.5</td>
      <td>[Turing](https://en.wikipedia.org/wiki/Turing_(microarchitecture)) support</td>
      <td>2018</td>
      <td>most 20xx</td>
    </tr>
    <tr>
      <td>sm_80, sm_86 and sm_87</td>
      <td>8.0 8.6 8.7</td>
      <td>[Ampere](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) support</td>
      <td>2020</td>
      <td>RTX 30xx, Axx[xx]</td>
    </tr>
    <tr>
      <td>sm_89</td>
      <td>8.9</td>
      <td>[Ada](https://en.wikipedia.org/wiki/Ada_Lovelace_(microarchitecture)) support</td>
      <td>2022</td>
      <td>RTX xxxx</td>
    </tr>
    <tr>
      <td>sm_90, sm_90a</td>
      <td>9.0 9.0a</td>
      <td>[Hopper](https://en.wikipedia.org/wiki/Hopper_(microarchitecture)) support</td>
      <td>2022</td>
      <td>H100</td>
    </tr>
  </tbody>
</table>

<p>Back to our scenarios, we need check whether PyTorch 1.12.1 can be compatible with NVIDIA Ampere GPU</p>

<p>The quickest step towards judging the capability is to check if the application binary already contains compatible GPU code. As long as PyTorch libraries are built to include GPU arch&gt;=8.0or PTX form or both in  in  <code class="language-plaintext highlighter-rouge">TORCH_CUDA_ARCH_LIST</code>, they should work smoothly with the NVIDIA Ampere GPU architecture.</p>

<p>If the PyTorch libraries you are using is either compiled with corresponding <code class="language-plaintext highlighter-rouge">TORCH_CUDA_ARCH_LIST</code>, nor compiled in PTX, you can find an error like:</p>

<blockquote>
  <p>A100-PCIE-40Gb with CUDA capability sm_80 is not compatible with current PyTorch installation</p>

  <p>The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70</p>
</blockquote>

<p>Back to our scenarios, this time, the combability test fails.</p>

<ul>
  <li><strong>Pytorch 1.12.1 fails to be compatible with  NVIDIA A100-PCIE-40Gb</strong> ❌</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Now we can certainly know if the service which is built with <strong>PyTorch 1.12.1</strong>, and based on <strong>nvidia-cuda:10.2-base-ubuntu20.04</strong>, is compatible with an <strong>NVIDIA A100-PCIE-40Gb</strong> machine with <strong>CUDA 11.2</strong> and <strong>Driver Version 460.32.03</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Compatibility</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CUDA and Base Image</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>PyTorch and GPU</td>
      <td>❌</td>
    </tr>
    <tr>
      <td>PyTorch and CUDA</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>CUDA and GPU</td>
      <td>✅</td>
    </tr>
  </tbody>
</table>

<p>The answer is <u><b>NO</b></u>. Then, how do we fix it?</p>

<p>Since current PyTorch fails to be compatible with A100, we might want to upgrade to PyTorch 1.13.1 or even later version. Besisdes, since PyTorch 1.13.1 needs CUDA runtime api &gt;= 11.6, we also need to upgrade the base image with a runtime &gt;= 11.6. To be compatible with the CUDA runtime, you may also want to upgrade the host CUDA driver to the latest, like Driver Version: 525.116.03 which supports up to CUDA 11.7, but this is not necessary, since according to NVIDIA <a href="https://docs.nvidia.com/deploy/cuda-compatibility/#default-to-minor-version">compatibility document</a>.</p>

<table>
  <thead>
    <tr>
      <th>CUDA Toolkit</th>
      <th>**Linux x86_64 Minimum Required Driver Versio**</th>
      <th>Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CUDA 11.x</td>
      <td>$$\ge$$ 450.80.02*</td>
      <td>CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows) as indicated, <u>minor version compatibility is possible across the CUDA 11.x family of toolkits</u>.</td>
    </tr>
  </tbody>
</table>

<p>One good recipe is as below:</p>

<p><strong>host:</strong> NVIDIA A100-PCIE-40Gb, No driver update is necessary, but I prefer to keep it updated to the latest version.</p>

<p><strong>service:</strong> PyTorch: 1.13.1, base-image: <a href="https://hub.docker.com/layers/nvidia/cuda/11.7.1-base-ubuntu20.04/images/sha256-335148f1f4b11529269e668ff3ac57667e5f21458d7f461fd70d667699cf7819?context=explore">nvidia/cuda:11.7.1-base-ubuntu20.04</a></p>

<p>The compatitibilty matrix now passes all checks.</p>

<table>
  <thead>
    <tr>
      <th>Compatibility</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CUDA and Base Image</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>PyTorch and GPU</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>PyTorch and CUDA</td>
      <td>✅</td>
    </tr>
    <tr>
      <td>CUDA and GPU</td>
      <td>✅</td>
    </tr>
  </tbody>
</table>

<h2 id="one-more-thing">One More Thing</h2>

<p><strong>Q:</strong>  I initiate a container with a image without any CUDA runtime installed inside. Then, after I execute <code class="language-plaintext highlighter-rouge">docker run --gpus all  &lt;image_name&gt;</code>, I access the container and find all CUDA-related files on the host system, including CUDA runtime api. My assumption is that <code class="language-plaintext highlighter-rouge">--gpus all</code> will map all CUDA Toolkits to the CPU image, and thereby turn it to a CUDA runtime image. However, this assumption seems wrong for a container initilized from a CUDA 10.2 runtime base image in the same way, since all applications inside such a container still use CUDA 10.2 runtime API, suggesting that the host system’s CUDA runtime isn’t being mapped into the container. What the hell is going on?</p>

<p><strong>A:</strong> When you run a Docker container with the <code class="language-plaintext highlighter-rouge">--gpus all</code> flag, you enable that container to access the host’s GPUs. However, this does not mean that all CUDA-related files and libraries from the host are automatically mapped into the container. What happens under the hood may differ based on whether the Docker image itself contains CUDA runtime libraries or not.</p>

<h3 id="image-without-cuda-runtime">Image without CUDA runtime</h3>

<p>When you start a container based on an image that doesn’t contain any CUDA runtime libraries, and you use <code class="language-plaintext highlighter-rouge">--gpus all</code>, you might observe that certain CUDA functionalities are available in the container. This is often because NVIDIA’s Docker runtime (nvidia-docker) ensures that the minimum necessary libraries and binaries related to the GPU are mounted into the container, including the compatible CUDA driver libraries.</p>

<h3 id="image-with-cuda-runtime">Image with CUDA runtime</h3>

<p>If you start a container from an image that already has a specific CUDA runtime version (say, CUDA 10.2), the container will use that version for its operations. NVIDIA’s Docker runtime (nvidia-docker) generally won’t override the CUDA libraries in a container that already has them. The container is designed to be a standalone, consistent environment, and one of the benefits of using containers is that they package the application along with its dependencies, ensuring that it runs the same way regardless of where it’s deployed.</p>

<h3 class="no_toc"> Reference </h3>

<ul>
  <li><a href="https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html#building-applications-with-ampere-support">NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications</a></li>
  <li><a href="https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/">Matching CUDA arch and CUDA gencode for various NVIDIA architectures</a></li>
  <li><a href="https://pytorch.org/get-started/previous-versions/">Install previous versions of Pytorch under different CUDA machine</a></li>
  <li><a href="https://docs.nvidia.com/deploy/cuda-compatibility/#minor-version-compatibility">CUDA Compatibility Matrix from NVIDIA official documentations</a></li>
</ul>]]></content><author><name>Norm Inui</name></author><category term="Engineering" /><summary type="html"><![CDATA[TL; DR Host CUDA VS Base Image CUDA: The CUDA verision within a runtime docker image has no relationship with the CUDA version on the host machie. The only thing we need to care about is whether the driver version on the host supports the base image’s CUDA runtime. Check the driver compatibility here PyTorch VS CUDA: PyTorch is compatible with one or a few specific CUDA versions, more precisely, CUDA runtime APIs. Check the compatible matrix here CUDA VS GPU: Each GPU architecture is compatible with certain CUDA versions, more precisely, CUDA driver versions. Quick check here PyTorch and GPU: PyTorch only supports GPU specified in TORCH_CUDA_ARCH_LIST when compiled]]></summary></entry><entry><title type="html">Expand the Context Length with RoPE, Part 3 – Unlocking the Unlimited Extrapolation Potential with ReRoPE</title><link href="/Rethinking-Rotary-Position-Embedding-3/" rel="alternate" type="text/html" title="Expand the Context Length with RoPE, Part 3 – Unlocking the Unlimited Extrapolation Potential with ReRoPE" /><published>2023-08-16T00:00:00+00:00</published><updated>2023-08-16T00:00:00+00:00</updated><id>/Rethinking-Rotary-Position-Embedding-3</id><content type="html" xml:base="/Rethinking-Rotary-Position-Embedding-3/"><![CDATA[<blockquote>
  <p>Translated from the <a href="https://kexue.fm/archives/9708">post</a> and <a href="https://kexue.fm/archives/9728">post</a>, originally written in Chinese by Su, Jianlin</p>

  <p>Translated by Norm Inui</p>
</blockquote>

<h3 id="tl-dr">TL; DR</h3>

<ul>
  <li>
    <p>Introduce ReRoPE (Rectified RoPE), a post-processing optimization approach for RoPE.</p>
  </li>
  <li>
    <p>Experimental results reveal that ReRoPE’s extrapolation capabilities, without fine-tuning, significantly surpass the previous NTK-aware Scaled RoPE</p>
  </li>
  <li>
    <p>ReRoPE appears to consistently perform well across any length</p>
  </li>
  <li>
    <p>ReRoPE significantly reduces inference speed. However, training with ReRoPE and inferring with RoPE can benefit the extrapolation ability of LLMs without sacrificing throughput in inference</p>
  </li>
  <li>
    <p>Code is available <a href="https://github.com/bojone/rerope">here</a></p>
  </li>
</ul>

<hr />

<p>In the previous blog, I introduced the mixture-of-base encoding and believed we might have maxed out the potential of RoPE regarding extrapolation. It appeared we might need to explore another method for further extending on context length. It reminds me a method I previously set aside due to its complexity. Since we have run out of ideas, why not revisit it and see what we can learn from it. Sometimes, ‘The best solution is the only solution’.</p>

<p>Surprisingly, even though this method will increase time complexity, the experimental results are promising and even shows a potential to unlock the unlimited extrapolation ability of the language model. I can’t wait to write this article and share the method. Due to its similarity with the ReLU activation function, I’ve named this method <strong>ReRoPE (Rectified Rotary Position Embeddings)</strong></p>

<h3 id="background">Background</h3>

<p>We explain in the previous blog that although RoPE is regarded as an absolute position embedding, it can inject relative positional information into the Attention matrix with a Toeplitz matrix.</p>

\[\begin{equation} \begin{pmatrix}
0 &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp; &amp; \\ 
1 &amp; 0 &amp;  &amp;  &amp;  &amp;  &amp;  &amp; &amp; \\ 
2 &amp;  1&amp; 0 &amp;  &amp;  &amp;  &amp;  &amp; &amp; \\ 
3  &amp;  2&amp; 1 &amp; 0 &amp;  &amp;  &amp;  &amp; &amp; \\ 
\ddots  &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp;  &amp;  &amp; &amp; \\ 
\ddots  &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; \\ 
L-2 &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ 
L-1 &amp; L-2 &amp; \ddots &amp; \ddots &amp; \ddots &amp; 3 &amp; 2 &amp; 1 &amp; 0
\end{pmatrix} \end{equation}\]

<p>\(L\) is the input sequence length. When \(L\) is greatly larger than the pretrained max sequence length, the model typically exhibits poor extrapolation because it hasn’t been adequately trained on longer sequences.</p>

<p>The Position Interpolation modifies the Toeplitz matrix as:</p>

\[\begin{equation} \begin{pmatrix}
0 &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  &amp; &amp; \\ 
1/k &amp; 0 &amp;  &amp;  &amp;  &amp;  &amp;  &amp; &amp; \\ 
2/k &amp;  1/k&amp; 0 &amp;  &amp;  &amp;  &amp;  &amp; &amp; \\ 
3/k  &amp;  2/k&amp; 1/k &amp; 0 &amp;  &amp;  &amp;  &amp; &amp; \\ 
\ddots  &amp; 3/k &amp; 2/k &amp; 1/k &amp; 0 &amp;  &amp;  &amp; &amp; \\ 
\ddots  &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; \\ 
(L-2)/k &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ 
(L-1)/k &amp; (L-2)/k &amp; \ddots &amp; \ddots &amp; \ddots &amp; 3/k &amp; 2/k &amp; 1/k &amp; 0
\end{pmatrix} \end{equation}\]

<p>Position Interpolation (PI) ensures that the maximum relative position does not exceed the training length by tuning \(k\), therefore, it is free from any extrapolation on dimension. However, it makes each dimension carry more position information. Consequently, a few fine-tuning steps are necessary to get the model to adapt to the “crowded” dimension. Neural networks are often better at interpolation rather than extrapolation, just consider extrapolation as adding an extra dimension, while interpolation inserts more data into the already trained dimension. Intuitively, neural networks struggle with extrapolation. Therefore, PI is an efficient method to extend the context length with minimal fine-tuning.</p>

<p>As for the NTK-aware Scaled RoPE, it cleverly distributes the “crowded” dimension across every dimension. As a result, it can get even better perplexity value without fine-tuning. However, as we mention above,  neural networks struggle with extrapolation, which explains why an extended long context model can’t quite match a pretrained model with an identical max sequence length.</p>

<h3 id="combine-interpolation-and-extrapolation">Combine Interpolation and Extrapolation</h3>

<p>Let’s revisit extending methods we have through the lens of the definition of the locality. By mentioning ‘locality,’ we try to describe a preference of a language model when it predicts the next token, it heavily relies on the nearing tokens. Extrapolation preserves this locality since position encoding near 0s of the Toeptile matrix is unchanged, but its performance suffers due to the introduction of position encodings beyond the trained length. Although position interpolation doesn’t introduce extrapolated position encodings, it harms the locality since position encoding near 0 is compressed to \(1/k\), leading to necessary fine-tuning. On the other hand, NTK-aware Scaled RoPE combines the advantages of both methods by “high-frequency extrapolation and low-frequency interpolation”. This ensures the preservation of locality without introducing new position encoding, yielding good results even without fine-tuning.
Besides NTK Scaled RoPE, is there any other method that can realize both extrapolation and interpolation? The answer is <strong>YES</strong>.
Suppose we set a window with size \(w\), the interval between positions inside the window is \(1\), while the interval outside the window is \(1/k\), the Toepiltz matrix is shown as:</p>

\[\begin{equation}\begin{pmatrix} 
\color{red}{0} &amp; \\ 
\color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\ddots} &amp; \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \\ 
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\tiny{w + \frac{L-1-w}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\end{pmatrix}\end{equation}\]

<p>Numbers in \(\color{red} \text{red}\) are within the sliding window, in \(\color{green} \text{green}\) are outside the sliding window.</p>

<p>By adjusting \(k\), we can ensure \(w &lt; \text{max pretraining length}\), which allows us to maintain locality while keeping the position encoding within the pretraining length. This sliding window approach to the input sequence achieves interpolation outside the window and preserves locality within the window concurrently.</p>

<p>Moreover, when we extend the context length \(\to \infty\), then \(k \to \infty\), the matrix can be formulated as:</p>

\[\begin{equation}\begin{pmatrix} 
\color{red}{0} &amp; \\ 
\color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{w} &amp; \color{green}{w} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{w} &amp; \color{green}{w} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\ddots} &amp; \color{green}{w} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \\ 
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\color{green}{w} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\ 
\end{pmatrix}\end{equation}\]

<p>We can notice the locality can still be preserved within the window.
In conclusion, we can find a relation between <strong>eq(3)</strong>, <strong>eq(4)</strong> and <strong>eq(1)</strong></p>

<p>\(eq(3) = \text{LeakyReLU}(eq(1))\) and \(eq(4) = \text{ReLU}(eq(1))\)</p>

<h3 id="computation-cost">Computation Cost</h3>

<p>The concept of incorporating sliding windows into the input sequence is not new today and has been widely used in Attention Bias, like T5-bias, and relative position embedding. Yet, integrating a sliding window with RoPE can increase computation costs. Regarding <strong>eq(3)</strong> and <strong>eq(4)</strong>, since the values in each matrix row don’t increase linearly, RoPE needs to encode twice: one for positions within the window and another for those outside. These encodings are then combined together as ReRoPE.</p>

<p>To be specific, we compute attention scores with RoPE position encoding within the window:</p>

\[a_{i,j}^{(1)} = (R^i q_i)^T(R^j k_j) = q_i^T R^{j-i} k_j\]

<p>\(R\) is the RoPE rotation matrix, we omit the attention scale factor and softmax for concise. 
Then we compute attention scores outside the window, whose interval between numbers is \(1/k\). We denote this equation as Leaky ReRoPE:</p>

\[a_{i,j}^{(2)} = (R^{(i-w)/k+w} q_i)^T(R^{j/k} k_j) = q_i^T R^{(j-i+w)/k - w} k_j\]

<p>When \(k \to \infty\), the equation is simpler:</p>

\[a_{i,j}^{(2)} = (R^{w} q_i)^T k_j = q_i^T R^{w} k_j\]

<p>Let’s combine them together:</p>

\[\begin{equation}
    a_{i,j}=
    \begin{cases}
      a^{(1)}_{i,j}, &amp;  i -j &lt; w\\
      a^{(2)}_{i,j}, &amp; i -j \ge w
    \end{cases}
  \end{equation}\]

<p>According to the equations, we can notice both ReRoPE and Leaky ReRoPE inevitably require calculating the Attention matrix twice. If you have a more efficient implementation, please feel free to contact me. Moreover, this Attention matrix cannot directly be optimized with the current flash attention implementation, leading to more computational cost.</p>

<p>On the other hand, the non-linear relative positioning means that during autoregressive decoding, only the RoPE keys within the window can be cached. As the sequence length increases, keys that were once inside the window shift outside, and they need to be recomputed and appended with the cached keys for decoding tokens beyond the maximum sequence length. This process amplifies computation cost during inference. In token-by-token decoding, the query sequence length after the input prompt is merely \(1\). Unless the prompt exceeds the maximum sequence length, only the keys need to be recalculated.</p>

\[\begin{equation}a_{i,j} = \left\{\begin{aligned} 
&amp;\boldsymbol{q}_i^{\top}\left(\boldsymbol{\mathcal{R}}^{\max(j-i,-w)}\boldsymbol{k}_j\right), \quad(\text{ReRoPE})\\[8pt] 
&amp;\boldsymbol{q}_i^{\top}\left(\boldsymbol{\mathcal{R}}^{\max(j-i,(j-i+w)/k-w)}\boldsymbol{k}_j\right), \quad(\text{Leaky ReRoPE}) 
\end{aligned}\right.\end{equation}\]

<p>However, using ReRoPE/Leaky ReRoPE  in LLMs is computationally intensive. While it enables LLMs to process longer extended contexts, the input length during inference often exceeds the pretrained maximum sequence length. This results in significant latency, making it challenging for real-time applications.</p>

<p>What if we train with ReRoPE/Leaky ReRoPE but infer using standard RoPE? ReRoPE/Leaky ReRoPE serves as an extrapolation method for the ideal goal: “Train Short, Test Long”. Training an LLM with ReRoPE/Leaky ReRoPE certainly demands more time; however, this slowdown during training is acceptable when compared to the potential drop in inference speed.</p>

<p>To be specific, when a model is trained with RoPE and its context length is extrapolated using LeakyReRoPE, the interval outside the window is  \(1\) during training and \(\dfrac{1}{k} &lt; 1\) during inference. When swapping the embedding strategy, the model is trained with an interval greater than  \(1\) but infers with an interval of \(1\). This means that, during inference, LeakyReRoPE behaves like RoPE. We refer to this approach as InvLeaky ReRoPE (Inverse Leaky ReRoPE). <strong>Table 5</strong> demonstrates the effectiveness of this strategy. Since the embedding behaves like RoPE at inference, optimization techniques like FlashAttenion can be seamlessly integrated. After experimenting the different hyperparameters, we propose the empirical optimal parameter rule:</p>

<p>expanding scale:</p>

\[b = \dfrac{\text{expanded\_len}}{\text{max\_seq\_len}}\]

<p>number interval outside window:</p>

\[k=\dfrac{1}{2 b}\]

<p>window size:</p>

\[w = \dfrac{\text{max\_seq\_len}}{4}\]

<p>In <strong>Table 5</strong>, the model has 100M parameters, with a training length of 512. The training time for every 1,000 steps grows from \(330\) seconds to \(350\) seconds, an increase less than \(10\%\). Since the model is a hybrid of Transformer and GAU (Gated Attention Unit), with single-head attention in HAU. As for a multi-head attention LLM, the time increase could be more significant, possibly up to \(50\%\), but it is still acceptable.</p>

<h3 id="ablation-experiments">Ablation Experiments</h3>
<p>We follow the same experiment setup as in <a href="https://normxu.github.io/Rethinking-Rotary-Position-Embedding/">part 1</a> on an 100M <a href="https://arxiv.org/abs/2202.10447">GAU</a> model. The result is shown below.</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>49.41%</td>
      <td>24.17%</td>
      <td>23.16%                   \</td>
    </tr>
    <tr>
      <td>Baseline-$$\log n$$</td>
      <td>49.40%</td>
      <td>24.60%</td>
      <td>24.02%</td>
    </tr>
    <tr>
      <td>PI-RoPE</td>
      <td>49.41%</td>
      <td>15.04%</td>
      <td>13.54%                   \</td>
    </tr>
    <tr>
      <td>PI-RoPE-$$\log n$$</td>
      <td>49.40%</td>
      <td>14.99%</td>
      <td>16.51%</td>
    </tr>
    <tr>
      <td>NTK-RoPE</td>
      <td>49.41%</td>
      <td>51.28%</td>
      <td>39.27%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$</td>
      <td>49.40%</td>
      <td>61.71%</td>
      <td>43.75%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-fixed</td>
      <td>49.41%</td>
      <td>51.86%</td>
      <td>39.61%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n^{*}$$-fixed</td>
      <td>49.41%</td>
      <td>55.94%</td>
      <td>41.11%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$-fixed</td>
      <td>49.40%</td>
      <td>62.85%</td>
      <td>44.14%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-mixed</td>
      <td>49.41%</td>
      <td>53.09%</td>
      <td>40.12%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n^{*}$$-mixed</td>
      <td>49.41%</td>
      <td>59.11%</td>
      <td>42.38%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$-mixed</td>
      <td>49.40%</td>
      <td>68.91%</td>
      <td>45.41%</td>
    </tr>
    <tr>
      <td>ReRoPE-w256</td>
      <td>49.41%</td>
      <td>77.90%</td>
      <td>48.48%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>82.40%</td>
      <td>48.85%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n$$</td>
      <td>49.40%</td>
      <td>***<u>85.12%</u>***</td>
      <td>***<u>49.07%</u>***</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: the average accuracy of predicting next token to match the ground-truth next token given previous context. The experiment is based on a hybrid Transformer-GAU (Gated Attention Unit) model with a size of 100M parameters. \(\log n\) indicates we add the scale factor \(\log n\) at pretraining stage; \(\log n^{*}\) denotes we apply the scale factor \(\log n\) is applied to the attention matrix only for text exceeding the max sequence length, without any pretraining ; \(w256\) denotes \(w=256\)</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ReRoPE-w64</td>
      <td>49.41%</td>
      <td>69.39%</td>
      <td>45.19%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w64-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>78.58%</td>
      <td>47.42%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w64-$$\log n$$</td>
      <td>49.40%</td>
      <td>84.38%</td>
      <td>48.14%</td>
    </tr>
    <tr>
      <td>ReRoPE-w128</td>
      <td>49.41%</td>
      <td>76.11%</td>
      <td>47.82%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w128-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>82.28%</td>
      <td>48.78%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w128-$$\log n$$</td>
      <td>49.40%</td>
      <td>***<u>85.47%</u>***</td>
      <td>48.87%</td>
    </tr>
    <tr>
      <td>ReRoPE-w256</td>
      <td>49.41%</td>
      <td>77.90%</td>
      <td>48.48%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>82.40%</td>
      <td>48.85%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n$$</td>
      <td>49.40%</td>
      <td>85.12%</td>
      <td>***<u>49.07%</u>***</td>
    </tr>
    <tr>
      <td>ReRoPE-w384</td>
      <td>49.41%</td>
      <td>70.72%</td>
      <td>48.15%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w384-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>76.42%</td>
      <td>48.31%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w384-$$\log n$$</td>
      <td>49.40%</td>
      <td>83.24%</td>
      <td>48.62%</td>
    </tr>
    <tr>
      <td>ReRoPE-w512</td>
      <td>49.41%</td>
      <td>7.09%</td>
      <td>8.25%                    \</td>
    </tr>
    <tr>
      <td>ReRoPE-w512-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>7.08%</td>
      <td>8.25%                    \</td>
    </tr>
    <tr>
      <td>ReRoPE-w512-$$\log n$$</td>
      <td>49.40%</td>
      <td>15.84%</td>
      <td>10.83%</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2</strong>: Ablation on window size of ReRoPE; experiment setting is the same as <strong>Table 1</strong></p>

<p>From <strong>Table 2</strong>, we can learn \(w\) is robust to the performance; the optimal <strong>w</strong> is \(1/4\) to \(1/2\) of the pretraining max sequence length.</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ReRoPE-w128-$$\log n$$</td>
      <td>49.40%</td>
      <td>***<u>85.47%</u>***</td>
      <td>48.87%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w128-k64-$$\log n$$</td>
      <td>49.40%</td>
      <td>85.29%</td>
      <td>48.96%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w128-k32-$$\log n$$</td>
      <td>49.40%</td>
      <td>85.31%</td>
      <td>49.03%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w128-k16-$$\log n$$</td>
      <td>49.40%</td>
      <td>85.15%</td>
      <td>***<u>49.10%</u>***      \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w128-k8-$$\log n$$</td>
      <td>49.40%</td>
      <td>80.00%</td>
      <td>48.11%</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n$$</td>
      <td>49.40%</td>
      <td>85.12%</td>
      <td>49.07%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w256-k64-$$\log n$$</td>
      <td>49.40%</td>
      <td>84.60%</td>
      <td>49.03%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w256-k32-$$\log n$$</td>
      <td>49.40%</td>
      <td>84.30%</td>
      <td>48.97%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w256-k16-$$\log n$$</td>
      <td>49.40%</td>
      <td>83.59%</td>
      <td>48.87%                   \</td>
    </tr>
    <tr>
      <td>Leaky-ReRoPE-w256-k8-$$\log n$$</td>
      <td>49.40%</td>
      <td>69.80%</td>
      <td>45.72%</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3</strong>: Ablation on interval \(k\) of Leaky ReRoPE and ReRoPE; experiment setting is the same as <strong>Table 1</strong></p>

<p>From <strong>Table 3</strong>: Fine-tuned Leaky ReRoPE, as a generalization of ReRoPE, might slightly surpass ReRoPE’s performance, though the gains are minimal. When setting \(k\) to a finite value, there’s an inherent limitation on the maximum length it can manage. Since predicting the length LLM will generate in advance is impossible, we usually set a large value for \(k\). However, even with a sufficiently large \(k\), a siginificant long input could severely degrade performance due to position encoding surpassing the trained length. While ReRoPE doesn’t have such an issue. In practical applications, fine-tuned Leaky ReRoPE may not be as universally adaptable as ReRoPE.</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>4096(trained)</th>
      <th>8192</th>
      <th>16384</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RoPE</td>
      <td>1.4967</td>
      <td>8.8615</td>
      <td>—</td>
    </tr>
    <tr>
      <td>NTK-RoPE</td>
      <td>1.6081</td>
      <td>1.5417</td>
      <td>1.5163</td>
    </tr>
    <tr>
      <td>ReRoPE</td>
      <td>1.4996</td>
      <td>1.4267</td>
      <td>1.4001</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 4</strong>: Experiments on LLaMa-2-13B, the value represent loss; smaller is better.</p>

<p>ReRoPE effectively achieves near-optimal results, aligning with our intuition that “longer context results in lower loss”, given that an extended context should benefit LLM comprehension ability. Furthermore, I evaluated the chat capabilities of the LLAMA2-13b model, open-source by <a href="https://huggingface.co/OpenBuddy">OpenBuddy</a>, and found its performance satisfying with an input length up to 20k tokens.</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>49.41%</td>
      <td>24.17%</td>
      <td>23.16%                   \</td>
    </tr>
    <tr>
      <td>Baseline-$$\log n$$</td>
      <td>49.40%</td>
      <td>24.60%</td>
      <td>24.02%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-fixed</td>
      <td>49.41%</td>
      <td>51.86%</td>
      <td>39.61%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n^{*}$$-fixed</td>
      <td>49.41%</td>
      <td>55.94%</td>
      <td>41.11%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$-fixed</td>
      <td>49.40%</td>
      <td>62.85%</td>
      <td>44.14%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-mixed</td>
      <td>49.41%</td>
      <td>53.09%</td>
      <td>40.12%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n^{*}$$-mixed</td>
      <td>49.41%</td>
      <td>59.11%</td>
      <td>42.38%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$-mixed</td>
      <td>49.40%</td>
      <td>68.91%</td>
      <td>45.41%</td>
    </tr>
    <tr>
      <td>ReRoPE-w256</td>
      <td>49.41%</td>
      <td>77.90%</td>
      <td>48.48%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n^{*}$$</td>
      <td>49.41%</td>
      <td>82.40%</td>
      <td>48.85%                   \</td>
    </tr>
    <tr>
      <td>ReRoPE-w256-$$\log n$$</td>
      <td>49.40%</td>
      <td>***<u>85.12%</u>***</td>
      <td>***<u>49.07%</u>***</td>
    </tr>
    <tr>
      <td>InvLeaky ReRoPE-w128-$$\log n$$</td>
      <td>49.38%</td>
      <td>82.25%</td>
      <td>48.32%                   \</td>
    </tr>
    <tr>
      <td>InvLeaky ReRoPE-w128-b8-$$\log n$$</td>
      <td>49.62%</td>
      <td>81.15%</td>
      <td>48.85%</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 5</strong>: Experiment setting is the same as <strong>Table 1</strong>； b8: replace the RoPE base from \(10000\) to \(80000\); InvLeaky ReRoPE is inferior to ReRoPE, but still promising compared to vanilla NTK-RoPE</p>

<p>The ReRoPE and Leaky ReRoPE codes can be found here. Feel free to play with it.</p>

<blockquote>
  <p><strong>Github: <a href="https://github.com/bojone/rerope">https://github.com/bojone/rerope</a></strong></p>
</blockquote>]]></content><author><name>Norm Inui</name></author><category term="LLM" /><summary type="html"><![CDATA[Translated from the post and post, originally written in Chinese by Su, Jianlin Translated by Norm Inui TL; DR Introduce ReRoPE (Rectified RoPE), a post-processing optimization approach for RoPE. Experimental results reveal that ReRoPE’s extrapolation capabilities, without fine-tuning, significantly surpass the previous NTK-aware Scaled RoPE ReRoPE appears to consistently perform well across any length ReRoPE significantly reduces inference speed. However, training with ReRoPE and inferring with RoPE can benefit the extrapolation ability of LLMs without sacrificing throughput in inference Code is available here In the previous blog, I introduced the mixture-of-base encoding and believed we might have maxed out the potential of RoPE regarding extrapolation. It appeared we might need to explore another method for further extending on context length. It reminds me a method I previously set aside due to its complexity. Since we have run out of ideas, why not revisit it and see what we can learn from it. Sometimes, ‘The best solution is the only solution’. Surprisingly, even though this method will increase time complexity, the experimental results are promising and even shows a potential to unlock the unlimited extrapolation ability of the language model. I can’t wait to write this article and share the method. Due to its similarity with the ReLU activation function, I’ve named this method ReRoPE (Rectified Rotary Position Embeddings) Background We explain in the previous blog that although RoPE is regarded as an absolute position embedding, it can inject relative positional information into the Attention matrix with a Toeplitz matrix. [\begin{equation} \begin{pmatrix} 0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \ 1 &amp; 0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \ 2 &amp; 1&amp; 0 &amp; &amp; &amp; &amp; &amp; &amp; \ 3 &amp; 2&amp; 1 &amp; 0 &amp; &amp; &amp; &amp; &amp; \ \ddots &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; &amp; &amp; &amp; \ \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; \ L-2 &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ L-1 &amp; L-2 &amp; \ddots &amp; \ddots &amp; \ddots &amp; 3 &amp; 2 &amp; 1 &amp; 0 \end{pmatrix} \end{equation}] \(L\) is the input sequence length. When \(L\) is greatly larger than the pretrained max sequence length, the model typically exhibits poor extrapolation because it hasn’t been adequately trained on longer sequences. The Position Interpolation modifies the Toeplitz matrix as: [\begin{equation} \begin{pmatrix} 0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \ 1/k &amp; 0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; \ 2/k &amp; 1/k&amp; 0 &amp; &amp; &amp; &amp; &amp; &amp; \ 3/k &amp; 2/k&amp; 1/k &amp; 0 &amp; &amp; &amp; &amp; &amp; \ \ddots &amp; 3/k &amp; 2/k &amp; 1/k &amp; 0 &amp; &amp; &amp; &amp; \ \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; \ (L-2)/k &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ (L-1)/k &amp; (L-2)/k &amp; \ddots &amp; \ddots &amp; \ddots &amp; 3/k &amp; 2/k &amp; 1/k &amp; 0 \end{pmatrix} \end{equation}] Position Interpolation (PI) ensures that the maximum relative position does not exceed the training length by tuning \(k\), therefore, it is free from any extrapolation on dimension. However, it makes each dimension carry more position information. Consequently, a few fine-tuning steps are necessary to get the model to adapt to the “crowded” dimension. Neural networks are often better at interpolation rather than extrapolation, just consider extrapolation as adding an extra dimension, while interpolation inserts more data into the already trained dimension. Intuitively, neural networks struggle with extrapolation. Therefore, PI is an efficient method to extend the context length with minimal fine-tuning. As for the NTK-aware Scaled RoPE, it cleverly distributes the “crowded” dimension across every dimension. As a result, it can get even better perplexity value without fine-tuning. However, as we mention above, neural networks struggle with extrapolation, which explains why an extended long context model can’t quite match a pretrained model with an identical max sequence length. Combine Interpolation and Extrapolation Let’s revisit extending methods we have through the lens of the definition of the locality. By mentioning ‘locality,’ we try to describe a preference of a language model when it predicts the next token, it heavily relies on the nearing tokens. Extrapolation preserves this locality since position encoding near 0s of the Toeptile matrix is unchanged, but its performance suffers due to the introduction of position encodings beyond the trained length. Although position interpolation doesn’t introduce extrapolated position encodings, it harms the locality since position encoding near 0 is compressed to \(1/k\), leading to necessary fine-tuning. On the other hand, NTK-aware Scaled RoPE combines the advantages of both methods by “high-frequency extrapolation and low-frequency interpolation”. This ensures the preservation of locality without introducing new position encoding, yielding good results even without fine-tuning. Besides NTK Scaled RoPE, is there any other method that can realize both extrapolation and interpolation? The answer is YES. Suppose we set a window with size \(w\), the interval between positions inside the window is \(1\), while the interval outside the window is \(1/k\), the Toepiltz matrix is shown as: [\begin{equation}\begin{pmatrix} \color{red}{0} &amp; \ \color{red}{1} &amp; \color{red}{0} &amp; \ \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\ddots} &amp; \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \ \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\tiny{w + \frac{L-1-w}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\tiny{w + \frac{2}{k}}} &amp; \color{green}{\tiny{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \end{pmatrix}\end{equation}] Numbers in \(\color{red} \text{red}\) are within the sliding window, in \(\color{green} \text{green}\) are outside the sliding window. By adjusting \(k\), we can ensure \(w &lt; \text{max pretraining length}\), which allows us to maintain locality while keeping the position encoding within the pretraining length. This sliding window approach to the input sequence achieves interpolation outside the window and preserves locality within the window concurrently. Moreover, when we extend the context length \(\to \infty\), then \(k \to \infty\), the matrix can be formulated as: [\begin{equation}\begin{pmatrix} \color{red}{0} &amp; \ \color{red}{1} &amp; \color{red}{0} &amp; \ \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{w} &amp; \color{green}{w} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{w} &amp; \color{green}{w} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\ddots} &amp; \color{green}{w} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \ \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \color{green}{w} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{green}{w} &amp; \color{red}{\tiny{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \ \end{pmatrix}\end{equation}] We can notice the locality can still be preserved within the window. In conclusion, we can find a relation between eq(3), eq(4) and eq(1) \(eq(3) = \text{LeakyReLU}(eq(1))\) and \(eq(4) = \text{ReLU}(eq(1))\) Computation Cost The concept of incorporating sliding windows into the input sequence is not new today and has been widely used in Attention Bias, like T5-bias, and relative position embedding. Yet, integrating a sliding window with RoPE can increase computation costs. Regarding eq(3) and eq(4), since the values in each matrix row don’t increase linearly, RoPE needs to encode twice: one for positions within the window and another for those outside. These encodings are then combined together as ReRoPE. To be specific, we compute attention scores with RoPE position encoding within the window: [a_{i,j}^{(1)} = (R^i q_i)^T(R^j k_j) = q_i^T R^{j-i} k_j] \(R\) is the RoPE rotation matrix, we omit the attention scale factor and softmax for concise. Then we compute attention scores outside the window, whose interval between numbers is \(1/k\). We denote this equation as Leaky ReRoPE: [a_{i,j}^{(2)} = (R^{(i-w)/k+w} q_i)^T(R^{j/k} k_j) = q_i^T R^{(j-i+w)/k - w} k_j] When \(k \to \infty\), the equation is simpler: [a_{i,j}^{(2)} = (R^{w} q_i)^T k_j = q_i^T R^{w} k_j] Let’s combine them together: [\begin{equation} a_{i,j}= \begin{cases} a^{(1)}{i,j}, &amp; i -j &lt; w a^{(2)}{i,j}, &amp; i -j \ge w \end{cases} \end{equation}] According to the equations, we can notice both ReRoPE and Leaky ReRoPE inevitably require calculating the Attention matrix twice. If you have a more efficient implementation, please feel free to contact me. Moreover, this Attention matrix cannot directly be optimized with the current flash attention implementation, leading to more computational cost. On the other hand, the non-linear relative positioning means that during autoregressive decoding, only the RoPE keys within the window can be cached. As the sequence length increases, keys that were once inside the window shift outside, and they need to be recomputed and appended with the cached keys for decoding tokens beyond the maximum sequence length. This process amplifies computation cost during inference. In token-by-token decoding, the query sequence length after the input prompt is merely \(1\). Unless the prompt exceeds the maximum sequence length, only the keys need to be recalculated. [\begin{equation}a_{i,j} = \left{\begin{aligned} &amp;\boldsymbol{q}_i^{\top}\left(\boldsymbol{\mathcal{R}}^{\max(j-i,-w)}\boldsymbol{k}_j\right), \quad(\text{ReRoPE})\[8pt] &amp;\boldsymbol{q}_i^{\top}\left(\boldsymbol{\mathcal{R}}^{\max(j-i,(j-i+w)/k-w)}\boldsymbol{k}_j\right), \quad(\text{Leaky ReRoPE}) \end{aligned}\right.\end{equation}] However, using ReRoPE/Leaky ReRoPE in LLMs is computationally intensive. While it enables LLMs to process longer extended contexts, the input length during inference often exceeds the pretrained maximum sequence length. This results in significant latency, making it challenging for real-time applications. What if we train with ReRoPE/Leaky ReRoPE but infer using standard RoPE? ReRoPE/Leaky ReRoPE serves as an extrapolation method for the ideal goal: “Train Short, Test Long”. Training an LLM with ReRoPE/Leaky ReRoPE certainly demands more time; however, this slowdown during training is acceptable when compared to the potential drop in inference speed. To be specific, when a model is trained with RoPE and its context length is extrapolated using LeakyReRoPE, the interval outside the window is \(1\) during training and \(\dfrac{1}{k} &lt; 1\) during inference. When swapping the embedding strategy, the model is trained with an interval greater than \(1\) but infers with an interval of \(1\). This means that, during inference, LeakyReRoPE behaves like RoPE. We refer to this approach as InvLeaky ReRoPE (Inverse Leaky ReRoPE). Table 5 demonstrates the effectiveness of this strategy. Since the embedding behaves like RoPE at inference, optimization techniques like FlashAttenion can be seamlessly integrated. After experimenting the different hyperparameters, we propose the empirical optimal parameter rule: expanding scale: [b = \dfrac{\text{expanded_len}}{\text{max_seq_len}}] number interval outside window: [k=\dfrac{1}{2 b}] window size: [w = \dfrac{\text{max_seq_len}}{4}] In Table 5, the model has 100M parameters, with a training length of 512. The training time for every 1,000 steps grows from \(330\) seconds to \(350\) seconds, an increase less than \(10\%\). Since the model is a hybrid of Transformer and GAU (Gated Attention Unit), with single-head attention in HAU. As for a multi-head attention LLM, the time increase could be more significant, possibly up to \(50\%\), but it is still acceptable. Ablation Experiments We follow the same experiment setup as in part 1 on an 100M GAU model. The result is shown below. context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) Baseline 49.41% 24.17% 23.16% \ Baseline-$$\log n$$ 49.40% 24.60% 24.02% PI-RoPE 49.41% 15.04% 13.54% \ PI-RoPE-$$\log n$$ 49.40% 14.99% 16.51% NTK-RoPE 49.41% 51.28% 39.27% \ NTK-RoPE-$$\log n$$ 49.40% 61.71% 43.75% NTK-RoPE-fixed 49.41% 51.86% 39.61% \ NTK-RoPE-$$\log n^{*}$$-fixed 49.41% 55.94% 41.11% \ NTK-RoPE-$$\log n$$-fixed 49.40% 62.85% 44.14% \ NTK-RoPE-mixed 49.41% 53.09% 40.12% \ NTK-RoPE-$$\log n^{*}$$-mixed 49.41% 59.11% 42.38% \ NTK-RoPE-$$\log n$$-mixed 49.40% 68.91% 45.41% ReRoPE-w256 49.41% 77.90% 48.48% \ ReRoPE-w256-$$\log n^{*}$$ 49.41% 82.40% 48.85% \ ReRoPE-w256-$$\log n$$ 49.40% ***85.12%*** ***49.07%*** Table 1: the average accuracy of predicting next token to match the ground-truth next token given previous context. The experiment is based on a hybrid Transformer-GAU (Gated Attention Unit) model with a size of 100M parameters. \(\log n\) indicates we add the scale factor \(\log n\) at pretraining stage; \(\log n^{*}\) denotes we apply the scale factor \(\log n\) is applied to the attention matrix only for text exceeding the max sequence length, without any pretraining ; \(w256\) denotes \(w=256\) context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) ReRoPE-w64 49.41% 69.39% 45.19% \ ReRoPE-w64-$$\log n^{*}$$ 49.41% 78.58% 47.42% \ ReRoPE-w64-$$\log n$$ 49.40% 84.38% 48.14% ReRoPE-w128 49.41% 76.11% 47.82% \ ReRoPE-w128-$$\log n^{*}$$ 49.41% 82.28% 48.78% \ ReRoPE-w128-$$\log n$$ 49.40% ***85.47%*** 48.87% ReRoPE-w256 49.41% 77.90% 48.48% \ ReRoPE-w256-$$\log n^{*}$$ 49.41% 82.40% 48.85% \ ReRoPE-w256-$$\log n$$ 49.40% 85.12% ***49.07%*** ReRoPE-w384 49.41% 70.72% 48.15% \ ReRoPE-w384-$$\log n^{*}$$ 49.41% 76.42% 48.31% \ ReRoPE-w384-$$\log n$$ 49.40% 83.24% 48.62% ReRoPE-w512 49.41% 7.09% 8.25% \ ReRoPE-w512-$$\log n^{*}$$ 49.41% 7.08% 8.25% \ ReRoPE-w512-$$\log n$$ 49.40% 15.84% 10.83% Table 2: Ablation on window size of ReRoPE; experiment setting is the same as Table 1 From Table 2, we can learn \(w\) is robust to the performance; the optimal w is \(1/4\) to \(1/2\) of the pretraining max sequence length. context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) ReRoPE-w128-$$\log n$$ 49.40% ***85.47%*** 48.87% \ Leaky-ReRoPE-w128-k64-$$\log n$$ 49.40% 85.29% 48.96% \ Leaky-ReRoPE-w128-k32-$$\log n$$ 49.40% 85.31% 49.03% \ Leaky-ReRoPE-w128-k16-$$\log n$$ 49.40% 85.15% ***49.10%*** \ Leaky-ReRoPE-w128-k8-$$\log n$$ 49.40% 80.00% 48.11% ReRoPE-w256-$$\log n$$ 49.40% 85.12% 49.07% \ Leaky-ReRoPE-w256-k64-$$\log n$$ 49.40% 84.60% 49.03% \ Leaky-ReRoPE-w256-k32-$$\log n$$ 49.40% 84.30% 48.97% \ Leaky-ReRoPE-w256-k16-$$\log n$$ 49.40% 83.59% 48.87% \ Leaky-ReRoPE-w256-k8-$$\log n$$ 49.40% 69.80% 45.72% Table 3: Ablation on interval \(k\) of Leaky ReRoPE and ReRoPE; experiment setting is the same as Table 1 From Table 3: Fine-tuned Leaky ReRoPE, as a generalization of ReRoPE, might slightly surpass ReRoPE’s performance, though the gains are minimal. When setting \(k\) to a finite value, there’s an inherent limitation on the maximum length it can manage. Since predicting the length LLM will generate in advance is impossible, we usually set a large value for \(k\). However, even with a sufficiently large \(k\), a siginificant long input could severely degrade performance due to position encoding surpassing the trained length. While ReRoPE doesn’t have such an issue. In practical applications, fine-tuned Leaky ReRoPE may not be as universally adaptable as ReRoPE. context length 4096(trained) 8192 16384 RoPE 1.4967 8.8615 — NTK-RoPE 1.6081 1.5417 1.5163 ReRoPE 1.4996 1.4267 1.4001 Table 4: Experiments on LLaMa-2-13B, the value represent loss; smaller is better. ReRoPE effectively achieves near-optimal results, aligning with our intuition that “longer context results in lower loss”, given that an extended context should benefit LLM comprehension ability. Furthermore, I evaluated the chat capabilities of the LLAMA2-13b model, open-source by OpenBuddy, and found its performance satisfying with an input length up to 20k tokens. context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) Baseline 49.41% 24.17% 23.16% \ Baseline-$$\log n$$ 49.40% 24.60% 24.02% NTK-RoPE-fixed 49.41% 51.86% 39.61% \ NTK-RoPE-$$\log n^{*}$$-fixed 49.41% 55.94% 41.11% \ NTK-RoPE-$$\log n$$-fixed 49.40% 62.85% 44.14% NTK-RoPE-mixed 49.41% 53.09% 40.12% \ NTK-RoPE-$$\log n^{*}$$-mixed 49.41% 59.11% 42.38% \ NTK-RoPE-$$\log n$$-mixed 49.40% 68.91% 45.41% ReRoPE-w256 49.41% 77.90% 48.48% \ ReRoPE-w256-$$\log n^{*}$$ 49.41% 82.40% 48.85% \ ReRoPE-w256-$$\log n$$ 49.40% ***85.12%*** ***49.07%*** InvLeaky ReRoPE-w128-$$\log n$$ 49.38% 82.25% 48.32% \ InvLeaky ReRoPE-w128-b8-$$\log n$$ 49.62% 81.15% 48.85% Table 5: Experiment setting is the same as Table 1； b8: replace the RoPE base from \(10000\) to \(80000\); InvLeaky ReRoPE is inferior to ReRoPE, but still promising compared to vanilla NTK-RoPE The ReRoPE and Leaky ReRoPE codes can be found here. Feel free to play with it. Github: https://github.com/bojone/rerope]]></summary></entry><entry><title type="html">Expand the Context Length with RoPE, Part 2 – Further Research about β-Based Encoding</title><link href="/Rethinking-Rotary-Position-Embedding-2/" rel="alternate" type="text/html" title="Expand the Context Length with RoPE, Part 2 – Further Research about β-Based Encoding" /><published>2023-08-13T00:00:00+00:00</published><updated>2023-08-13T00:00:00+00:00</updated><id>/Rethinking-Rotary-Position-Embedding-2</id><content type="html" xml:base="/Rethinking-Rotary-Position-Embedding-2/"><![CDATA[<blockquote>
  <p>Translated from the <a href="https://kexue.fm/archives/9706">post</a>, originally written in Chinese by Su, Jianlin</p>

  <p>Translated by Norm Inui</p>
</blockquote>

<h3 id="tl-dr">TL; DR</h3>

<ul>
  <li>NTK-Scale RoPE has flaw</li>
  <li>Introduce a mixture-of-based encoding method, which can significantly enhance LLM performance beyond its pretraining max length, without the need for fine-tuning</li>
  <li>Introduce a scale factor \(\log n\) for attention calculation, which can be incorporated either during the pretraining phase or directly applied to an off-the-shell LLM</li>
</ul>
<hr />

<p>In <a href="https://normxu.github.io/Rethinking-Rotary-Position-Embedding/">part 1</a>, we interpret RoPE using a β-based encoding and demonstrated why NTK-aware Scaled RoPE can extend the context length without the need for fine-tuning. Viewing position encoding through the lens of β-based encoding indeed offers me some fresh insights and inspiration.</p>

<h3 id="modification-to-ntk">Modification to NTK</h3>
<p>Suppose we encode integer \(n\) in the \(\beta\)-base, and \(m\) is the digit of the representation counting from the right.</p>

\[\begin{equation} \lfloor\dfrac{n}{\beta^{m-1}}\rfloor \mod \beta \end{equation}\]

<p>If we represent it as a RoPE vector:</p>

\[\begin{equation} p_n = [\text{cos}\theta_1, \text{sin}\theta_1, \text{cos}\theta_2, \text{sin}\theta_2, …, \text{cos}\theta_{d/2}, \text{sin}\theta_{d/2}] \end{equation}\]

<p>where \(\theta_m = \dfrac{n}{\beta^{m-1}}\), \(\beta= 10000^{2/d}\)</p>

<p>We have successfully demonstrated that the NTK Scale RoPE exhibits extrapolation in the high-frequency dimension (for a large value of m), whereas it shows interpolation in the low-frequency dimension (for a small value of m). Since a densely interpolated dimension can harm the Language Model’s (LLM) to accurately compare relative positions, the NTK Scale RoPE successfully mitigates the comparison confusion posed by extrapolation from a base conversion perspective, and ensure each dimension is not too crowded. This approach significantly benefits LLMs that rely on relative positional cues to understand context, enabling them to effectively expand their contextual understanding over pretrained max sequence length without fine-tuning.</p>

<blockquote>
  <p>from translator: If you feel confused about how NTK Scale RoPE combines both interpolation and extrapolation together, I strongly suggest you read the <a href="https://normxu.github.io/Rethinking-Rotary-Position-Embedding/">part 1</a></p>
</blockquote>

<p>Now let’s review <strong>eq2</strong>, notice that cos and sin share the same rotation frequency, which means RoPE encodes n with a base of \(\beta\) into \(d/2\) digits. If we want to extend the context length by \(k\), the intuitive idea is to scale the \(\beta\) to \(\beta \lambda\), then:</p>

\[\lambda^{d/2}=k \Rightarrow \lambda=k^{2/d}\]

<p>Then, the RoPE becomes:</p>

\[\begin{equation} p_n = [\text{cos}\theta_1, \text{sin}\theta_1, \text{cos}\theta_2, \text{sin}\theta_2, …, \text{cos}\theta_{d/2}, \text{sin}\theta_{d/2}] \end{equation}\]

<p>where \(\theta_m = \dfrac{n}{(\beta\lambda)^{m-1}}\), \(\beta= 10000^{2/d}\), \(\lambda=k^{2/d}\)</p>

<p>This is how we implement NTK-RoPE.</p>

<p>However, back to <strong>eq1</strong>, we can see that if we want to encode \(n\) with a base of \(\beta \lambda\), the <strong>eq1</strong> should be:</p>

\[\begin{equation} \lfloor\dfrac{n}{(\beta\lambda)^{m-1}}\rfloor \mod (\beta\lambda) \end{equation}\]

<p>Therefore, our derivation from <strong>eq2</strong> to <strong>eq3</strong> has flaws, besides replacing the \(\dfrac{n}{\beta^{m-1}}\) with \(\dfrac{n}{(\beta\lambda)^{m-1}}\), the \(\text{mod}\) needs to scale up its period by \(\lambda\) as well, then the corrected Scaled RoPE should be:</p>

\[\begin{equation} p_n = [\text{cos}\theta_1, \text{sin}\theta_1, \text{cos}\theta_2, \text{sin}\theta_2, …, \text{cos}\theta_{d/2}, \text{sin}\theta_{d/2}] \end{equation}\]

<p>where \(\theta_m = \dfrac{n}{\lambda(\beta\lambda)^{m-1}}\), \(\beta= 10000^{2/d}\), \(\lambda=k^{2/d}\)</p>

<p>In the following context, we denote <strong>eq3</strong> as <strong>NTK-RoPE-old</strong>, and <strong>eq5</strong> as <strong>NTK-RoPE-fixed</strong>.</p>

<h3 id="why-a-mixture-of-base-is-necessary">Why a mixture of base is necessary</h3>

<p>If we can encode an integer in \(\beta\) base, how about generalizing to a mixed-based encoding where each digit is encoded in a different base? Just like the time system we daily use, 60 seconds make up 1 minute, 60 minutes equal 1 hour, 24 hours is 1 day, and 7 days amount to 1 week. Here, the numbers 60, 60, 24, and 7 can be regarded as different encoding bases. In essence, any timestamp can be encoded into seconds, minutes, hours, days, and weeks with the mixed-based system.
Counted from right to left, the first digit is encoded in \(\beta_1\), the second digit is in \(\beta_2\), and the third is in \(\beta_3\), …. The \(m\)th digit of an integer \(n\) can then be represented as:</p>

\[\begin{equation} \lfloor\dfrac{n}{\beta^{1}\beta^{1}...\beta^{m-1}}\rfloor \mod \beta_m \end{equation}\]

<p>Since RoPE is a relative position encoding, it can be viewed as a specific instance of the Toeplitz matrix, which looks like this (given our discussion mainly focuses on language models, the top-right part of the matrix is trimmed to fit the page).</p>

\[\begin{pmatrix}
0 \\
1 &amp; 0 &amp;  \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1  &amp; 0\\ 
4 &amp; 3 &amp; 2 &amp; 1 &amp; 0\\
5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0\\
6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0\\

\end{pmatrix}\]

<p>Upon the matrix, it is evident that the distribution of relative position encoding is not uniform! The 0 is the most frequent, followed by 1, 2, and so on. In other words, as \(n\) grows larger, its appearance becomes less frequent. This suggests that, as a form of \(\beta\)-base encoding, the higher bits of RoPE might be under-trained. This implies that the generalization capability of the higher bits might be inferior to the lower bits. As mentioned, NTK-RoPE mitigated the confusion introduced by extrapolation across all bits uniformly. However, if our hypothesis holds, this strategy might not be optimal. Lower bits can be more robust than higher bits and can hold a larger data range than the higher bits. Inspired by the timestamp encoding system, we should redesign RoPE with a mix-based encoding system.</p>
<h3 id="encoding-with-a-mixture-of-bases">Encoding with a mixture of bases</h3>
<p>To be specific, we extend the context length by \(k\) with a mixture of bases, \(\beta_1\), \(\beta_2\), \(...\), \(\beta_{d/2}\), where \(\beta_m = \beta\lambda_m\)</p>

<p>Thus, <strong>eq4</strong> shold be be written as:</p>

\[\begin{equation} \lfloor\dfrac{n}{\beta^{m-1}(\lambda_1\lambda_2…\lambda_{m-1})}\rfloor \mod (\beta\lambda_m) \end{equation}\]

<p>where \(\theta_m = \dfrac{n}{\beta^{m-1}(\lambda_1\lambda_2…\lambda_m)}\), \(\beta = 10000^{2/d}\)</p>

<p>According to the goal to ensure lower digits hold a larger range of data and to extend the context length by a scale factor \(k\), <strong>eq 7</strong> is subject to the conditions</p>

<p>\(\lambda_1\lambda_2…\lambda_m = k\) and  \(\lambda_1 \ge \lambda_2 \ge … \ge \lambda_{d/2} \ge 1\)</p>

<p>Given these two conditions, one possible solution is:</p>

<p>\(\lambda_1\lambda_2…\lambda_m = \text{exp}(am^b)\),  where \(a \ge 0\), \(b \le 1\)</p>

<blockquote>
  <p>from translator: The original post doesn’t cover any proof of this statement, please check Appendix for the proof I derive</p>
</blockquote>

<p>When \(b=1\),  \(\lambda_1 = \lambda_2 = … = \lambda_{d/2} &gt; 1\), we denote as “NTK-RoPE-fixed”;</p>

<p>when \(b=0\), \(\lambda_1 = \lambda_2 = … = \lambda_{d/2} = 1\), this exactly meets the definition of “Positional Interpolation (PI)”</p>

<p>Given one of the constrains we mention above:</p>

\[\lambda_1 \lambda_2 … \lambda_{d/2} =k\]

<p>We can derive:</p>

\[a(\dfrac{d}{2})^b = \log k\]

<p>\(b=0.625\) is an empirical value that can achieve optimal performance in an expanded long context; (Optimal values may vary across models, feel free to tune it), and we denoted this method as NTK-RoPE-mixed.</p>

<h2 id="experiment">Experiment</h2>
<p>We follow the same experiment setup as in part 1 and compare the NTK-RoPE-mixed and NTK-RoPE-fixed in an extended context.</p>

<p><strong>Table 1</strong></p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>49.41%</td>
      <td>24.17%</td>
      <td>23.16%                   \</td>
    </tr>
    <tr>
      <td>Baseline-$$\log n$$</td>
      <td>49.40%</td>
      <td>24.60%</td>
      <td>24.02%</td>
    </tr>
    <tr>
      <td>PI-RoPE</td>
      <td>49.41%</td>
      <td>15.04%</td>
      <td>13.54%                   \</td>
    </tr>
    <tr>
      <td>PI-RoPE-$$\log n$$</td>
      <td>49.40%</td>
      <td>14.99%</td>
      <td>16.51%</td>
    </tr>
    <tr>
      <td>NTK-RoPE</td>
      <td>49.41%</td>
      <td>51.28%</td>
      <td>39.27%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$</td>
      <td>49.40%</td>
      <td>61.71%</td>
      <td>43.75%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-fixed</td>
      <td>49.41%</td>
      <td>51.86%</td>
      <td>39.61%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$-fixed</td>
      <td>49.40%</td>
      <td>62.85%</td>
      <td>44.14%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-mixed</td>
      <td>49.41%</td>
      <td>53.09%</td>
      <td>40.12%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$-mixed</td>
      <td>49.40%</td>
      <td>***<u>68.91%</u>***</td>
      <td>***<u>45.41%</u>***</td>
    </tr>
  </tbody>
</table>

<p>From the <strong>Table 1</strong>, we can clearly see when compared to the “NTK-RoPE-old” and “NTK-RoPE-fixed,” the mixture-of-base “NTK-RoPE-mixed” shows a significant accuracy improvement without fine-tuning. This effectively provides a ‘free lunch’ approach to enhance LLM performance in a longer context. In addition, the table shows the scale factor \(\log n\) can benefit as well. But this trick requires \(\log n\) to be inserted into attention during the pre-training phase, unaffordable and expensive.</p>

<p>Can models like LLaMA leverage this technique without the need for pre-training? Based on my experiments, a compromised way is to apply the \(\log n\) factor only to the attention beyond the pretraining length:</p>

<p>\(\max(1, \log_{\text{maxlen}}n)\) , where \(\text{maxlen}\)  is the max sequence length during pretraining phase​;</p>

<p>For LLaMA-1, it is \(2048\), and for LLaMA-2, it is \(4096\); we can scale the attention of an off-the-shelf model on text that exceeds its \(\text{maxlen}\)</p>

<blockquote>
  <p>from translator: it is simple to implement this log trick in LLaMA self-attention, see Appendix for more details.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>NTK-RoPE-fixed</td>
      <td>49.41%</td>
      <td>51.86%</td>
      <td>39.61%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n^*$$-fixed</td>
      <td>49.41%</td>
      <td>55.94%</td>
      <td>41.11%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-mixed</td>
      <td>49.41%</td>
      <td>53.09%</td>
      <td>40.12%</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n^*$$-mixed</td>
      <td>49.41%</td>
      <td>***<u>59.11%</u>***</td>
      <td>***<u>42.38%</u>***</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> \(\log n^{*}\) denotes we apply the scale factor \(\log n\) is applied to the attention matrix only for text exceeding the max sequence length, without any pretraining</p>

<p>We can see from <strong>Table 2</strong>, \(\log n\) can still enhance performance even without adding it at pretraining phase.  In conclusion, if you are ready to start a pretraining, I suggest you consider incorporated this trick in your network; If you don’t want to train at all, this trick can also benefit performance on long context.</p>

<hr />

<h3 id="appendix">Appendix</h3>

<h4 id="1-proof">1. Proof</h4>
<blockquote>
  <p>Suppose \(\lambda_1\lambda_2…\lambda_m = \text{exp}(am^b)\)</p>

  <p>We claim that : When \(a \ge 0\), \(b \le 1\), then \(\lambda_1 \ge \lambda_2 \ge … \ge \lambda_{d/2} \ge 1\)</p>
</blockquote>

<p><strong>Proof</strong>:
According to the statement,</p>

<p>when \(m=1\):  \(\lambda_1 = \text{exp}(a)\),</p>

<p>when \(m&gt;1\):</p>

\[\begin{split}
\lambda_m &amp;= \dfrac{\text{exp}(am^b)}{\text{exp}(a(m-1)^b)} \\
&amp;=\text{exp}(a[m^b-(m-1)^b])
\end{split}\]

<p>Therefore, when \(a \ge 0\), we have \(\lambda_m \ge \text{exp}(0) = 1\)</p>

<p>Similarly, if the assumption is true, we can derive:</p>

\[\lambda_m =\text{exp}(a[m^b-(m-1)^b])\]

\[\lambda_{m+1} =\text{exp}(a[(m+1)^b-m^b])\]

<p>Since \(\text{exp}()\) is a monotonically increasing function, suppose \(\lambda_m \ge \lambda_{m+1}\)</p>

<p>According to <a href="http://hyperphysics.phy-astr.gsu.edu/hbase/alg3.html">Binomial Theorem</a>, we can derive:</p>

\[\begin{split}
\text{exp}(a[m^b - (m-1)^b]) &amp;\ge \text{exp}(a[(m+1)^b - m^b)])\\
\Rightarrow m^b - (m-1)^b &amp;\ge (m+1)^b - m^b\\
2m^b &amp;\ge(m+1)^b + (m-1)^b\\
2m^b &amp;\ge (m^b+bm^{b-1} + \dfrac{b(b-1)}{2}m^{b-2 }+ \dfrac{b(b-1)(b-2)}{6}m^{b-3 }...) + (m^b - bm^{b-1}+\dfrac{b(b-1)}{2}m^{b-2 }- \dfrac{b(b-1)(b-2)}{6}m^{b-3 } ...)\\
2m^b &amp;\ge 2m^b + 2 (\dfrac{b(b-1)}{2}m^{b-2} + \dfrac{b(b-1)(b-2)(b-3)}{24}m^{b-4} + ...)\\
2m^b &amp;\ge 2m^b + 2\sum_{k=2,4,6...}\dfrac{b!}{(b-k)!k!}m^{b-k}\\
\end{split}\]

<p>Thus, only when \(b \le 1\),  \(\sum_{k=2,4,6...} \dfrac{b!}{(b-k)!k!}m^{b-k} \le 0\)</p>

<p>In conclusion, we can conclude the assumption stays true.</p>

<h4 id="2-minor-changes-in-llamaattention">2. Minor changes in LlamaAttention</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LlamaAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LlamaConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="p">...</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
        <span class="p">...</span>
        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
        <span class="c1"># ---- + new code
</span>        <span class="n">query_states</span> <span class="o">*=</span> <span class="p">((</span><span class="n">position_ids</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">].</span><span class="n">log</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">training_length</span><span class="p">)).</span><span class="n">clip</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">query_states</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># -------
</span>        <span class="p">...</span>
        <span class="c1"># repeat k/v heads if n_kv_heads &lt; n_heads
</span>        <span class="n">key_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="p">...</span>
</code></pre></div></div>]]></content><author><name>Norm Inui</name></author><category term="LLM" /><summary type="html"><![CDATA[Translated from the post, originally written in Chinese by Su, Jianlin Translated by Norm Inui TL; DR NTK-Scale RoPE has flaw Introduce a mixture-of-based encoding method, which can significantly enhance LLM performance beyond its pretraining max length, without the need for fine-tuning Introduce a scale factor \(\log n\) for attention calculation, which can be incorporated either during the pretraining phase or directly applied to an off-the-shell LLM In part 1, we interpret RoPE using a β-based encoding and demonstrated why NTK-aware Scaled RoPE can extend the context length without the need for fine-tuning. Viewing position encoding through the lens of β-based encoding indeed offers me some fresh insights and inspiration. Modification to NTK Suppose we encode integer \(n\) in the \(\beta\)-base, and \(m\) is the digit of the representation counting from the right. [\begin{equation} \lfloor\dfrac{n}{\beta^{m-1}}\rfloor \mod \beta \end{equation}] If we represent it as a RoPE vector: [\begin{equation} p_n = [\text{cos}\theta_1, \text{sin}\theta_1, \text{cos}\theta_2, \text{sin}\theta_2, …, \text{cos}\theta_{d/2}, \text{sin}\theta_{d/2}] \end{equation}] where \(\theta_m = \dfrac{n}{\beta^{m-1}}\), \(\beta= 10000^{2/d}\) We have successfully demonstrated that the NTK Scale RoPE exhibits extrapolation in the high-frequency dimension (for a large value of m), whereas it shows interpolation in the low-frequency dimension (for a small value of m). Since a densely interpolated dimension can harm the Language Model’s (LLM) to accurately compare relative positions, the NTK Scale RoPE successfully mitigates the comparison confusion posed by extrapolation from a base conversion perspective, and ensure each dimension is not too crowded. This approach significantly benefits LLMs that rely on relative positional cues to understand context, enabling them to effectively expand their contextual understanding over pretrained max sequence length without fine-tuning. from translator: If you feel confused about how NTK Scale RoPE combines both interpolation and extrapolation together, I strongly suggest you read the part 1 Now let’s review eq2, notice that cos and sin share the same rotation frequency, which means RoPE encodes n with a base of \(\beta\) into \(d/2\) digits. If we want to extend the context length by \(k\), the intuitive idea is to scale the \(\beta\) to \(\beta \lambda\), then: [\lambda^{d/2}=k \Rightarrow \lambda=k^{2/d}] Then, the RoPE becomes: [\begin{equation} p_n = [\text{cos}\theta_1, \text{sin}\theta_1, \text{cos}\theta_2, \text{sin}\theta_2, …, \text{cos}\theta_{d/2}, \text{sin}\theta_{d/2}] \end{equation}] where \(\theta_m = \dfrac{n}{(\beta\lambda)^{m-1}}\), \(\beta= 10000^{2/d}\), \(\lambda=k^{2/d}\) This is how we implement NTK-RoPE. However, back to eq1, we can see that if we want to encode \(n\) with a base of \(\beta \lambda\), the eq1 should be: [\begin{equation} \lfloor\dfrac{n}{(\beta\lambda)^{m-1}}\rfloor \mod (\beta\lambda) \end{equation}] Therefore, our derivation from eq2 to eq3 has flaws, besides replacing the \(\dfrac{n}{\beta^{m-1}}\) with \(\dfrac{n}{(\beta\lambda)^{m-1}}\), the \(\text{mod}\) needs to scale up its period by \(\lambda\) as well, then the corrected Scaled RoPE should be: [\begin{equation} p_n = [\text{cos}\theta_1, \text{sin}\theta_1, \text{cos}\theta_2, \text{sin}\theta_2, …, \text{cos}\theta_{d/2}, \text{sin}\theta_{d/2}] \end{equation}] where \(\theta_m = \dfrac{n}{\lambda(\beta\lambda)^{m-1}}\), \(\beta= 10000^{2/d}\), \(\lambda=k^{2/d}\) In the following context, we denote eq3 as NTK-RoPE-old, and eq5 as NTK-RoPE-fixed. Why a mixture of base is necessary If we can encode an integer in \(\beta\) base, how about generalizing to a mixed-based encoding where each digit is encoded in a different base? Just like the time system we daily use, 60 seconds make up 1 minute, 60 minutes equal 1 hour, 24 hours is 1 day, and 7 days amount to 1 week. Here, the numbers 60, 60, 24, and 7 can be regarded as different encoding bases. In essence, any timestamp can be encoded into seconds, minutes, hours, days, and weeks with the mixed-based system. Counted from right to left, the first digit is encoded in \(\beta_1\), the second digit is in \(\beta_2\), and the third is in \(\beta_3\), …. The \(m\)th digit of an integer \(n\) can then be represented as: [\begin{equation} \lfloor\dfrac{n}{\beta^{1}\beta^{1}…\beta^{m-1}}\rfloor \mod \beta_m \end{equation}] Since RoPE is a relative position encoding, it can be viewed as a specific instance of the Toeplitz matrix, which looks like this (given our discussion mainly focuses on language models, the top-right part of the matrix is trimmed to fit the page). [\begin{pmatrix} 0 1 &amp; 0 &amp; 2 &amp; 1 &amp; 0 3 &amp; 2 &amp; 1 &amp; 0\ 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0\ \end{pmatrix}] Upon the matrix, it is evident that the distribution of relative position encoding is not uniform! The 0 is the most frequent, followed by 1, 2, and so on. In other words, as \(n\) grows larger, its appearance becomes less frequent. This suggests that, as a form of \(\beta\)-base encoding, the higher bits of RoPE might be under-trained. This implies that the generalization capability of the higher bits might be inferior to the lower bits. As mentioned, NTK-RoPE mitigated the confusion introduced by extrapolation across all bits uniformly. However, if our hypothesis holds, this strategy might not be optimal. Lower bits can be more robust than higher bits and can hold a larger data range than the higher bits. Inspired by the timestamp encoding system, we should redesign RoPE with a mix-based encoding system. Encoding with a mixture of bases To be specific, we extend the context length by \(k\) with a mixture of bases, \(\beta_1\), \(\beta_2\), \(...\), \(\beta_{d/2}\), where \(\beta_m = \beta\lambda_m\) Thus, eq4 shold be be written as: [\begin{equation} \lfloor\dfrac{n}{\beta^{m-1}(\lambda_1\lambda_2…\lambda_{m-1})}\rfloor \mod (\beta\lambda_m) \end{equation}] where \(\theta_m = \dfrac{n}{\beta^{m-1}(\lambda_1\lambda_2…\lambda_m)}\), \(\beta = 10000^{2/d}\) According to the goal to ensure lower digits hold a larger range of data and to extend the context length by a scale factor \(k\), eq 7 is subject to the conditions \(\lambda_1\lambda_2…\lambda_m = k\) and \(\lambda_1 \ge \lambda_2 \ge … \ge \lambda_{d/2} \ge 1\) Given these two conditions, one possible solution is: \(\lambda_1\lambda_2…\lambda_m = \text{exp}(am^b)\), where \(a \ge 0\), \(b \le 1\) from translator: The original post doesn’t cover any proof of this statement, please check Appendix for the proof I derive When \(b=1\), \(\lambda_1 = \lambda_2 = … = \lambda_{d/2} &gt; 1\), we denote as “NTK-RoPE-fixed”; when \(b=0\), \(\lambda_1 = \lambda_2 = … = \lambda_{d/2} = 1\), this exactly meets the definition of “Positional Interpolation (PI)” Given one of the constrains we mention above: [\lambda_1 \lambda_2 … \lambda_{d/2} =k] We can derive: [a(\dfrac{d}{2})^b = \log k] \(b=0.625\) is an empirical value that can achieve optimal performance in an expanded long context; (Optimal values may vary across models, feel free to tune it), and we denoted this method as NTK-RoPE-mixed. Experiment We follow the same experiment setup as in part 1 and compare the NTK-RoPE-mixed and NTK-RoPE-fixed in an extended context. Table 1 context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) Baseline 49.41% 24.17% 23.16% \ Baseline-$$\log n$$ 49.40% 24.60% 24.02% PI-RoPE 49.41% 15.04% 13.54% \ PI-RoPE-$$\log n$$ 49.40% 14.99% 16.51% NTK-RoPE 49.41% 51.28% 39.27% \ NTK-RoPE-$$\log n$$ 49.40% 61.71% 43.75% NTK-RoPE-fixed 49.41% 51.86% 39.61% \ NTK-RoPE-$$\log n$$-fixed 49.40% 62.85% 44.14% \ NTK-RoPE-mixed 49.41% 53.09% 40.12% \ NTK-RoPE-$$\log n$$-mixed 49.40% ***68.91%*** ***45.41%*** From the Table 1, we can clearly see when compared to the “NTK-RoPE-old” and “NTK-RoPE-fixed,” the mixture-of-base “NTK-RoPE-mixed” shows a significant accuracy improvement without fine-tuning. This effectively provides a ‘free lunch’ approach to enhance LLM performance in a longer context. In addition, the table shows the scale factor \(\log n\) can benefit as well. But this trick requires \(\log n\) to be inserted into attention during the pre-training phase, unaffordable and expensive. Can models like LLaMA leverage this technique without the need for pre-training? Based on my experiments, a compromised way is to apply the \(\log n\) factor only to the attention beyond the pretraining length: \(\max(1, \log_{\text{maxlen}}n)\) , where \(\text{maxlen}\) is the max sequence length during pretraining phase​; For LLaMA-1, it is \(2048\), and for LLaMA-2, it is \(4096\); we can scale the attention of an off-the-shelf model on text that exceeds its \(\text{maxlen}\) from translator: it is simple to implement this log trick in LLaMA self-attention, see Appendix for more details. context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) NTK-RoPE-fixed 49.41% 51.86% 39.61% NTK-RoPE-$$\log n^*$$-fixed 49.41% 55.94% 41.11% NTK-RoPE-mixed 49.41% 53.09% 40.12% NTK-RoPE-$$\log n^*$$-mixed 49.41% ***59.11%*** ***42.38%*** Table 2: \(\log n^{*}\) denotes we apply the scale factor \(\log n\) is applied to the attention matrix only for text exceeding the max sequence length, without any pretraining We can see from Table 2, \(\log n\) can still enhance performance even without adding it at pretraining phase. In conclusion, if you are ready to start a pretraining, I suggest you consider incorporated this trick in your network; If you don’t want to train at all, this trick can also benefit performance on long context. Appendix 1. Proof Suppose \(\lambda_1\lambda_2…\lambda_m = \text{exp}(am^b)\) We claim that : When \(a \ge 0\), \(b \le 1\), then \(\lambda_1 \ge \lambda_2 \ge … \ge \lambda_{d/2} \ge 1\) Proof: According to the statement, when \(m=1\): \(\lambda_1 = \text{exp}(a)\), when \(m&gt;1\): [\begin{split} \lambda_m &amp;= \dfrac{\text{exp}(am^b)}{\text{exp}(a(m-1)^b)} &amp;=\text{exp}(a[m^b-(m-1)^b]) \end{split}] Therefore, when \(a \ge 0\), we have \(\lambda_m \ge \text{exp}(0) = 1\) Similarly, if the assumption is true, we can derive: [\lambda_m =\text{exp}(a[m^b-(m-1)^b])] [\lambda_{m+1} =\text{exp}(a[(m+1)^b-m^b])] Since \(\text{exp}()\) is a monotonically increasing function, suppose \(\lambda_m \ge \lambda_{m+1}\) According to Binomial Theorem, we can derive: [\begin{split} \text{exp}(a[m^b - (m-1)^b]) &amp;\ge \text{exp}(a[(m+1)^b - m^b)]) \Rightarrow m^b - (m-1)^b &amp;\ge (m+1)^b - m^b 2m^b &amp;\ge(m+1)^b + (m-1)^b 2m^b &amp;\ge (m^b+bm^{b-1} + \dfrac{b(b-1)}{2}m^{b-2 }+ \dfrac{b(b-1)(b-2)}{6}m^{b-3 }…) + (m^b - bm^{b-1}+\dfrac{b(b-1)}{2}m^{b-2 }- \dfrac{b(b-1)(b-2)}{6}m^{b-3 } …) 2m^b &amp;\ge 2m^b + 2 (\dfrac{b(b-1)}{2}m^{b-2} + \dfrac{b(b-1)(b-2)(b-3)}{24}m^{b-4} + …) 2m^b &amp;\ge 2m^b + 2\sum_{k=2,4,6…}\dfrac{b!}{(b-k)!k!}m^{b-k} \end{split}] Thus, only when \(b \le 1\), \(\sum_{k=2,4,6...} \dfrac{b!}{(b-k)!k!}m^{b-k} \le 0\) In conclusion, we can conclude the assumption stays true. 2. Minor changes in LlamaAttention class LlamaAttention(nn.Module): def __init__(self, config: LlamaConfig): super().__init__() ... self.max_position_embeddings = config.max_position_embeddings ... def forward( self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_value: Optional[Tuple[torch.Tensor]] = None, output_attentions: bool = False, use_cache: bool = False, ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]: ... bsz, q_len, _ = hidden_states.size() ... query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids) # ---- + new code query_states *= ((position_ids + 1)[:, None, :, None].log() / np.log(training_length)).clip(1).to(query_states.dtype) # ------- ... # repeat k/v heads if n_kv_heads &lt; n_heads key_states = repeat_kv(key_states, self.num_key_value_groups) value_states = repeat_kv(value_states, self.num_key_value_groups) attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim) ...]]></summary></entry><entry><title type="html">Expand the Context Length with RoPE, Part 1 – RoPE is a β-based Encoding</title><link href="/Rethinking-Rotary-Position-Embedding/" rel="alternate" type="text/html" title="Expand the Context Length with RoPE, Part 1 – RoPE is a β-based Encoding" /><published>2023-08-09T00:00:00+00:00</published><updated>2023-08-09T00:00:00+00:00</updated><id>/Rethinking-Rotary-Position-Embedding</id><content type="html" xml:base="/Rethinking-Rotary-Position-Embedding/"><![CDATA[<blockquote>
  <p>Translated from the <a href="https://kexue.fm/archives/9675">post</a>, originally written in Chinese by Su, Jianlin</p>

  <p>Translated by Norm Inui</p>
</blockquote>

<h3 id="tldr">TL;DR</h3>

<ul>
  <li>Interpret RoPE from the perspective of a β-based encoding.</li>
  <li>Introduce recent developments in the open-source community regarding long contexts.</li>
  <li>Some approaches, such as NTK-aware Scale RoPE, can extend context length without fine-tuning.</li>
</ul>

<hr />

<p>For developers who are interested in how to extend the context length of LLMs (Large Language Models), the open-source community has continuously presented us with fascinating methods in the past few weeks. First, <a href="https://www.reddit.com/user/kaiokendev">@kaiokendev</a> experimented with a “positional linear interpolation” approach in his project <a href="https://kaiokendev.github.io/til#extending-context-to-8k">SuperHOT</a>.</p>

<p>He demonstrated that with minimal fine-tuning on long texts, existing LLMs can be easily adapted to handle contexts over their pretraining context length. Almost simultaneously, Meta proposed the same idea in the paper titled “<a href="https://arxiv.org/abs/2306.15595">Extending Context Window of Large Language Models via Positional Interpolation</a>.”</p>

<p>Shortly after the paper was published, <a href="https://www.reddit.com/user/bloc97">@bloc97</a> introduced the <a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-aware Scaled RoPE</a>, enabling LLM to extend its context length without fine-tuning! With all these methods, especially the NTK-aware Scaled RoPE, it persuades me to revisit the idea behind RoPE from a \(\beta\)-based encoding perspective.</p>

<h3 id="encoding-the-number">Encoding the Number</h3>
<p>Suppose we have an integer \(n\) within \(1,000\) (not including \(1,000\)) that we want to input into a model. What would be the best way to do this?</p>

<p>The most intuitive idea is to input it directly as a one-dimensional vector. However, the value of this vector has a large range from \(0\) to \(999\), which is not easy to optimize for gradient-based optimizers. What if we scale it between 0 and 1? That’s not good either, because the difference between adjacent numbers changes from \(1\) to \(0.001\), making it challenging for both the model and optimizer to distinguish between the numbers. In general, gradient-based optimizers are a bit “vulnerable” and can only handle inputs that aren’t too large or too small.</p>

<p>To solve this problem, it is necessary to find a smart way to represent the input. We might think about how we humans do. For an integer, like \(759\), it’s a three-digit number in decimal, with each digit ranging from 0 to 9. This inspires me to represent the input in decimal directly as a vector. That is, we transform the integer  \(n\) as a three-dimensional vector \([a,b,c]\), where \(a\), \(b\), and \(c\) represent the hundreds, tens, and units of \(n\) respectively. By increasing the input dimension, we can both reduce the range of each digit and get rid of small resolution between numbers. Luckily, neural networks are good at handling high-dimensional vectors.</p>

<p>If we want to further reduce the value span of each dimension, we can simply decrease the base, like using 8, 6, or even 2 as the encoding base at a cost of an increase in input vector dimensions.</p>

<h3 id="direct-extrapolation">Direct Extrapolation</h3>

<p>Let’s say we have trained a model with the input ranging from \(0\) to \(999\) represented as a three-dimensional vector in decimal. Then we want to enlarge the upper bound of \(n\) from \(1,000\) to \(2,000\). How can we realize this?</p>

<p>If we follow the same idea discussed above, the input will now be a \(4\)-dimensional vector. However, the model was trained for a \(3\)-dimensional vector. Therefore, the input with an extra dimension may confuse the model. Some might wonder why can’t we reserve extra dimensions in advance? Indeed, we can pre-allocate a few more dimensions. During the training phase with the upper bound as \(1,000\), they can be set to \(0\), but during the inference phase with an upper bound as \(2,000\), the pre-allocate dimension has to be set to value besides \(0\). This is what we call <strong>Extrapolation</strong>.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/0/extrapolation.png" alt="Extrapolation" /></p>

<p>However, the dimensions reserved during the training phase have always been set to \(0\). If these dimensions were changed to other values during the inference phase, the performance might be significantly harmed. This is because the model is never trained to adapt to those pre-allocated dimension in different values.</p>

<h3 id="linear-interpolation">Linear Interpolation</h3>
<p>Considering the challenges above, some proposed interpolation instead of extrapolation to compress the value upper bound of \(2,000\) down to \(1,000\). For example, the number \(1749\) can simply compress to \(874.5\) by dividing \(2\). Thus, \(874.5\) can be converted into a three-dimensional vector \([8,7,4.5]\). Following this double mapping strategy, the vector \([7,4,9]\) now corresponds to \(1498\). 
However, the difference between adjacent numbers was \(1\), but now it is \(0.5\), making the last dimension more “crowded”. Therefore, interpolation usually needs to fine-tune the model to make it adapt to the “crowded” dimensions.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/0/interpolation.png" alt="interpolation" /></p>

<p>You might notice that the extrapolation can also be fine-tuned to adapt to the pre-allocated dimension. Well, that is correct, but the interpolation requires far fewer steps compared to extrapolation. It is because position encoding (both absolute and relative) has taught the model to understand the relative concept rather than precisely knowing what the number it is, that is: the model knows \(875\) is greater than \(874\), but it doesn’t know what is \(875\). 
Given the generalized ability of LLM, injecting the concept that \(874.5\) is greater than \(874\) is not particularly challenging.</p>

<p>However, this interpolation approach is not without flaws. The broader the range we want to expand, the smaller the unit dimension resolution becomes, while the hundreds and tens dimensions still remain at a resolution of 1. This means that the interpolation implicitly introduces resolution inconsistency across dimensions. Each dimension is not equally interpolated, making fine-tuning/continuing learning challenging.</p>

<h3 id="base-conversion">Base Conversion</h3>
<p>Is there a method that doesn’t require adding extra dimensions and can still maintain resolution across dimensions? The answer is <strong>YES</strong>.</p>

<p>It is a solution that we are all familiar with, base conversion. A three-digit decimal number can represent \(0\) to \(999\). What if it’s in hexadecimal? Its maximum value in 16-base can be \(16^3-1=4095 &gt; 1999\). So, by simply converting to hexadecimal, with a base of 16, such as turning \(1749\) into \([6,13,5]\), a three-dimensional vector can represent a larger range. The cost? Each dimension’s value changes from a range of \(0-9\) to \(0-15\).</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/0/conversion.png" alt="conversion" /></p>

<p>It’s indeed a clever idea. As mentioned earlier, what we care about is relative position information in the sequence since a trained model has already learned \(875 &gt; 874\). This holds true in hexadecimal as well. The model doesn’t care what base you use to represent the input, but only the relative information. Now, the only problem would be if the value of each dimension exceeds \(9\) (values between \(10-15\)), will the model still know how to compare accurately? Luckily, most LLMs have such capability. Based on this hypothesis, we can now extend the range without fine-tuning! Furthermore, to make interpolation more robust, we can use a smaller base like \(\lceil\sqrt[3]{2000}\rceil = 13\) instead of \(16\) to limit the value range of each dimension.</p>

<p>This idea of base conversion finally leads us to the NTK-aware scaled RoPE that was mentioned at the beginning.</p>

<h3 id="positional-encoding">Positional Encoding</h3>
<p>Based on the explanation above, we can claim:</p>
<blockquote>
  <p>The Rotational Positional Encoding (RoPE) at position \(n\) is the \(\beta\)-based encoding of the number \(n\)</p>
</blockquote>

<p>This might surprise you at first glance, however, it does hold true.</p>

<p><em>Proof:</em></p>

<p>Suppose we have a decimal number \(n\). To calculate the digit at position \(m\) (counting from right to left) in its β-based encoding, we have:</p>

\[\begin{equation}\lfloor\dfrac{n}{\beta^{m-1}}\rfloor \mod \beta \end{equation}\]

<p>As for RoPE, which is adapted from Sinusoidal Position Embedding</p>

\[\begin{equation}[\text{cos}(\dfrac{n}{\beta^0}), \text{sin}(\dfrac{n}{\beta^0}), \text{cos}(\dfrac{n}{\beta^1}), \text{sin}(\dfrac{n}{\beta^1}), …, \text{cos}(\dfrac{n}{\beta^{d/2-1}}), \text{sin}(\dfrac{n}{\beta^{d/2-1}})]\end{equation}\]

<p>where \(\beta = 10000^{2/d}\)</p>

<p>we can notice that:</p>

<p>1) <strong>eq1</strong> and <strong>eq2</strong> share the same component \(\frac n {\beta^{m-1}}\);</p>

<p>2) \(\text{mod}\) introduces periodicity, while \(\text{sin}\) and \(\text{cos}\) are also periodical functions.</p>

<p>Therefore, if we ignore the ceiling operation, we can say RoPE (or Sinusoidal Position Embedding) is a kind of β-based encoding.</p>

<p>With this property, we can now apply extrapolation on \(n\) by simply replacing \(n\) as \(n/k\), \(k\) is the scale we want to enlarge. This is the <strong>Positional Interpolation</strong> proposed in Meta’s paper, and the experimental results show that extrapolation indeed requires more fine-tuning steps than interpolation.</p>

<p>Regarding numeral base conversion, the objective is to expand the representation range by \(k\). Therefore, the β-base should be converted to at least \(β(k^{2/d})\) (according to <strong>eq2</strong>, \(\text{cos}\) and \(\text{sin}\) appear in pairs. This can be regarded as a β-base representation with \(d/2\) bits, not \(d\)). Alternatively, the original base \(10000\) can be replaced with \(10000k\), which is the NTK-aware Scaled RoPE. As discussed earlier, since positional embedding has taught the model the sequence relative information, NTK-aware Scaled RoPE can achieve good performance in longer contexts without fine-tuning.</p>

<h3 id="lets-dig-further">Let’s dig further</h3>
<p>You might wonder why we call it NTK (Neural Tangent Kernel). In fact, it is the academic background of @bloc97 that makes him use this term to name it.</p>

<p>In “<a href="https://arxiv.org/abs/2006.10739">Fourier Features Let Networks Learn High-Frequency Functions in Low-Dimensional Domains</a>”, authors use NTK methods to demonstrate that neural networks cannot learn high-frequency signals efficiently. Instead, their solution is to transform it into Fourier features, which share the same idea with the Sinusoidal position encoding we mention in <strong>eq1</strong>.</p>

<p>Thus, based on the findings from this NTK paper, @bloc97 proposed the NTK-aware Scaled RoPE. I ask him about how he derived it. Surprisingly, his derivation is quite straightforward. The main idea is to combine extrapolation with interpolation:</p>

<p><b><font color="red">extrapolation in high-frequency and  interpolation in low-frequency</font></b></p>

<p>According to <strong>eq2</strong>, the lowest frequency in each element of the position features is 
\(\dfrac{n}{\beta^{d/2-1}}\)
Here we introduce a factor \(\lambda\) in base, now we have: 
\(\dfrac{n}{(\beta\lambda)^{d/2-1}}\)</p>

<p>We expect that scaling the rotation base \(\beta\) can work as interpolation, therefore</p>

\[\begin{equation}\dfrac{n}{(\beta\lambda)^{d/2-1}} = \dfrac{n/k}{\beta^{d/2-1}}\end{equation}\]

<p>We can solve from <strong>eq3</strong>:</p>

\[\lambda = k^{2/(d-2)}\]

<p>The same idea for the highest frequency in the RoPE feature:</p>

<p>\(\dfrac{n}{\beta}\) now becomes \(\dfrac{n}{\lambda\beta}\).</p>

<p>Let’s insert the value \(\lambda\) we solve from <strong>eq3</strong>, which allows a low frequency represented as interpolation, into \(\dfrac{n}{\lambda\beta}\). Since \(d\) is very large ( 64 for BERT, 128 for LLAMA-1), \(\lambda \to 1\), thus, we can conclude from <strong>eq3</strong>:</p>

\[\begin{equation}\dfrac{n}{\beta\lambda}\simeq \dfrac{n}{\beta}\end{equation}\]

<p>You can see the frequency remains relatively stable w.r.t \(\lambda\), indicating that the corresponding dimension doesn’t become too crowded. Therefore, to represent a larger number, a high-frequency dimension is more likely to extrapolate by using an additional dimension rather than expanding the value range one dimension can hold. This is what we call: extrapolation in high-frequency.</p>

<p>From the derivation, we can see that NTK-aware Scaled RoPE cleverly combines interpolation and extrapolation together. Besides scaling the base, I believe any transformations on the frequencies will be also effective as long as it ensures the extrapolation in high frequencies and interpolation in low-frequencies.</p>

<blockquote>
  <p><strong>from translator</strong>: We can actually inteprete the <strong>extrapolation in high-frequency and  interpolation in low-frequency</strong> by considering from a wavelength perspective in RoPE. Specifically, the wavelength in RoPE is used to define the length of the token sequence required for the encoding at dimension \(d\) to complete a full rotation, \(2\pi\). The higher the frequency, the smaller the wavelength and vice verse. Therefore, a longer wavelength can hold more interpolated tokens, while a shorter one cannot. That is why we employ interpolation in low-frequency.</p>

  <p>please refer to <a href="https://github.com/jquesnelle/yarn/tree/master">YaRN: Efficient Context Window Extension of Large Language Models</a> for more details</p>
</blockquote>

<h3 id="experiments">Experiments</h3>
<blockquote>
  <p><strong>from translator</strong>: the table shows: the average accuracy of predicting next token to match the ground-truth next token given previous context. The experiment is based on a hybrid Transformer-GAU (Gated Attention Unit) model with a size of 100M parameters.</p>

  <p>For more details on the GAU,  please refer to: <a href="https://arxiv.org/abs/2202.10447">https://arxiv.org/abs/2202.10447</a></p>
</blockquote>

<p>When \(k=8\)</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>49.41%</td>
      <td>24.17%</td>
      <td>23.16%                   \</td>
    </tr>
    <tr>
      <td>Baseline-$$\log n$$</td>
      <td>49.40%</td>
      <td>24.60%</td>
      <td>24.02%</td>
    </tr>
    <tr>
      <td>PI-RoPE</td>
      <td>49.41%</td>
      <td>15.04%</td>
      <td>13.54%                   \</td>
    </tr>
    <tr>
      <td>PI-RoPE-$$\log n$$</td>
      <td>49.40%</td>
      <td>14.99%</td>
      <td>16.51%</td>
    </tr>
    <tr>
      <td>NTK-RoPE</td>
      <td>49.41%</td>
      <td>51.28%</td>
      <td>39.27%                   \</td>
    </tr>
    <tr>
      <td>NTK-RoPE-$$\log n$$</td>
      <td>49.40%</td>
      <td>***<u>61.71%</u>***</td>
      <td>***<u>43.75%</u>***</td>
    </tr>
  </tbody>
</table>

<p>No fine-tuning is applied on all tests. <strong>Baseline</strong>: use extrapolation; <strong>PI（Positional Interpolation)</strong>: replaces extrapolation in Baseline with interpolation; <strong>NTK-RoPE</strong>: replace extrapolation in Baseline with NTK-aware Scaled RoPE; \(\log n\): apply a scale to optimize self-attention for long context <a href="https://openreview.net/forum?id=qc9O2EtrMI-">ref_1</a></p>

<h3 id="conclusion">Conclusion</h3>

<ol>
  <li>
    <p>Direct extrapolation doesn’t work effectively on extension.</p>
  </li>
  <li>
    <p>Interpolation yields poor results without fine-tuning.</p>
  </li>
  <li>
    <p>NTK-RoPE achieves promising (though slightly reduced) results in extended context even without fine-tuning.</p>
  </li>
  <li>
    <p>A \(\log n\) factor indeed optimize self-attention for long context.</p>
  </li>
  <li>
    <p>What’s even more encouraging is that NTK-RoPE performs significantly better in ‘repeated’ extrapolation compared to ‘non-repeated’ one, suggesting that LLM with NTK-RoPE still retain the global attention ability across the expanded context, rather than confining its attention to a limited scope.</p>
  </li>
</ol>

<p>In just a few weeks, the open-source community concerning long contexts totally blows our minds. <del>Open</del>ClosedAI, you better watch out.</p>

<h3 id="future-research">Future Research</h3>
<p>please check <a href="https://normxu.github.io/Rethinking-Rotary-Position-Embedding-2/">part-2</a></p>

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="https://github.com/jquesnelle/yarn/tree/master">YaRN: Efficient Context Window Extension of Large Language Models</a></li>
</ul>]]></content><author><name>Norm Inui</name></author><category term="LLM" /><summary type="html"><![CDATA[Translated from the post, originally written in Chinese by Su, Jianlin Translated by Norm Inui TL;DR Interpret RoPE from the perspective of a β-based encoding. Introduce recent developments in the open-source community regarding long contexts. Some approaches, such as NTK-aware Scale RoPE, can extend context length without fine-tuning. For developers who are interested in how to extend the context length of LLMs (Large Language Models), the open-source community has continuously presented us with fascinating methods in the past few weeks. First, @kaiokendev experimented with a “positional linear interpolation” approach in his project SuperHOT. He demonstrated that with minimal fine-tuning on long texts, existing LLMs can be easily adapted to handle contexts over their pretraining context length. Almost simultaneously, Meta proposed the same idea in the paper titled “Extending Context Window of Large Language Models via Positional Interpolation.” Shortly after the paper was published, @bloc97 introduced the NTK-aware Scaled RoPE, enabling LLM to extend its context length without fine-tuning! With all these methods, especially the NTK-aware Scaled RoPE, it persuades me to revisit the idea behind RoPE from a \(\beta\)-based encoding perspective. Encoding the Number Suppose we have an integer \(n\) within \(1,000\) (not including \(1,000\)) that we want to input into a model. What would be the best way to do this? The most intuitive idea is to input it directly as a one-dimensional vector. However, the value of this vector has a large range from \(0\) to \(999\), which is not easy to optimize for gradient-based optimizers. What if we scale it between 0 and 1? That’s not good either, because the difference between adjacent numbers changes from \(1\) to \(0.001\), making it challenging for both the model and optimizer to distinguish between the numbers. In general, gradient-based optimizers are a bit “vulnerable” and can only handle inputs that aren’t too large or too small. To solve this problem, it is necessary to find a smart way to represent the input. We might think about how we humans do. For an integer, like \(759\), it’s a three-digit number in decimal, with each digit ranging from 0 to 9. This inspires me to represent the input in decimal directly as a vector. That is, we transform the integer \(n\) as a three-dimensional vector \([a,b,c]\), where \(a\), \(b\), and \(c\) represent the hundreds, tens, and units of \(n\) respectively. By increasing the input dimension, we can both reduce the range of each digit and get rid of small resolution between numbers. Luckily, neural networks are good at handling high-dimensional vectors. If we want to further reduce the value span of each dimension, we can simply decrease the base, like using 8, 6, or even 2 as the encoding base at a cost of an increase in input vector dimensions. Direct Extrapolation Let’s say we have trained a model with the input ranging from \(0\) to \(999\) represented as a three-dimensional vector in decimal. Then we want to enlarge the upper bound of \(n\) from \(1,000\) to \(2,000\). How can we realize this? If we follow the same idea discussed above, the input will now be a \(4\)-dimensional vector. However, the model was trained for a \(3\)-dimensional vector. Therefore, the input with an extra dimension may confuse the model. Some might wonder why can’t we reserve extra dimensions in advance? Indeed, we can pre-allocate a few more dimensions. During the training phase with the upper bound as \(1,000\), they can be set to \(0\), but during the inference phase with an upper bound as \(2,000\), the pre-allocate dimension has to be set to value besides \(0\). This is what we call Extrapolation. However, the dimensions reserved during the training phase have always been set to \(0\). If these dimensions were changed to other values during the inference phase, the performance might be significantly harmed. This is because the model is never trained to adapt to those pre-allocated dimension in different values. Linear Interpolation Considering the challenges above, some proposed interpolation instead of extrapolation to compress the value upper bound of \(2,000\) down to \(1,000\). For example, the number \(1749\) can simply compress to \(874.5\) by dividing \(2\). Thus, \(874.5\) can be converted into a three-dimensional vector \([8,7,4.5]\). Following this double mapping strategy, the vector \([7,4,9]\) now corresponds to \(1498\). However, the difference between adjacent numbers was \(1\), but now it is \(0.5\), making the last dimension more “crowded”. Therefore, interpolation usually needs to fine-tune the model to make it adapt to the “crowded” dimensions. You might notice that the extrapolation can also be fine-tuned to adapt to the pre-allocated dimension. Well, that is correct, but the interpolation requires far fewer steps compared to extrapolation. It is because position encoding (both absolute and relative) has taught the model to understand the relative concept rather than precisely knowing what the number it is, that is: the model knows \(875\) is greater than \(874\), but it doesn’t know what is \(875\). Given the generalized ability of LLM, injecting the concept that \(874.5\) is greater than \(874\) is not particularly challenging. However, this interpolation approach is not without flaws. The broader the range we want to expand, the smaller the unit dimension resolution becomes, while the hundreds and tens dimensions still remain at a resolution of 1. This means that the interpolation implicitly introduces resolution inconsistency across dimensions. Each dimension is not equally interpolated, making fine-tuning/continuing learning challenging. Base Conversion Is there a method that doesn’t require adding extra dimensions and can still maintain resolution across dimensions? The answer is YES. It is a solution that we are all familiar with, base conversion. A three-digit decimal number can represent \(0\) to \(999\). What if it’s in hexadecimal? Its maximum value in 16-base can be \(16^3-1=4095 &gt; 1999\). So, by simply converting to hexadecimal, with a base of 16, such as turning \(1749\) into \([6,13,5]\), a three-dimensional vector can represent a larger range. The cost? Each dimension’s value changes from a range of \(0-9\) to \(0-15\). It’s indeed a clever idea. As mentioned earlier, what we care about is relative position information in the sequence since a trained model has already learned \(875 &gt; 874\). This holds true in hexadecimal as well. The model doesn’t care what base you use to represent the input, but only the relative information. Now, the only problem would be if the value of each dimension exceeds \(9\) (values between \(10-15\)), will the model still know how to compare accurately? Luckily, most LLMs have such capability. Based on this hypothesis, we can now extend the range without fine-tuning! Furthermore, to make interpolation more robust, we can use a smaller base like \(\lceil\sqrt[3]{2000}\rceil = 13\) instead of \(16\) to limit the value range of each dimension. This idea of base conversion finally leads us to the NTK-aware scaled RoPE that was mentioned at the beginning. Positional Encoding Based on the explanation above, we can claim: The Rotational Positional Encoding (RoPE) at position \(n\) is the \(\beta\)-based encoding of the number \(n\) This might surprise you at first glance, however, it does hold true. Proof: Suppose we have a decimal number \(n\). To calculate the digit at position \(m\) (counting from right to left) in its β-based encoding, we have: [\begin{equation}\lfloor\dfrac{n}{\beta^{m-1}}\rfloor \mod \beta \end{equation}] As for RoPE, which is adapted from Sinusoidal Position Embedding [\begin{equation}[\text{cos}(\dfrac{n}{\beta^0}), \text{sin}(\dfrac{n}{\beta^0}), \text{cos}(\dfrac{n}{\beta^1}), \text{sin}(\dfrac{n}{\beta^1}), …, \text{cos}(\dfrac{n}{\beta^{d/2-1}}), \text{sin}(\dfrac{n}{\beta^{d/2-1}})]\end{equation}] where \(\beta = 10000^{2/d}\) we can notice that: 1) eq1 and eq2 share the same component \(\frac n {\beta^{m-1}}\); 2) \(\text{mod}\) introduces periodicity, while \(\text{sin}\) and \(\text{cos}\) are also periodical functions. Therefore, if we ignore the ceiling operation, we can say RoPE (or Sinusoidal Position Embedding) is a kind of β-based encoding. With this property, we can now apply extrapolation on \(n\) by simply replacing \(n\) as \(n/k\), \(k\) is the scale we want to enlarge. This is the Positional Interpolation proposed in Meta’s paper, and the experimental results show that extrapolation indeed requires more fine-tuning steps than interpolation. Regarding numeral base conversion, the objective is to expand the representation range by \(k\). Therefore, the β-base should be converted to at least \(β(k^{2/d})\) (according to eq2, \(\text{cos}\) and \(\text{sin}\) appear in pairs. This can be regarded as a β-base representation with \(d/2\) bits, not \(d\)). Alternatively, the original base \(10000\) can be replaced with \(10000k\), which is the NTK-aware Scaled RoPE. As discussed earlier, since positional embedding has taught the model the sequence relative information, NTK-aware Scaled RoPE can achieve good performance in longer contexts without fine-tuning. Let’s dig further You might wonder why we call it NTK (Neural Tangent Kernel). In fact, it is the academic background of @bloc97 that makes him use this term to name it. In “Fourier Features Let Networks Learn High-Frequency Functions in Low-Dimensional Domains”, authors use NTK methods to demonstrate that neural networks cannot learn high-frequency signals efficiently. Instead, their solution is to transform it into Fourier features, which share the same idea with the Sinusoidal position encoding we mention in eq1. Thus, based on the findings from this NTK paper, @bloc97 proposed the NTK-aware Scaled RoPE. I ask him about how he derived it. Surprisingly, his derivation is quite straightforward. The main idea is to combine extrapolation with interpolation: extrapolation in high-frequency and interpolation in low-frequency According to eq2, the lowest frequency in each element of the position features is \(\dfrac{n}{\beta^{d/2-1}}\) Here we introduce a factor \(\lambda\) in base, now we have: \(\dfrac{n}{(\beta\lambda)^{d/2-1}}\) We expect that scaling the rotation base \(\beta\) can work as interpolation, therefore [\begin{equation}\dfrac{n}{(\beta\lambda)^{d/2-1}} = \dfrac{n/k}{\beta^{d/2-1}}\end{equation}] We can solve from eq3: [\lambda = k^{2/(d-2)}] The same idea for the highest frequency in the RoPE feature: \(\dfrac{n}{\beta}\) now becomes \(\dfrac{n}{\lambda\beta}\). Let’s insert the value \(\lambda\) we solve from eq3, which allows a low frequency represented as interpolation, into \(\dfrac{n}{\lambda\beta}\). Since \(d\) is very large ( 64 for BERT, 128 for LLAMA-1), \(\lambda \to 1\), thus, we can conclude from eq3: [\begin{equation}\dfrac{n}{\beta\lambda}\simeq \dfrac{n}{\beta}\end{equation}] You can see the frequency remains relatively stable w.r.t \(\lambda\), indicating that the corresponding dimension doesn’t become too crowded. Therefore, to represent a larger number, a high-frequency dimension is more likely to extrapolate by using an additional dimension rather than expanding the value range one dimension can hold. This is what we call: extrapolation in high-frequency. From the derivation, we can see that NTK-aware Scaled RoPE cleverly combines interpolation and extrapolation together. Besides scaling the base, I believe any transformations on the frequencies will be also effective as long as it ensures the extrapolation in high frequencies and interpolation in low-frequencies. from translator: We can actually inteprete the extrapolation in high-frequency and interpolation in low-frequency by considering from a wavelength perspective in RoPE. Specifically, the wavelength in RoPE is used to define the length of the token sequence required for the encoding at dimension \(d\) to complete a full rotation, \(2\pi\). The higher the frequency, the smaller the wavelength and vice verse. Therefore, a longer wavelength can hold more interpolated tokens, while a shorter one cannot. That is why we employ interpolation in low-frequency. please refer to YaRN: Efficient Context Window Extension of Large Language Models for more details Experiments from translator: the table shows: the average accuracy of predicting next token to match the ground-truth next token given previous context. The experiment is based on a hybrid Transformer-GAU (Gated Attention Unit) model with a size of 100M parameters. For more details on the GAU, please refer to: https://arxiv.org/abs/2202.10447 When \(k=8\) context length 512(trained) 4096 (repeated text) 4096 (non-repeated text) Baseline 49.41% 24.17% 23.16% \ Baseline-$$\log n$$ 49.40% 24.60% 24.02% PI-RoPE 49.41% 15.04% 13.54% \ PI-RoPE-$$\log n$$ 49.40% 14.99% 16.51% NTK-RoPE 49.41% 51.28% 39.27% \ NTK-RoPE-$$\log n$$ 49.40% ***61.71%*** ***43.75%*** No fine-tuning is applied on all tests. Baseline: use extrapolation; PI（Positional Interpolation): replaces extrapolation in Baseline with interpolation; NTK-RoPE: replace extrapolation in Baseline with NTK-aware Scaled RoPE; \(\log n\): apply a scale to optimize self-attention for long context ref_1 Conclusion Direct extrapolation doesn’t work effectively on extension. Interpolation yields poor results without fine-tuning. NTK-RoPE achieves promising (though slightly reduced) results in extended context even without fine-tuning. A \(\log n\) factor indeed optimize self-attention for long context. What’s even more encouraging is that NTK-RoPE performs significantly better in ‘repeated’ extrapolation compared to ‘non-repeated’ one, suggesting that LLM with NTK-RoPE still retain the global attention ability across the expanded context, rather than confining its attention to a limited scope. In just a few weeks, the open-source community concerning long contexts totally blows our minds. OpenClosedAI, you better watch out. Future Research please check part-2 Reference YaRN: Efficient Context Window Extension of Large Language Models]]></summary></entry></feed>