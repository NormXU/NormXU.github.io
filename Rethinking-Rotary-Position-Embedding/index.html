<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Expand the Context Length with RoPE, Part 1 -- RoPE is a β-based Encoding | まいどぅー</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="author" content="Norm Inui">
<meta name="generator" content="Jekyll v4.2.2">
<link rel="canonical" href="/Rethinking-Rotary-Position-Embedding/">

<link rel="stylesheet" href="/assets/css/frame.css">

<link rel="alternate" href="/feed.xml" type="application/atom+xml" title="まいどぅー">

<link rel="stylesheet" href="/assets/katex/katex.min.css">
<script defer src="/assets/katex/katex.min.js"></script>
<script defer src="/assets/katex/contrib/auto-render.min.js" onload="renderMathInElement(document.body)"></script>







<header>
  <a href="/" class="title">まいどぅー</a>
  <nav><a href="/">Home</a><a href="/about/">Nuo Xu</a></nav>

</header>

<article><script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  backgroundColor: 'rgb(255, 82, 82)',
  textColor: '#fff'})
</script><header>
  <h1><a href="/Rethinking-Rotary-Position-Embedding/">Expand the Context Length with RoPE, Part 1 -- RoPE is a β-based Encoding</a></h1>
<time datetime="2023-08-09T00:00:00+00:00">August 09, 2023</time>
</header>

  <div class="entry">
      <div id="markdown-content">
          <blockquote>
  <p>Translated from the <a href="https://kexue.fm/archives/9675">post</a>, originally written in Chinese by Su, Jianlin</p>

  <p>Translated by Norm Inui</p>
</blockquote>

<h3 id="tldr">TL;DR</h3>

<ul>
  <li>Interpret RoPE from the perspective of a β-based encoding.</li>
  <li>Introduce recent developments in the open-source community regarding long contexts.</li>
  <li>Some approaches, such as NTK-aware Scale RoPE, can extend context length without fine-tuning.</li>
</ul>

<hr>

<p>For developers who are interested in how to extend the context length of LLMs (Large Language Models), the open-source community has continuously presented us with fascinating methods in the past few weeks. First, <a href="https://www.reddit.com/user/kaiokendev">@kaiokendev</a> experimented with a “positional linear interpolation” approach in his project <a href="https://kaiokendev.github.io/til#extending-context-to-8k">SuperHOT</a>.</p>

<p>He demonstrated that with minimal fine-tuning on long texts, existing LLMs can be easily adapted to handle contexts over their pretraining context length. Almost simultaneously, Meta proposed the same idea in the paper titled “<a href="https://arxiv.org/abs/2306.15595">Extending Context Window of Large Language Models via Positional Interpolation</a>.”</p>

<p>Shortly after the paper was published, <a href="https://www.reddit.com/user/bloc97">@bloc97</a> introduced the <a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-aware Scaled RoPE</a>, enabling LLM to extend its context length without fine-tuning! With all these methods, especially the NTK-aware Scaled RoPE, it persuades me to revisit the idea behind RoPE from a \(\beta\)-based encoding perspective.</p>

<h3 id="encoding-the-number">Encoding the Number</h3>
<p>Suppose we have an integer \(n\) within \(1,000\) (not including \(1,000\)) that we want to input into a model. What would be the best way to do this?</p>

<p>The most intuitive idea is to input it directly as a one-dimensional vector. However, the value of this vector has a large range from \(0\) to \(999\), which is not easy to optimize for gradient-based optimizers. What if we scale it between 0 and 1? That’s not good either, because the difference between adjacent numbers changes from \(1\) to \(0.001\), making it challenging for both the model and optimizer to distinguish between the numbers. In general, gradient-based optimizers are a bit “vulnerable” and can only handle inputs that aren’t too large or too small.</p>

<p>To solve this problem, it is necessary to find a smart way to represent the input. We might think about how we humans do. For an integer, like \(759\), it’s a three-digit number in decimal, with each digit ranging from 0 to 9. This inspires me to represent the input in decimal directly as a vector. That is, we transform the integer  \(n\) as a three-dimensional vector \([a,b,c]\), where \(a\), \(b\), and \(c\) represent the hundreds, tens, and units of \(n\) respectively. By increasing the input dimension, we can both reduce the range of each digit and get rid of small resolution between numbers. Luckily, neural networks are good at handling high-dimensional vectors.</p>

<p>If we want to further reduce the value span of each dimension, we can simply decrease the base, like using 8, 6, or even 2 as the encoding base at a cost of an increase in input vector dimensions.</p>

<h3 id="direct-extrapolation">Direct Extrapolation</h3>

<p>Let’s say we have trained a model with the input ranging from \(0\) to \(999\) represented as a three-dimensional vector in decimal. Then we want to enlarge the upper bound of \(n\) from \(1,000\) to \(2,000\). How can we realize this?</p>

<p>If we follow the same idea discussed above, the input will now be a \(4\)-dimensional vector. However, the model was trained for a \(3\)-dimensional vector. Therefore, the input with an extra dimension may confuse the model. Some might wonder why can’t we reserve extra dimensions in advance? Indeed, we can pre-allocate a few more dimensions. During the training phase with the upper bound as \(1,000\), they can be set to \(0\), but during the inference phase with an upper bound as \(2,000\), the pre-allocate dimension has to be set to value besides \(0\). This is what we call <strong>Extrapolation</strong>.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/0/extrapolation.png" alt="Extrapolation"></p>

<p>However, the dimensions reserved during the training phase have always been set to \(0\). If these dimensions were changed to other values during the inference phase, the performance might be significantly harmed. This is because the model is never trained to adapt to those pre-allocated dimension in different values.</p>

<h3 id="linear-interpolation">Linear Interpolation</h3>
<p>Considering the challenges above, some proposed interpolation instead of extrapolation to compress the value upper bound of \(2,000\) down to \(1,000\). For example, the number \(1749\) can simply compress to \(874.5\) by dividing \(2\). Thus, \(874.5\) can be converted into a three-dimensional vector \([8,7,4.5]\). Following this double mapping strategy, the vector \([7,4,9]\) now corresponds to \(1498\). 
However, the difference between adjacent numbers was \(1\), but now it is \(0.5\), making the last dimension more “crowded”. Therefore, interpolation usually needs to fine-tune the model to make it adapt to the “crowded” dimensions.</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/0/interpolation.png" alt="interpolation"></p>

<p>You might notice that the extrapolation can also be fine-tuned to adapt to the pre-allocated dimension. Well, that is correct, but the interpolation requires far fewer steps compared to extrapolation. It is because position encoding (both absolute and relative) has taught the model to understand the relative concept rather than precisely knowing what the number it is, that is: the model knows \(875\) is greater than \(874\), but it doesn’t know what is \(875\). 
Given the generalized ability of LLM, injecting the concept that \(874.5\) is greater than \(874\) is not particularly challenging.</p>

<p>However, this interpolation approach is not without flaws. The broader the range we want to expand, the smaller the unit dimension resolution becomes, while the hundreds and tens dimensions still remain at a resolution of 1. This means that the interpolation implicitly introduces resolution inconsistency across dimensions. Each dimension is not equally interpolated, making fine-tuning/continuing learning challenging.</p>

      </div>
      <div id="table-of-contents">
          
      </div>
      <div id="markdown-content">
          
<h3 id="base-conversion">Base Conversion</h3>
<p>Is there a method that doesn’t require adding extra dimensions and can still maintain resolution across dimensions? The answer is <strong>YES</strong>.</p>

<p>It is a solution that we are all familiar with, base conversion. A three-digit decimal number can represent \(0\) to \(999\). What if it’s in hexadecimal? Its maximum value in 16-base can be \(16^3-1=4095 &gt; 1999\). So, by simply converting to hexadecimal, with a base of 16, such as turning \(1749\) into \([6,13,5]\), a three-dimensional vector can represent a larger range. The cost? Each dimension’s value changes from a range of \(0-9\) to \(0-15\).</p>

<p><img src="https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/0/conversion.png" alt="conversion"></p>

<p>It’s indeed a clever idea. As mentioned earlier, what we care about is relative position information in the sequence since a trained model has already learned \(875 &gt; 874\). This holds true in hexadecimal as well. The model doesn’t care what base you use to represent the input, but only the relative information. Now, the only problem would be if the value of each dimension exceeds \(9\) (values between \(10-15\)), will the model still know how to compare accurately? Luckily, most LLMs have such capability. Based on this hypothesis, we can now extend the range without fine-tuning! Furthermore, to make interpolation more robust, we can use a smaller base like \(\lceil\sqrt[3]{2000}\rceil = 13\) instead of \(16\) to limit the value range of each dimension.</p>

<p>This idea of base conversion finally leads us to the NTK-aware scaled RoPE that was mentioned at the beginning.</p>

<h3 id="positional-encoding">Positional Encoding</h3>
<p>Based on the explanation above, we can claim:</p>
<blockquote>
  <p>The Rotational Positional Encoding (RoPE) at position \(n\) is the \(\beta\)-based encoding of the number \(n\)</p>
</blockquote>

<p>This might surprise you at first glance, however, it does hold true.</p>

<p><em>Proof:</em></p>

<p>Suppose we have a decimal number \(n\). To calculate the digit at position \(m\) (counting from right to left) in its β-based encoding, we have:</p>

\[\begin{equation}\lfloor\dfrac{n}{\beta^{m-1}}\rfloor \mod \beta \end{equation}\]

<p>As for RoPE, which is adapted from Sinusoidal Position Embedding</p>

\[\begin{equation}[\text{cos}(\dfrac{n}{\beta^0}), \text{sin}(\dfrac{n}{\beta^0}), \text{cos}(\dfrac{n}{\beta^1}), \text{sin}(\dfrac{n}{\beta^1}), …, \text{cos}(\dfrac{n}{\beta^{d/2-1}}), \text{sin}(\dfrac{n}{\beta^{d/2-1}})]\end{equation}\]

<p>where \(\beta = 10000^{2/d}\)</p>

<p>we can notice that:</p>

<p>1) <strong>eq1</strong> and <strong>eq2</strong> share the same component \(\frac n {\beta^{m-1}}\);</p>

<p>2) \(\text{mod}\) introduces periodicity, while \(\text{sin}\) and \(\text{cos}\) are also periodical functions.</p>

<p>Therefore, if we ignore the ceiling operation, we can say RoPE (or Sinusoidal Position Embedding) is a kind of β-based encoding.</p>

<p>With this property, we can now apply extrapolation on \(n\) by simply replacing \(n\) as \(n/k\), \(k\) is the scale we want to enlarge. This is the <strong>Positional Interpolation</strong> proposed in Meta’s paper, and the experimental results show that extrapolation indeed requires more fine-tuning steps than interpolation.</p>

<p>Regarding numeral base conversion, the objective is to expand the representation range by \(k\). Therefore, the β-base should be converted to at least \(β(k^{2/d})\) (according to <strong>eq2</strong>, \(\text{cos}\) and \(\text{sin}\) appear in pairs. This can be regarded as a β-base representation with \(d/2\) bits, not \(d\)). Alternatively, the original base \(10000\) can be replaced with \(10000k\), which is the NTK-aware Scaled RoPE. As discussed earlier, since positional embedding has taught the model the sequence relative information, NTK-aware Scaled RoPE can achieve good performance in longer contexts without fine-tuning.</p>

<h3 id="lets-dig-further">Let’s dig further</h3>
<p>You might wonder why we call it NTK (Neural Tangent Kernel). In fact, it is the academic background of @bloc97 that makes him use this term to name it.</p>

<p>In “<a href="https://arxiv.org/abs/2006.10739">Fourier Features Let Networks Learn High-Frequency Functions in Low-Dimensional Domains</a>”, authors use NTK methods to demonstrate that neural networks cannot learn high-frequency signals efficiently. Instead, their solution is to transform it into Fourier features, which share the same idea with the Sinusoidal position encoding we mention in <strong>eq1</strong>.</p>

<p>Thus, based on the findings from this NTK paper, @bloc97 proposed the NTK-aware Scaled RoPE. I ask him about how he derived it. Surprisingly, his derivation is quite straightforward. The main idea is to combine extrapolation with interpolation:</p>

<p><b><font color="red">extrapolation in high-frequency and  interpolation in low-frequency</font></b></p>

<p>According to <strong>eq2</strong>, the lowest frequency in each element of the position features is 
\(\dfrac{n}{\beta^{d/2-1}}\)
Here we introduce a factor \(\lambda\) in base, now we have: 
\(\dfrac{n}{(\beta\lambda)^{d/2-1}}\)</p>

<p>We expect that scaling the rotation base \(\beta\) can work as interpolation, therefore</p>

\[\begin{equation}\dfrac{n}{(\beta\lambda)^{d/2-1}} = \dfrac{n/k}{\beta^{d/2-1}}\end{equation}\]

<p>We can solve from <strong>eq3</strong>:</p>

\[\lambda = k^{2/(d-2)}\]

<p>The same idea for the highest frequency in the RoPE feature:</p>

<p>\(\dfrac{n}{\beta}\) now becomes \(\dfrac{n}{\lambda\beta}\).</p>

<p>Let’s insert the value \(\lambda\) we solve from <strong>eq3</strong>, which allows a low frequency represented as interpolation, into \(\dfrac{n}{\lambda\beta}\). Since \(d\) is very large ( 64 for BERT, 128 for LLAMA-1), \(\lambda \to 1\), thus, we can conclude from <strong>eq3</strong>:</p>

\[\begin{equation}\dfrac{n}{\beta\lambda}\simeq \dfrac{n}{\beta}\end{equation}\]

<p>You can see the frequency remains relatively stable w.r.t \(\lambda\), indicating that the corresponding dimension doesn’t become too crowded. Therefore, to represent a larger number, a high-frequency dimension is more likely to extrapolate by using an additional dimension rather than expanding the value range one dimension can hold. This is what we call: extrapolation in high-frequency.</p>

<p>From the derivation, we can see that NTK-aware Scaled RoPE cleverly combines interpolation and extrapolation together. Besides scaling the base, I believe any transformations on the frequencies will be also effective as long as it ensures the extrapolation in high frequencies and interpolation in low-frequencies.</p>

<blockquote>
  <p><strong>from translator</strong>: We can actually inteprete the <strong>extrapolation in high-frequency and  interpolation in low-frequency</strong> by considering from a wavelength perspective in RoPE. Specifically, the wavelength in RoPE is used to define the length of the token sequence required for the encoding at dimension \(d\) to complete a full rotation, \(2\pi\). The higher the frequency, the smaller the wavelength and vice verse. Therefore, a longer wavelength can hold more interpolated tokens, while a shorter one cannot. That is why we employ interpolation in low-frequency.</p>

  <p>please refer to <a href="https://github.com/jquesnelle/yarn/tree/master">YaRN: Efficient Context Window Extension of Large Language Models</a> for more details</p>
</blockquote>

<h3 id="experiments">Experiments</h3>
<blockquote>
  <p><strong>from translator</strong>: the table shows: the average accuracy of predicting next token to match the ground-truth next token given previous context. The experiment is based on a hybrid Transformer-GAU (Gated Attention Unit) model with a size of 100M parameters.</p>

  <p>For more details on the GAU,  please refer to: <a href="https://arxiv.org/abs/2202.10447">https://arxiv.org/abs/2202.10447</a></p>
</blockquote>

<p>When \(k=8\)</p>

<table>
  <thead>
    <tr>
      <th>context length</th>
      <th>512(trained)</th>
      <th>4096 (repeated text)</th>
      <th>4096 (non-repeated text)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline
<br>
Baseline-\(\log n\)</td>
      <td>49.41%
<br>
49.40%</td>
      <td>24.17%
<br>
24.60%</td>
      <td>23.16%
<br>
24.02%</td>
    </tr>
    
    <tr>
      <td>PI-RoPE
<br>
PI-RoPE-\(\log n\)</td>
      <td>49.41%
<br>
49.40%</td>
      <td>15.04%
<br>
14.99%</td>
      <td>13.54%
<br>
16.51%</td>
    </tr>
    
    <tr>
      <td>NTK-RoPE
<br>
NTK-RoPE-\(\log n\)</td>
      <td>49.41%
<br>
49.40%</td>
      <td>51.28%
<br>
<strong><em><u>61.71%</u></em></strong>
</td>
      <td>39.27%
<br>
<strong><em><u>43.75%</u></em></strong>
</td>
    </tr>
    
  </tbody>
</table>

<p>No fine-tuning is applied on all tests. <strong>Baseline</strong>: use extrapolation; <strong>PI（Positional Interpolation)</strong>: replaces extrapolation in Baseline with interpolation; <strong>NTK-RoPE</strong>: replace extrapolation in Baseline with NTK-aware Scaled RoPE; \(\log n\): apply a scale to optimize self-attention for long context <a href="https://openreview.net/forum?id=qc9O2EtrMI-">ref_1</a></p>

<h3 id="conclusion">Conclusion</h3>

<ol>
  <li>
    <p>Direct extrapolation doesn’t work effectively on extension.</p>
  </li>
  <li>
    <p>Interpolation yields poor results without fine-tuning.</p>
  </li>
  <li>
    <p>NTK-RoPE achieves promising (though slightly reduced) results in extended context even without fine-tuning.</p>
  </li>
  <li>
    <p>A \(\log n\) factor indeed optimize self-attention for long context.</p>
  </li>
  <li>
    <p>What’s even more encouraging is that NTK-RoPE performs significantly better in ‘repeated’ extrapolation compared to ‘non-repeated’ one, suggesting that LLM with NTK-RoPE still retain the global attention ability across the expanded context, rather than confining its attention to a limited scope.</p>
  </li>
</ol>

<p>In just a few weeks, the open-source community concerning long contexts totally blows our minds. <del>Open</del>ClosedAI, you better watch out.</p>

<h3 id="future-research">Future Research</h3>
<p>please check <a href="https://normxu.github.io/Rethinking-Rotary-Position-Embedding-2/">part-2</a></p>

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="https://github.com/jquesnelle/yarn/tree/master">YaRN: Efficient Context Window Extension of Large Language Models</a></li>
</ul>

      </div>
  </div>
  
</article>



<footer>
  <div><b style="color: #f45;">All Generation Tasks are Denoising Tasks.</b></div>
  <nav><a href="mailto:nxu8@outlook.com"><svg aria-label="Mail" class="icon"><use xlink:href="/assets/fontawesome/icons.svg#envelope"></use></svg></a><a href="https://github.com/NormXU"><svg aria-label="Github" class="icon"><use xlink:href="/assets/fontawesome/icons.svg#github"></use></svg></a></nav>

</footer>


</head>
</html>
