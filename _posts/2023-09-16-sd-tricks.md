---
layout: post
title: "SD/SDXL Tricks beneath the paper and codes"
tags: ["Diffusion"]
toc: true
excerpt_separator: <!--more-->
---
<h3 class="no_toc"> TL; DR</h3>
- Collect effective tricks about SD/SDXL pipeline

<!--more-->

<hr>
From the moment the SD/SDXL was unveiled, the pace of advancements in image generation has been breathtaking. Almost every day, the open-source community rolls out novel techniques that enhance the pipeline's aesthetic appeal and versatility. Yet, many of these innovations lack detailed documentation or in-depth explanations. To learn these tricks, one has to spend hours reading through the source codes. To simplify this process for developers and provide a convenient reference, I've written this blog to collect the tricks that lie beneath the published and source codes.


## Two Text Encoders in SDXL
In the SDXL paper, the two encoders that SDXL introduces into its pipeline are explained as follows:

> We opt for a more powerful pre-trained text encoder that we use for text conditioning. Specifically, we use OpenCLIP ViT-bigG in combination with CLIP ViT-L, where we concatenate the penultimate text encoder outputs along the channel-axis. Besides using cross-attention layers to condition the model on the text-input, we follow and additionally condition the model on the pooled text embedding from the OpenCLIP model.

To clarify how the two text encoders work together, here is a diagram Iâ€™ve made to illustrate the pipeline.

![Extrapolation](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/sdxl_text_encoder.png)

It's interesting to notice that the text features with 768-dim hidden features from the smaller text encoder are directly concatenated to the 1280-dim hidden features of the larger text encoder. Note that ```text_prompt_1``` and ```text_prompt_2``` can be different. Given the intuition that higher dimensions can capture fine-grained features, some AI painters prefer to feed style descriptions into CLIP-ViT-L and prompts about body motion, clothes, or facial prompts into the other text encoder. Additionally, it's worth noting that the pooled feature from the larger text encoder  acts as a bias to adjust the time embedding

