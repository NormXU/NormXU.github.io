---
layout: post
title: "SD/SDXL Tricks beneath the Papers and Codes"
tags: ["Diffusion"]
toc: true
excerpt_separator: <!--more-->
---
<h3 class="no_toc"> TL; DR</h3>
- Collect awesome tricks about SD/SDXL pipeline
<!--more-->

<hr>
From the moment the SD/SDXL was unveiled, the pace of advancements in image generation never stops. Almost every day, the open-source community rolls out novel techniques that enhance the pipeline's aesthetic appeal and versatility. Yet, many of these innovations lack detailed documentation or in-depth explanations. To learn these tricks, one has to spend hours reading through the source codes. To simplify this process for developers and provide a convenient reference, I've written this blog to collect the tricks that lie beneath the published and source codes.
> **Artistic Respect and Recognition:** All artworks referenced in this blog are attributed to their original creators. Images inspired by these artists' unique styles are not permitted for commercial use. If you create an image by referring their art style, please kindly give credit by acknowledging the artist's name.

## Two Text Encoders in SDXL
In the SDXL [paper](https://arxiv.org/abs/2307.01952), the two encoders that SDXL introduces are explained as below:

> We opt for a more powerful pre-trained text encoder that we use for text conditioning. Specifically, we use OpenCLIP ViT-bigG in combination with CLIP ViT-L, where we concatenate the penultimate text encoder outputs along the channel-axis. Besides using cross-attention layers to condition the model on the text-input, we follow and additionally condition the model on the pooled text embedding from the OpenCLIP model.

To clarify how the two text encoders work together, here is a diagram I’ve made to illustrate the pipeline.

![SDXL_Text_Encoders](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/sdxl_text_encoder.png)

Figure1. text encoder pipeline in SDXL; text_prompt 1 and text_prompt 2 are two prompts, which can be different; x0, y0, ∆x, ∆y, h, w are 6 spatial conditions newly introduced by SDXL 

It's interesting to notice that the 768-dim hidden features from the smaller text encoder are directly concatenated to the 1280-dim hidden features of the larger text encoder. Note that ```text_prompt_1``` and ```text_prompt_2``` can be different. Given the intuition that higher dimensions can capture fine-grained features, some AI painters prefer to feed style descriptions into CLIP-ViT-L and prompts about body motion, clothes, or facial expressions into the other text encoder. Additionally, it's worth noting that the pooled feature from the larger text encoder  acts as a bias to adjust the time embedding

## Reference-only Mode

Reference-only mode is one of my favorite features in the SD pipeline, which enables art style transfer from a reference image to a new image based on a given prompt, without training.

It is first introduced by [lllyasviel](<https://github.com/lllyasviel>) in [this](https://github.com/Mikubill/sd-webui-controlnet/discussions/1280) discussion thread.

Here is a showcase from this reference only.  

![Showcase](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/showcase.png)
Figure2. A reference-only showcase;  **Left:** reference image credit to [@001_31_](<https://twitter.com/001_31_/media>); **Right:** generated image; We can clearly see the generated image "plagiarizes" the art style of the reference image like colors, lines and character face; please check the appendix for more setting details.

The idea behind the `reference-only` is quite straightforward. It requires running the diffusion process twice. In the first round, the VAE encoder encodes the reference image into a latent feature, termed as $$x_{ref}$$. This reference latent feature is then interpolated with a randomly initialized latent feature. We denote the noisy latent feature as $$x$$.  

![Reference_only](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/1/ref_only.png)
Figure3. reference-only pipeline; ①: the first diffusion forward pass, ②: the second diffusion forward pass


Suppose we are playing this `reference-only` mode with an SD1.5 checkpoint. As we know, SD1.5 has 16 TransformerBlocks across all UNet layers. Therefore, during the first diffusion forward pass, 16 hidden features are generated and stored in the memory band. Subsequently, we pass $$x$$ to the second diffusion forward pass. 

The self-attention block uses the previously cached features from the corresponding Transformer Block layer in the prior diffusion as a reference clue. In other words, there are now “two” cross-attention blocks in the second diffusion forward pass. One is conditioned on the reference clues generated from $$x_{ref}$$, while the other is conditioned on the text features.

As for the self-attention part in the second diffusion forward pass,  There exist implementation variants that can yield different generation outcomes.

In the following discussion, we denote the input features of the self-attention block as $$h_{in}$$ within one TransformerBlock, and the resulting feature as $$h_{out}$$. Typically, we apply the classifier-free method for text guiding, $$h_{in}, h_{out} \in \mathbb{R^{2 \times N \times dim}}$$, where $$N$$ is the number of elements in the latent feature, $$dim$$ is the hidden feature dimension.

Additionally, we denote the cached features from the same TransformerBlock generated in the first diffusion forward pass as $$ h_{cond} \in \mathbb{R^{2 \times N \times dim}}$$

<h4 class="no_toc">  Reference-fine-grained </h4>
One implementation is only the conditioned latent features use the cached features as the cross-attention clue, while the unconditioned latent features compute cross-attention itself. The pseudo-code is provided below:

```python
attn_output_uc = attn( h_in[1],
                       encoder_hidden_states=torch.cat([h_in[1], h_cond[1]] , dim=1))

attn_output_c =  attn( h_in[0], encoder_hidden_states=h_in[0])

h_out = torch.cat((style_fidelity * attn_output_c, (1.0 - style_fidelity) * attn_output_uc),  dim=0)
```

This is how huggingface implements. check the code [here](https://github.com/huggingface/diffusers/blob/73bb97adfc3d0cb184c5fd66a1d5699c249a7fd8/examples/community/stable_diffusion_reference.py#L405).
I'd like to refer it as `Reference-fine-grained`

<h4 class="no_toc">  Reference-coarse</h4>
Another implementation processes both the unconditional and conditional latent features in the same way. I prefer to call it `Reference-coarse`

```python
h_out = attn( h_in,
              encoder_hidden_states=torch.cat([h_in, h_cond], dim=1))
```
I suggest you play with different reference methods and find your favorite one.




## Appendix

<h4 class="no_toc"> 1. reference-only showcase generation setting in Figure 2</h4>

- **prompt:**  masterpiece,best quality, ultra highres, detailed illustration, portrait, detailed, girls, detailed frilled clothes, detailed beautiful skin, face focus

- **negative embedding:** EasyNegative, bad-picture-chill

- **sd1.5 checkpoint:** A fine-tuned checkpoint based on SD1.5 with proprietary dataset

