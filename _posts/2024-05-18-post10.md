---
layout: post 
title: "All Generation Tasks are Denoising Tasks ðŸš§ (WIP)"
tags: ["Sparks"]
excerpt_separator: <!--more-->

---

<h3 class="no_toc"> TL; DR</h3>

<hr>

No matter what we expect the model to generateâ€”whether it's a series of words or an image, whether using auto-regressive or inverse diffusionâ€”the underlying mechanism is the same: <b>denoising</b>.

In fact, many works have utilized the denoising process to accelerate inference. In language models, this denoising process is known as parallel decoding. For example, [lookahead decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/) introduces a N-Gram Pool to mitigate the limitation of Jacobi iteration that generation always go wrong. [CLLMs](https://arxiv.org/pdf/2403.00835) fine-tunes a LLM to generate the same n-token as AR decoding. To simplify the task, they map the randomly initially states to an intermediate points on the Jacabi trajectory.  

In the image/video generation, the denoising processing is known as [Rectified Flow](https://arxiv.org/pdf/2209.03003). Rectified Flow is one of the most effective denosing algorithms to accelerate image/video generation model. The main idea is that: since mapping directly from $$X_0$$ to $$X_1$$ is too difficult to learn, we introduces multiple milestones throughout the process, transforming $$X_0 â†’ X_1$$ into $$Z_0^{k} â†’ Z_1^{k}$$. Here, $$X_0$$ is the state sampled from a Gaussian Distribution, $$X_1$$ is the target image. $$Z_0^{k}$$, $$Z_1^{k}$$ is the kth intermediate states during denosing from $$X_0$$ to $$X_1$$. The optimization target between milestones is velocity, which ensures the overall goal of moving from $$X_0$$ to $$X_1$$ can still be accomplished effectively.

![CLLMs](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/10/cllm.png)

Figure1. Image Credit to [CLLMs](https://arxiv.org/pdf/2403.00835)

## SFT from a Denosing Perspective

In a recent study by [Gekhman, Zorik, et al.](https://arxiv.org/pdf/2405.05904), the authors reveal that supervised
fine-tuning (SFT) struggles to integrate new knowledge into Large Language Model (LLM). The authors propose an approach by categorizing fine-tuning datasets into four categories as below:

![knowledge](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/10/knowledge_categories.png)

Figure 2. Knowledge Categories

Their findings suggest that the most effective fine-tuning datasets should consist of HighlyKnown, MaybeKnown, and WeaklyKnown data, with minimal amount of Unknown data.

This conclusion makes me rethink the nature of SFT. To simplify the problem, let's take LLMs as a linear systems. Under this assumption, We define $$A$$ as the weight matrix of multiple transformer blocks, while $$x$$ and $$b$$ represent input embeddings and text hidden features, respectively. We link autoregressive generation to denoising process:

$$A \cdot [x_{bos}; x_{prefill}; x_{gen}^{\prime}]$$

As for the input embedding $$x$$, since the $$x_{gen}$$ is unknown yet, we initialize it randomly as $$x_{gen}^{\prime}$$

During SFT LLM, we actually try to solve the following equation:

$$A \cdot [x_{bos}; x_{prefill}; x_{gen}^{\prime}] = [b_{prefill}; b_{gen}; b_{eos}]$$

Here is an intriguing question: if the linear equation has no solution, can fine-tuning still be effective?

A Hypothesis:

- If $$Ax=b$$ has no solution, it is exactly the Unknown case mention
  in [Gekhman, Zorik, et al.](https://arxiv.org/pdf/2405.05904) Since we expect LLM always responses, it has to hallucintate with an approximate solution to $$Ax=b$$ and this is where hallucination happens. 

- If $$Ax=b$$ has a unique solution, it links to the HighlyKnown when Greedy decoding always predicts the correct answer

- If $$Ax=b$$ has infinitely many solutions, it relates to MaybeKnown or WeaklyKnown instances when Greedy decoding sometimes predict the correct answers.

This hypothesis explains why hallucination happens: it is when $$Ax=b$$ has no solutions but we force the LLM to return us an approximate solution.
