---
layout: post 
title: "All Generation Tasks are Path Plainning Tasks ðŸš§ (WIP)"
tags: ["Sparks"]
excerpt_separator: <!--more-->

---

<h3 class="no_toc"> TL; DR</h3>

<hr>

Imagine an unknown latent feature space. We begin at a randomly initialized point sampled from a normal distribution, and our goal is to somehow reach the target that aligns with human preferences, regardless of what the target isâ€”could it be a sequence of words, an image or a video. This is fundamentally a <mark>path planning problem</mark>. Today, we already have tools including the Large Language Model (LLM) and Diffusion-based generation models to guide us toward the target, whether through next-token prediction or IDDPM. 

In fact, if we take all generative tasks as path-planning tasks, we can explain many empirical results.

### Less is More?

For instance, [LIMA](https://arxiv.org/pdf/2305.11206) claims that data quality matters than quantity, which has become a common knowledge in community today. But why is this the case? Returning to our path-planning analogy, we can notice that defining a clear goal is really hard since human preferences are diverse and complex. High-quality data provides a clearer, more specific target, allowing models to plan a more precise path. On the other hand, an ambiguous goal from amount of low-quality data makes this process far less efficient. 



It's also worth menting that some papers refer to path planning in this context as **denoising**.



In language models, this denoising is known as parallel decoding. For example, [lookahead decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/) introduces a N-Gram Pool to mitigate the limitation of Jacobi iteration that generation always go wrong. [CLLMs](https://arxiv.org/pdf/2403.00835) fine-tunes a LLM to generate the same n-token as AR decoding. To simplify the task, they map the randomly initially states to an intermediate points on the Jacabi trajectory.  

In the image/video generation, the denoising is known as [Flow Matching](https://arxiv.org/pdf/2210.02747) and its optimal Transport case, [Rectified Flow](https://arxiv.org/pdf/2209.03003).

![CLLMs](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/10/cllm.png)

Figure1. Image Credit to [CLLMs](https://arxiv.org/pdf/2403.00835)



### Why hallucination happens?

In a recent study by [Gekhman, Zorik, et al.](https://arxiv.org/pdf/2405.05904), the authors reveal that supervised
fine-tuning (SFT) struggles to integrate new knowledge into Large Language Model (LLM). The authors propose an approach by categorizing fine-tuning datasets into four categories as below:

![knowledge](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/10/knowledge_categories.png)

Figure 2. Knowledge Categories

Their findings suggest that the most effective fine-tuning datasets should consist of HighlyKnown, MaybeKnown, and WeaklyKnown data, with minimal amount of Unknown data.

This conclusion makes me rethink what is SFT inherently. 

#### From Denosing Perspective

To simplify the problem, let's take LLMs as a linear systems. Under this assumption, We define $$A$$ as the weight matrix of multiple transformer blocks, while $$x$$ and $$b$$ represent input embeddings and text hidden features, respectively. We link autoregressive generation to denoising process:

$$A \cdot [x_{bos}; x_{prefill}; x_{gen}^{\prime}]$$

As for the input embedding $$x$$, since the $$x_{gen}$$ is unknown yet, we initialize it randomly as $$x_{gen}^{\prime}$$

During SFT LLM, we actually try to solve the following equation:

$$A \cdot [x_{bos}; x_{prefill}; x_{gen}^{\prime}] = [b_{prefill}; b_{gen}; b_{eos}]$$

Here is an intriguing question: if the linear equation has no solution, can fine-tuning still be effective?

A Hypothesis:

- If $$Ax=b$$ has no solution, it is exactly the Unknown case mention
  in [Gekhman, Zorik, et al.](https://arxiv.org/pdf/2405.05904) Since we expect LLM always responses, it has to hallucintate with an approximate solution to $$Ax=b$$ and this is where hallucination happens. 

- If $$Ax=b$$ has a unique solution, it links to the HighlyKnown when Greedy decoding always predicts the correct answer

- If $$Ax=b$$ has infinitely many solutions, it relates to MaybeKnown or WeaklyKnown instances when Greedy decoding sometimes predict the correct answers.

This hypothesis explains why hallucination happens: it is when $$Ax=b$$ has no solutions but we force the LLM to return us an approximate solution.

#### From Path Planning Perspective
