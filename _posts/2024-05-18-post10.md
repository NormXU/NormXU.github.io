---
layout: post
title: "Reimagining Supervised Fine-Tuning for Large Language Models and Diffusion from a Denoising Perspective"
tags: ["Sparks"]
excerpt_separator: <!--more-->
toc: true
---

<h3 class="no_toc"> TL; DR</h3>

<hr>

> ðŸš§ (WIP)

In a recent study by [Gekhman, Zorik, et al.](https://arxiv.org/pdf/2405.05904), the authors reveal that supervised fine-tuning (SFT) struggles to integrate new knowledge into Large Language Model (LLM). The authors propose an approach by categorizing fine-tuning datasets into four categories as below:

![SDM-Unet](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/10/knowledge_categories.png)



Their findings suggest that the most effective fine-tuning datasets should consist of HighlyKnown, MaybeKnown, and WeaklyKnown data, with minimal amount of Unknown data.

This conclusion makes me rethink the nature of SFT. To simplify the problem, let's take LLMs as a linear systems. In this condition, 

we define A as the weight matrix of multiple transformer blocks, while x and b represent input embeddings and text hidden features, respectively.




Following [lookahead decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/), we can link autoregressive generation to a parallel decoding process


$A \cdot \text{concat}[x_{bos}, x_{prefill}, x_{gen}^{\prime}]$

As for the input embedding x, since the $x_{gen}$ is unknown yet, we initialize it randomly as $x_{gen}^{\prime}$



During SFT LLM, we actually try to solve the following equation:

$A \cdot \text{concat}[x_{bos}, x_{prefill}, x_{gen}^{\prime}] = \text{concat}[b_{prefill}, b_{gen}, b_{eos}]$



Here is an intriguing question: if the linear equation has no solution, can fine-tuning still be effective?

Here is a Hypothesis:

- If $Ax=b$ has no solution, it is exactly the Unknown case mention in [Gekhman, Zorik, et al.](https://arxiv.org/pdf/2405.05904)


- If $Ax=b$ has a unique solution, it links to the HighlyKnown when Greedy decoding always predicts the correct answer


- If $Ax=b$ has infinitely many solutions, it relates to MaybeKnown or WeaklyKnown instances when Greedy decoding sometimes predict the correct answers.



