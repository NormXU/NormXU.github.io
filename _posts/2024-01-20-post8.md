---
layout: post
title: "Your SDXL is a slow learner"
tags: ["Diffusion"]
---

> WIP ðŸš§

### TL; DR
- Initially, fine-tuning SDXL UNET with high-quality datasets can enhance the AES score of generated images, but there is a limitation to this improvement. Specifically, you can easily upgrade the AES score of SDXL-base, which is initially aesthetically evaluated as 6.0, to reach a score of 6.7. However, a further enhancement in the AES score becomes significantly challenging, even tuning on a high-quality dataset whose average AES score exceed 7.5 with many steps.

- High-quality fine-tuning can negatively affect text fidelity.

- Fine-tuning UNET without incorporating high-level noise latent features can enhance text fidelity in trade with a slight decrease in aesthetic quality

- A self-reward SDXL-DPO pipeline can both benefit text fidelity and aesthetic quality

## Background

### high-quality fine-tuning

It is widely known that fine-tuning SDXL using a carefully curated set of aesthetically pleasing data can significantly enhance the quality of SDXL's output. In the [EMU](https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/) paper, the authors assert that quality tuning can effectively enhance the pretrained base model, similar to how we do supervised fine-tuning with a Language Model (LLM).

The EMU paper has provided a good fine-tuning recipe for aesthetic alignment as follows:

- 2k highly visually appealing images

- a small batch size of 64

- noise-offset is set to 0.1  

- no more than 15K iterations regardless of loss decreasing  

However, it's worth noting that the EMU paper hid certain crucial details regarding this recipe. For instance, it doesn't show us any samples from the 2,000 images to let us know how appealing these images are, the choice of the optimizer, and the criteria for determining when to early stop the fine-tuning process.

### SDXL-DPO

If you download a highly-rated checkpoint from Civitai and use it to create an image, you'll likely find it  a bit tricky to generate an image that can follow detailed textual descriptions. This issue is likely because these checkpoints are overfitted on a curated dataset. Considering the fact that it is really hard to find when to early stop the training, the checkpoints are easily overfitted and harm the text fidelity.

To address such an issue, a promising solution is to push the diffusion models towards high text fidelity with [Diffusion-DPO](https://arxiv.org/abs/2311.12908). Diffusion-DPO enhances the training target for SDXL besides MES loss of noises by incorporating a rewarding loss without the need for an online rewarding model. However, the efficacy of DPO in fixing text fidelity largely depends on the quality of the preference data pairs. Whatâ€™s worse, it is hard to find any reliable metrics to determine whether SDXL-DPO can fix the text fidelity without sacrificing aesthetic quality.

### Self-Rewarding

Recently, in [this](https://arxiv.org/abs/2401.10020) paper, Meta researchers showed that using an iteratively updated model, which generates its own rewards during training, can significantly enhance the instruction following ability when compared to a frozen reward model during Large Language Model SFT. What's particularly surprising is that by fine-tuning Llama 2 70B through only three iterations of this self-rewarding approach, they achieved a model that outperforms several closed-source LLMs, including Claude 2, Gemini Pro, and even GPT-4 0613. This finding has inspired me to incorporate this concept into the SDXL-DPO pipeline to explore the potential benefits for image generation.

## Method

We follow the idea of Self-Rewarding and slowly updated the reference SDXL unet

![rewarding_pipeline](https://raw.githubusercontent.com/NormXU/NormXU.github.io/main/_data/resources/blog/8/pipeline.png)
Figure 1. Self-Reward SDXL pipeline